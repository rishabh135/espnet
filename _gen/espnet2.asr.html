<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>espnet2.asr package &mdash; ESPnet 202205 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="espnet2.text package" href="espnet2.text.html" />
    <link rel="prev" title="espnet2.tts package" href="espnet2.tts.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> ESPnet
          </a>
              <div class="version">
                202205
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelization.html">Using Job scheduling system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Docker</a></li>
</ul>
<p><span class="caption-text">ESPnet2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html">ESPnet2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html#instruction-for-run-sh">Instruction for run.sh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_training_option.html">Change the configuration for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_task.html">Task class and data input system for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_distributed.html">Distributed training</a></li>
</ul>
<p><span class="caption-text">Notebook:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_cli.html">Speech Recognition (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_library.html">Speech Recognition (Library)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_realtime_demo.html">ESPnet2-ASR realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html"><strong>Use transfer learning for ASR in ESPnet2</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#Abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#ESPnet-installation-(about-10-minutes-in-total)">ESPnet installation (about 10 minutes in total)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#mini_an4-recipe-as-a-transfer-learning-example">mini_an4 recipe as a transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_streaming_asr_demo.html">ESPnet2 real streaming Transformer demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tts_realtime_demo.html">ESPnet2-TTS realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html">CMU 11751/18781 2021: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-an-inference-example">Run an inference example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Full-installation">Full installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-a-recipe-example">Run a recipe example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#Contents">Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#(1)-Tutorials-on-the-Basic-Usage">(1) Tutorials on the Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#(2)-Tutorials-on-Contributing-to-ESPNet-SE-Project">(2) Tutorials on Contributing to ESPNet-SE Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/pretrained.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/se_demo.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/st_demo.html">ESPnet Speech Translation Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_cli.html">Text-to-Speech (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_realtime_demo.html">ESPnet real time E2E-TTS demonstration</a></li>
</ul>
<p><span class="caption-text">Package Reference:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="espnet.utils.html">espnet.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.st.html">espnet.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.vc.html">espnet.vc package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.lm.html">espnet.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.bin.html">espnet.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.mt.html">espnet.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.scheduler.html">espnet.scheduler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.optimizer.html">espnet.optimizer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.nets.html">espnet.nets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.utils.html">espnet2.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.st.html">espnet2.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.optimizers.html">espnet2.optimizers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.layers.html">espnet2.layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.iterators.html">espnet2.iterators package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tasks.html">espnet2.tasks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.lm.html">espnet2.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.bin.html">espnet2.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.main_funcs.html">espnet2.main_funcs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.fileio.html">espnet2.fileio package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.samplers.html">espnet2.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.gan_tts.html">espnet2.gan_tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.mt.html">espnet2.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.hubert.html">espnet2.hubert package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.schedulers.html">espnet2.schedulers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.enh.html">espnet2.enh package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.diar.html">espnet2.diar package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.torch_utils.html">espnet2.torch_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.fst.html">espnet2.fst package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tts.html">espnet2.tts package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">espnet2.asr package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-maskctc-model">espnet2.asr.maskctc_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-espnet-model">espnet2.asr.espnet_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-adversarial-branch">espnet2.asr.adversarial_branch</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-ctc">espnet2.asr.ctc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-init">espnet2.asr.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-transducer-decoder">espnet2.asr.transducer.transducer_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-joint-network">espnet2.asr.transducer.joint_network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-error-calculator">espnet2.asr.transducer.error_calculator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-utils">espnet2.asr.transducer.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-beam-search-transducer">espnet2.asr.transducer.beam_search_transducer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-transducer-init">espnet2.asr.transducer.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-abs-preencoder">espnet2.asr.preencoder.abs_preencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-sinc">espnet2.asr.preencoder.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-linear">espnet2.asr.preencoder.linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-preencoder-init">espnet2.asr.preencoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-mlm-decoder">espnet2.asr.decoder.mlm_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-abs-decoder">espnet2.asr.decoder.abs_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-transformer-decoder">espnet2.asr.decoder.transformer_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-rnn-decoder">espnet2.asr.decoder.rnn_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-decoder-init">espnet2.asr.decoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-abs-specaug">espnet2.asr.specaug.abs_specaug</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-specaug">espnet2.asr.specaug.specaug</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-specaug-init">espnet2.asr.specaug.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-abs-frontend">espnet2.asr.frontend.abs_frontend</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-default">espnet2.asr.frontend.default</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-fused">espnet2.asr.frontend.fused</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-s3prl">espnet2.asr.frontend.s3prl</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-init">espnet2.asr.frontend.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-frontend-windowing">espnet2.asr.frontend.windowing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-postencoder-abs-postencoder">espnet2.asr.postencoder.abs_postencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-postencoder-hugging-face-transformers-postencoder">espnet2.asr.postencoder.hugging_face_transformers_postencoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-postencoder-init">espnet2.asr.postencoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-wav2vec2-encoder">espnet2.asr.encoder.wav2vec2_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-transformer-encoder">espnet2.asr.encoder.transformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-abs-encoder">espnet2.asr.encoder.abs_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-longformer-encoder">espnet2.asr.encoder.longformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-rnn-encoder">espnet2.asr.encoder.rnn_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-hubert-encoder">espnet2.asr.encoder.hubert_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-contextual-block-transformer-encoder">espnet2.asr.encoder.contextual_block_transformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-vgg-rnn-encoder">espnet2.asr.encoder.vgg_rnn_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-conformer-encoder">espnet2.asr.encoder.conformer_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-init">espnet2.asr.encoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-asr-encoder-contextual-block-conformer-encoder">espnet2.asr.encoder.contextual_block_conformer_encoder</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.text.html">espnet2.text package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.train.html">espnet2.train package</a></li>
</ul>
<p><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet2_bin.html">core tools (espnet2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_sh.html">bash utility tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ESPnet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>espnet2.asr package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/_gen/espnet2.asr.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="espnet2-asr-package">
<h1>espnet2.asr package<a class="headerlink" href="#espnet2-asr-package" title="Permalink to this headline">¶</a></h1>
<section id="espnet2-asr-maskctc-model">
<span id="id1"></span><h2>espnet2.asr.maskctc_model<a class="headerlink" href="#espnet2-asr-maskctc-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.maskctc_model"></span><dl class="class">
<dt id="espnet2.asr.maskctc_model.MaskCTCInference">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.maskctc_model.</code><code class="sig-name descname">MaskCTCInference</code><span class="sig-paren">(</span><em class="sig-param">asr_model: espnet2.asr.maskctc_model.MaskCTCModel</em>, <em class="sig-param">n_iterations: int</em>, <em class="sig-param">threshold_probability: float</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCInference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCInference" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Mask-CTC-based non-autoregressive inference</p>
<p>Initialize Mask-CTC inference</p>
<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCInference.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet.nets.beam_search.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCInference.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCInference.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Mask-CTC inference</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCInference.ids2text">
<code class="sig-name descname">ids2text</code><span class="sig-paren">(</span><em class="sig-param">ids: List[int]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCInference.ids2text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCInference.ids2text" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.maskctc_model.</code><code class="sig-name descname">MaskCTCModel</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int, token_list: Union[Tuple[str, ...], List[str]], frontend: Optional[espnet2.asr.frontend.abs_frontend.AbsFrontend], specaug: Optional[espnet2.asr.specaug.abs_specaug.AbsSpecAug], normalize: Optional[espnet2.layers.abs_normalize.AbsNormalize], preencoder: Optional[espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder], encoder: espnet2.asr.encoder.abs_encoder.AbsEncoder, postencoder: Optional[espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder], decoder: espnet2.asr.decoder.mlm_decoder.MLMDecoder, ctc: espnet2.asr.ctc.CTC, joint_network: Optional[torch.nn.modules.module.Module] = None, ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '&lt;space&gt;', sym_blank: str = '&lt;blank&gt;', sym_mask: str = '&lt;mask&gt;', extract_feats_in_collect_stats: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.espnet_model.ESPnetASRModel" title="espnet2.asr.espnet_model.ESPnetASRModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.espnet_model.ESPnetASRModel</span></code></a></p>
<p>Hybrid CTC/Masked LM Encoder-Decoder model (Mask-CTC)</p>
<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel.batchify_nll">
<code class="sig-name descname">batchify_nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em>, <em class="sig-param">batch_size: int = 100</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel.batchify_nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel.batchify_nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>To avoid OOM, this fuction seperate the input into batches.
Then call nll for each batch and combine and return results.
:param encoder_out: (Batch, Length, Dim)
:param encoder_out_lens: (Batch,)
:param ys_pad: (Batch, Length)
:param ys_pad_lens: (Batch,)
:param batch_size: int, samples each batch contain when computing nll,</p>
<blockquote>
<div><p>you may change this to avoid OOM or increase
GPU memory usage</p>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
<li><p><strong>text</strong> – (Batch, Length)</p></li>
<li><p><strong>text_lengths</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.maskctc_model.MaskCTCModel.nll">
<code class="sig-name descname">nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/maskctc_model.html#MaskCTCModel.nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.maskctc_model.MaskCTCModel.nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>Normally, this function is called in batchify_nll.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_out</strong> – (Batch, Length, Dim)</p></li>
<li><p><strong>encoder_out_lens</strong> – (Batch,)</p></li>
<li><p><strong>ys_pad</strong> – (Batch, Length)</p></li>
<li><p><strong>ys_pad_lens</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-espnet-model">
<span id="id2"></span><h2>espnet2.asr.espnet_model<a class="headerlink" href="#espnet2-asr-espnet-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.espnet_model"></span><dl class="class">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.espnet_model.</code><code class="sig-name descname">ESPnetASRModel</code><span class="sig-paren">(</span><em class="sig-param">adv_flag, grlalpha, vocab_size: int, token_list: Union[Tuple[str, ...], List[str]], frontend: Optional[espnet2.asr.frontend.abs_frontend.AbsFrontend], specaug: Optional[espnet2.asr.specaug.abs_specaug.AbsSpecAug], normalize: Optional[espnet2.layers.abs_normalize.AbsNormalize], preencoder: Optional[espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder], encoder: espnet2.asr.encoder.abs_encoder.AbsEncoder, adversarial_branch: Optional[espnet2.asr.adversarial_branch.SpeakerAdv], postencoder: Optional[espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder], decoder: espnet2.asr.decoder.abs_decoder.AbsDecoder, ctc: espnet2.asr.ctc.CTC, joint_network: Optional[torch.nn.modules.module.Module], ctc_weight: float = 0.5, interctc_weight: float = 0.0, ignore_id: int = -1, lsm_weight: float = 0.0, length_normalized_loss: bool = False, report_cer: bool = True, report_wer: bool = True, sym_space: str = '&lt;space&gt;', sym_blank: str = '&lt;blank&gt;', extract_feats_in_collect_stats: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="espnet2.train.html#espnet2.train.abs_espnet_model.AbsESPnetModel" title="espnet2.train.abs_espnet_model.AbsESPnetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.train.abs_espnet_model.AbsESPnetModel</span></code></a></p>
<p>CTC-attention hybrid Encoder-Decoder model</p>
<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.batchify_nll">
<code class="sig-name descname">batchify_nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em>, <em class="sig-param">batch_size: int = 100</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.batchify_nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.batchify_nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>To avoid OOM, this fuction seperate the input into batches.
Then call nll for each batch and combine and return results.
:param encoder_out: (Batch, Length, Dim)
:param encoder_out_lens: (Batch,)
:param ys_pad: (Batch, Length)
:param ys_pad_lens: (Batch,)
:param batch_size: int, samples each batch contain when computing nll,</p>
<blockquote>
<div><p>you may change this to avoid OOM or increase
GPU memory usage</p>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.collect_feats">
<code class="sig-name descname">collect_feats</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.collect_feats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.collect_feats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.encode">
<code class="sig-name descname">encode</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder. Note that this method is used by asr_inference.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
<li><p><strong>text</strong> – (Batch, Length)</p></li>
<li><p><strong>text_lengths</strong> – (Batch,)</p></li>
<li><p><strong>kwargs</strong> – “utt_id” is among the input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.freeze_adversarial">
<code class="sig-name descname">freeze_adversarial</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.freeze_adversarial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.freeze_adversarial" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.freeze_encoder">
<code class="sig-name descname">freeze_encoder</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.freeze_encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.freeze_encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.nll">
<code class="sig-name descname">nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>Normally, this function is called in batchify_nll.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_out</strong> – (Batch, Length, Dim)</p></li>
<li><p><strong>encoder_out_lens</strong> – (Batch,)</p></li>
<li><p><strong>ys_pad</strong> – (Batch, Length)</p></li>
<li><p><strong>ys_pad_lens</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.unfreeze_adversarial">
<code class="sig-name descname">unfreeze_adversarial</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.unfreeze_adversarial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.unfreeze_adversarial" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.espnet_model.ESPnetASRModel.unfreeze_encoder">
<code class="sig-name descname">unfreeze_encoder</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/espnet_model.html#ESPnetASRModel.unfreeze_encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.espnet_model.ESPnetASRModel.unfreeze_encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-adversarial-branch">
<span id="id3"></span><h2>espnet2.asr.adversarial_branch<a class="headerlink" href="#espnet2-asr-adversarial-branch" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.adversarial_branch"></span><dl class="class">
<dt id="espnet2.asr.adversarial_branch.ReverseLayerF">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.adversarial_branch.</code><code class="sig-name descname">ReverseLayerF</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#ReverseLayerF"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.ReverseLayerF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<dl class="method">
<dt id="espnet2.asr.adversarial_branch.ReverseLayerF.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#ReverseLayerF.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.ReverseLayerF.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#espnet2.asr.adversarial_branch.ReverseLayerF.forward" title="espnet2.asr.adversarial_branch.ReverseLayerF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#espnet2.asr.adversarial_branch.ReverseLayerF.forward" title="espnet2.asr.adversarial_branch.ReverseLayerF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#espnet2.asr.adversarial_branch.ReverseLayerF.backward" title="espnet2.asr.adversarial_branch.ReverseLayerF.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#espnet2.asr.adversarial_branch.ReverseLayerF.forward" title="espnet2.asr.adversarial_branch.ReverseLayerF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.adversarial_branch.ReverseLayerF.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ctx</em>, <em class="sig-param">x</em>, <em class="sig-param">alpha</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#ReverseLayerF.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.ReverseLayerF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.adversarial_branch.SpeakerAdv">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.adversarial_branch.</code><code class="sig-name descname">SpeakerAdv</code><span class="sig-paren">(</span><em class="sig-param">odim</em>, <em class="sig-param">eprojs</em>, <em class="sig-param">advunits</em>, <em class="sig-param">advlayers</em>, <em class="sig-param">dropout_rate=0.2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#SpeakerAdv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.SpeakerAdv" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Speaker adversarial module
:param int odim: dimension of outputs
:param int eprojs: number of encoder projection units
:param float dropout_rate: dropout rate (0.0 ~ 1.0)</p>
<dl class="attribute">
<dt id="espnet2.asr.adversarial_branch.SpeakerAdv.advnet">
<code class="sig-name descname">advnet</code><em class="property"> = None</em><a class="headerlink" href="#espnet2.asr.adversarial_branch.SpeakerAdv.advnet" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>linears = [torch.nn.Linear(eprojs, advunits), torch.nn.ReLU(),</dt><dd><p>torch.nn.Dropout(p=dropout_rate)]</p>
</dd>
<dt>for l in six.moves.range(1, self.advlayers):</dt><dd><dl class="simple">
<dt>linears.extend([torch.nn.Linear(advunits, advunits),</dt><dd><p>torch.nn.ReLU(), torch.nn.Dropout(p=dropout_rate)])</p>
</dd>
</dl>
</dd>
</dl>
<p>self.advnet = torch.nn.Sequential(<a href="#id4"><span class="problematic" id="id5">*</span></a>linears)</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.adversarial_branch.SpeakerAdv.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">text_length</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#SpeakerAdv.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.SpeakerAdv.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Adversarial branch forward
:param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D)
:param torch.Tensor hlens: batch of lengths of hidden state sequences (B)
:param torch.Tensor speech_length: list of speaker index so that we can create a per frame label to used for y_adv by extending the tensor to a dimnetion of (B, average_sequence_length) with each frame  (B)
:param torch.Tensor y_adv: batch of speaker class (B, #Speakers)
:return: loss value
:rtype: torch.Tensor
:return: accuracy
:rtype: float</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.adversarial_branch.SpeakerAdv.zero_state">
<code class="sig-name descname">zero_state</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#SpeakerAdv.zero_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.SpeakerAdv.zero_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.adversarial_branch.th_accuracy">
<code class="sig-prename descclassname">espnet2.asr.adversarial_branch.</code><code class="sig-name descname">th_accuracy</code><span class="sig-paren">(</span><em class="sig-param">pad_outputs</em>, <em class="sig-param">pad_targets</em>, <em class="sig-param">ignore_label</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#th_accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.th_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to calculate accuracy
:param torch.Tensor pad_outputs: prediction tensors (B*Lmax, D)
:param torch.Tensor pad_targets: target tensors (B, Lmax, D)
:param int ignore_label: ignore label id
:retrun: accuracy value (0.0 - 1.0)
:rtype: float</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.asr.adversarial_branch.to_cuda">
<code class="sig-prename descclassname">espnet2.asr.adversarial_branch.</code><code class="sig-name descname">to_cuda</code><span class="sig-paren">(</span><em class="sig-param">m</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/adversarial_branch.html#to_cuda"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.adversarial_branch.to_cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to send tensor into corresponding device
:param torch.nn.Module m: torch module
:param torch.Tensor x: torch tensor
:return: torch tensor located in the same place as torch module
:rtype: torch.Tensor</p>
</dd></dl>

</section>
<section id="espnet2-asr-ctc">
<span id="id6"></span><h2>espnet2.asr.ctc<a class="headerlink" href="#espnet2-asr-ctc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.ctc"></span><dl class="class">
<dt id="espnet2.asr.ctc.CTC">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.ctc.</code><code class="sig-name descname">CTC</code><span class="sig-paren">(</span><em class="sig-param">odim: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">ctc_type: str = 'builtin'</em>, <em class="sig-param">reduce: bool = True</em>, <em class="sig-param">ignore_nan_grad: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>CTC module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>odim</strong> – dimension of outputs</p></li>
<li><p><strong>encoder_output_size</strong> – number of encoder projection units</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate (0.0 ~ 1.0)</p></li>
<li><p><strong>ctc_type</strong> – builtin or warpctc</p></li>
<li><p><strong>reduce</strong> – reduce the CTC loss into a scalar</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.ctc.CTC.argmax">
<code class="sig-name descname">argmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>argmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>torch.Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>argmax applied 2d tensor (B, Tmax)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">ys_pad</em>, <em class="sig-param">ys_lens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate CTC loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – batch of padded hidden state sequences (B, Tmax, D)</p></li>
<li><p><strong>hlens</strong> – batch of lengths of hidden state sequences (B)</p></li>
<li><p><strong>ys_pad</strong> – batch of padded character id sequence tensor (B, Lmax)</p></li>
<li><p><strong>ys_lens</strong> – batch of lengths of character sequence (B)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.log_softmax">
<code class="sig-name descname">log_softmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log softmax applied 3d tensor (B, Tmax, odim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.loss_fn">
<code class="sig-name descname">loss_fn</code><span class="sig-paren">(</span><em class="sig-param">th_pred</em>, <em class="sig-param">th_target</em>, <em class="sig-param">th_ilen</em>, <em class="sig-param">th_olen</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.loss_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.loss_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.ctc.CTC.softmax">
<code class="sig-name descname">softmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/ctc.html#CTC.softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.ctc.CTC.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>softmax applied 3d tensor (B, Tmax, odim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-init">
<span id="id7"></span><h2>espnet2.asr.__init__<a class="headerlink" href="#espnet2-asr-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.__init__"></span></section>
<section id="espnet2-asr-transducer-transducer-decoder">
<span id="id8"></span><h2>espnet2.asr.transducer.transducer_decoder<a class="headerlink" href="#espnet2-asr-transducer-transducer-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.transducer_decoder"></span><p>(RNN-)Transducer decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.transducer_decoder.</code><code class="sig-name descname">TransducerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">num_layers: int = 1</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">dropout_embed: float = 0.0</em>, <em class="sig-param">embed_pad: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<p>(RNN-)Transducer decoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – Output dimension.</p></li>
<li><p><strong>layers_type</strong> – (RNN-)Decoder layers type.</p></li>
<li><p><strong>num_layers</strong> – Number of decoder layers.</p></li>
<li><p><strong>hidden_size</strong> – Number of decoder units per layer.</p></li>
<li><p><strong>dropout</strong> – Dropout rate for decoder layers.</p></li>
<li><p><strong>dropout_embed</strong> – Dropout rate for embedding layer.</p></li>
<li><p><strong>embed_pad</strong> – Embed/Blank symbol ID.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.batch_score">
<code class="sig-name descname">batch_score</code><span class="sig-paren">(</span><em class="sig-param">hyps: Union[List[espnet2.asr.transducer.beam_search_transducer.Hypothesis], List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]], dec_states: Tuple[torch.Tensor, Optional[torch.Tensor]], cache: Dict[str, Any], use_lm: bool</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.batch_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.batch_score" title="Permalink to this definition">¶</a></dt>
<dd><p>One-step forward hypotheses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hyps</strong> – Hypotheses.</p></li>
<li><p><strong>states</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
<li><p><strong>cache</strong> – Pairs of (dec_out, dec_states) for each label sequences. (keys)</p></li>
<li><p><strong>use_lm</strong> – Whether to compute label ID sequences for LM.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder output sequences. (B, D_dec)
dec_states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))
lm_labels: Label ID sequences for LM. (B,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dec_out</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.create_batch_states">
<code class="sig-name descname">create_batch_states</code><span class="sig-paren">(</span><em class="sig-param">states: Tuple[torch.Tensor, Optional[torch.Tensor]], new_states: List[Tuple[torch.Tensor, Optional[torch.Tensor]]], check_list: Optional[List] = None</em><span class="sig-paren">)</span> &#x2192; List[Tuple[torch.Tensor, Optional[torch.Tensor]]]<a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.create_batch_states"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.create_batch_states" title="Permalink to this definition">¶</a></dt>
<dd><p>Create decoder hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>states</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
<li><p><strong>new_states</strong> – Decoder hidden states. [N x ((1, D_dec), (1, D_dec))]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>states</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">labels: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode source label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>labels</strong> – Label ID sequences. (B, L)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder output sequences. (B, T, U, D_dec)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dec_out</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.init_state">
<code class="sig-name descname">init_state</code><span class="sig-paren">(</span><em class="sig-param">batch_size: int</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[None._VariableFunctionsClass.tensor]]<a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.init_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.init_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize decoder states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_size</strong> – Batch size.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Initial decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.rnn_forward">
<code class="sig-name descname">rnn_forward</code><span class="sig-paren">(</span><em class="sig-param">sequence: torch.Tensor, state: Tuple[torch.Tensor, Optional[torch.Tensor]]</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]<a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.rnn_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.rnn_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode source label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequence</strong> – RNN input sequences. (B, D_emb)</p></li>
<li><p><strong>state</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>RNN output sequences. (B, D_dec)
(h_next, c_next): Decoder hidden states. (N, B, D_dec), (N, B, D_dec))</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sequence</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">hyp: espnet2.asr.transducer.beam_search_transducer.Hypothesis, cache: Dict[str, Any]</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>One-step forward hypothesis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hyp</strong> – Hypothesis.</p></li>
<li><p><strong>cache</strong> – Pairs of (dec_out, state) for each label sequence. (key)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder output sequence. (1, D_dec)
new_state: Decoder hidden states. ((N, 1, D_dec), (N, 1, D_dec))
label: Label ID for LM. (1,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dec_out</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.select_state">
<code class="sig-name descname">select_state</code><span class="sig-paren">(</span><em class="sig-param">states: Tuple[torch.Tensor, Optional[torch.Tensor]], idx: int</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.select_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.select_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get specified ID state from decoder hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>states</strong> – Decoder hidden states. ((N, B, D_dec), (N, B, D_dec))</p></li>
<li><p><strong>idx</strong> – State ID to extract.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Decoder hidden state for given ID.</dt><dd><p>((N, 1, D_dec), (N, 1, D_dec))</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.transducer_decoder.TransducerDecoder.set_device">
<code class="sig-name descname">set_device</code><span class="sig-paren">(</span><em class="sig-param">device: torch.device</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/transducer_decoder.html#TransducerDecoder.set_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.transducer_decoder.TransducerDecoder.set_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Set GPU device to use.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – Device ID.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-transducer-joint-network">
<span id="id9"></span><h2>espnet2.asr.transducer.joint_network<a class="headerlink" href="#espnet2-asr-transducer-joint-network" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.joint_network"></span><p>Transducer joint network implementation.</p>
<dl class="class">
<dt id="espnet2.asr.transducer.joint_network.JointNetwork">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.joint_network.</code><code class="sig-name descname">JointNetwork</code><span class="sig-paren">(</span><em class="sig-param">joint_output_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">decoder_output_size: int</em>, <em class="sig-param">joint_space_size: int = 256</em>, <em class="sig-param">joint_activation_type: str = 'tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/joint_network.html#JointNetwork"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.joint_network.JointNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Transducer joint network module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>joint_output_size</strong> – Joint network output dimension</p></li>
<li><p><strong>encoder_output_size</strong> – Encoder output dimension.</p></li>
<li><p><strong>decoder_output_size</strong> – Decoder output dimension.</p></li>
<li><p><strong>joint_space_size</strong> – Dimension of joint space.</p></li>
<li><p><strong>joint_activation_type</strong> – Type of activation for joint network.</p></li>
</ul>
</dd>
</dl>
<p>Joint network initializer.</p>
<dl class="method">
<dt id="espnet2.asr.transducer.joint_network.JointNetwork.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em>, <em class="sig-param">dec_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/transducer/joint_network.html#JointNetwork.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.joint_network.JointNetwork.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Joint computation of encoder and decoder hidden state sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>enc_out</strong> – Expanded encoder output state sequences (B, T, 1, D_enc)</p></li>
<li><p><strong>dec_out</strong> – Expanded decoder output state sequences (B, 1, U, D_dec)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Joint output state sequences. (B, T, U, D_out)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>joint_out</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-transducer-error-calculator">
<span id="id10"></span><h2>espnet2.asr.transducer.error_calculator<a class="headerlink" href="#espnet2-asr-transducer-error-calculator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.error_calculator"></span><p>Error Calculator module for Transducer.</p>
<dl class="class">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.error_calculator.</code><code class="sig-name descname">ErrorCalculatorTransducer</code><span class="sig-paren">(</span><em class="sig-param">decoder: espnet2.asr.decoder.abs_decoder.AbsDecoder, joint_network: torch.nn.modules.module.Module, token_list: List[int], sym_space: str, sym_blank: str, report_cer: bool = False, report_wer: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Calculate CER and WER for transducer models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decoder</strong> – Decoder module.</p></li>
<li><p><strong>token_list</strong> – List of tokens.</p></li>
<li><p><strong>sym_space</strong> – Space symbol.</p></li>
<li><p><strong>sym_blank</strong> – Blank symbol.</p></li>
<li><p><strong>report_cer</strong> – Whether to compute CER.</p></li>
<li><p><strong>report_wer</strong> – Whether to compute WER.</p></li>
</ul>
</dd>
</dl>
<p>Construct an ErrorCalculatorTransducer.</p>
<dl class="method">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_cer">
<code class="sig-name descname">calculate_cer</code><span class="sig-paren">(</span><em class="sig-param">char_pred: torch.Tensor</em>, <em class="sig-param">char_target: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer.calculate_cer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_cer" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate sentence-level CER score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>char_pred</strong> – Prediction character sequences. (B, ?)</p></li>
<li><p><strong>char_target</strong> – Target character sequences. (B, ?)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average sentence-level CER score.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_wer">
<code class="sig-name descname">calculate_wer</code><span class="sig-paren">(</span><em class="sig-param">char_pred: torch.Tensor</em>, <em class="sig-param">char_target: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer.calculate_wer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.calculate_wer" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate sentence-level WER score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>char_pred</strong> – Prediction character sequences. (B, ?)</p></li>
<li><p><strong>char_target</strong> – Target character sequences. (B, ?)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average sentence-level WER score</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.convert_to_char">
<code class="sig-name descname">convert_to_char</code><span class="sig-paren">(</span><em class="sig-param">pred: torch.Tensor</em>, <em class="sig-param">target: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[List, List]<a class="reference internal" href="../_modules/espnet2/asr/transducer/error_calculator.html#ErrorCalculatorTransducer.convert_to_char"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.error_calculator.ErrorCalculatorTransducer.convert_to_char" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert label ID sequences to character sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – Prediction label ID sequences. (B, U)</p></li>
<li><p><strong>target</strong> – Target label ID sequences. (B, L)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Prediction character sequences. (B, ?)
char_target: Target character sequences. (B, ?)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>char_pred</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-transducer-utils">
<span id="id11"></span><h2>espnet2.asr.transducer.utils<a class="headerlink" href="#espnet2-asr-transducer-utils" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.utils"></span><p>Utility functions for Transducer models.</p>
<dl class="function">
<dt id="espnet2.asr.transducer.utils.get_transducer_task_io">
<code class="sig-prename descclassname">espnet2.asr.transducer.utils.</code><code class="sig-name descname">get_transducer_task_io</code><span class="sig-paren">(</span><em class="sig-param">labels: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ignore_id: int = -1</em>, <em class="sig-param">blank_id: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/utils.html#get_transducer_task_io"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.utils.get_transducer_task_io" title="Permalink to this definition">¶</a></dt>
<dd><p>Get Transducer loss I/O.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels</strong> – Label ID sequences. (B, L)</p></li>
<li><p><strong>encoder_out_lens</strong> – Encoder output lengths. (B,)</p></li>
<li><p><strong>ignore_id</strong> – Padding symbol ID.</p></li>
<li><p><strong>blank_id</strong> – Blank symbol ID.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoder inputs. (B, U)
target: Target label ID sequences. (B, U)
t_len: Time lengths. (B,)
u_len: Label lengths. (B,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>decoder_in</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="espnet2-asr-transducer-beam-search-transducer">
<span id="id12"></span><h2>espnet2.asr.transducer.beam_search_transducer<a class="headerlink" href="#espnet2-asr-transducer-beam-search-transducer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.beam_search_transducer"></span><p>Search algorithms for Transducer models.</p>
<dl class="class">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.beam_search_transducer.</code><code class="sig-name descname">BeamSearchTransducer</code><span class="sig-paren">(</span><em class="sig-param">decoder: espnet2.asr.decoder.abs_decoder.AbsDecoder</em>, <em class="sig-param">joint_network: espnet2.asr.transducer.joint_network.JointNetwork</em>, <em class="sig-param">beam_size: int</em>, <em class="sig-param">lm: torch.nn.modules.module.Module = None</em>, <em class="sig-param">lm_weight: float = 0.1</em>, <em class="sig-param">search_type: str = 'default'</em>, <em class="sig-param">max_sym_exp: int = 2</em>, <em class="sig-param">u_max: int = 50</em>, <em class="sig-param">nstep: int = 1</em>, <em class="sig-param">prefix_alpha: int = 1</em>, <em class="sig-param">expansion_gamma: int = 2.3</em>, <em class="sig-param">expansion_beta: int = 2</em>, <em class="sig-param">score_norm: bool = True</em>, <em class="sig-param">nbest: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Beam search implementation for Transducer.</p>
<p>Initialize Transducer search module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decoder</strong> – Decoder module.</p></li>
<li><p><strong>joint_network</strong> – Joint network module.</p></li>
<li><p><strong>beam_size</strong> – Beam size.</p></li>
<li><p><strong>lm</strong> – LM class.</p></li>
<li><p><strong>lm_weight</strong> – LM weight for soft fusion.</p></li>
<li><p><strong>search_type</strong> – Search algorithm to use during inference.</p></li>
<li><p><strong>max_sym_exp</strong> – Number of maximum symbol expansions at each time step. (TSD)</p></li>
<li><p><strong>u_max</strong> – Maximum output sequence length. (ALSD)</p></li>
<li><p><strong>nstep</strong> – Number of maximum expansion steps at each time step. (NSC/mAES)</p></li>
<li><p><strong>prefix_alpha</strong> – Maximum prefix length in prefix search. (NSC/mAES)</p></li>
<li><p><strong>expansion_beta</strong> – Number of additional candidates for expanded hypotheses selection. (mAES)</p></li>
<li><p><strong>expansion_gamma</strong> – Allowed logp difference for prune-by-value method. (mAES)</p></li>
<li><p><strong>score_norm</strong> – Normalize final scores by length. (“default”)</p></li>
<li><p><strong>nbest</strong> – Number of final hypothesis.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.align_length_sync_decoding">
<code class="sig-name descname">align_length_sync_decoding</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.align_length_sync_decoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.align_length_sync_decoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Alignment-length synchronous beam search implementation.</p>
<p>Based on <a class="reference external" href="https://ieeexplore.ieee.org/document/9053040">https://ieeexplore.ieee.org/document/9053040</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>h</strong> – Encoder output sequences. (T, D)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.default_beam_search">
<code class="sig-name descname">default_beam_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.default_beam_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.default_beam_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Beam search implementation.</p>
<p>Modified from <a class="reference external" href="https://arxiv.org/pdf/1211.3711.pdf">https://arxiv.org/pdf/1211.3711.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.greedy_search">
<code class="sig-name descname">greedy_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.greedy_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.greedy_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Greedy search implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D_enc)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1-best hypotheses.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>hyp</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.modified_adaptive_expansion_search">
<code class="sig-name descname">modified_adaptive_expansion_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.modified_adaptive_expansion_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.modified_adaptive_expansion_search" title="Permalink to this definition">¶</a></dt>
<dd><p>It’s the modified Adaptive Expansion Search (mAES) implementation.</p>
<p>Based on/modified from <a class="reference external" href="https://ieeexplore.ieee.org/document/9250505">https://ieeexplore.ieee.org/document/9250505</a> and NSC.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D_enc)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.nsc_beam_search">
<code class="sig-name descname">nsc_beam_search</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.nsc_beam_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.nsc_beam_search" title="Permalink to this definition">¶</a></dt>
<dd><p>N-step constrained beam search implementation.</p>
<p>Based on/Modified from <a class="reference external" href="https://arxiv.org/pdf/2002.03577.pdf">https://arxiv.org/pdf/2002.03577.pdf</a>.
Please reference ESPnet (b-flo, PR #2444) for any usage outside ESPnet
until further modifications.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D_enc)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.prefix_search">
<code class="sig-name descname">prefix_search</code><span class="sig-paren">(</span><em class="sig-param">hyps: List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis], enc_out_t: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.prefix_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.prefix_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Prefix search for NSC and mAES strategies.</p>
<p>Based on <a class="reference external" href="https://arxiv.org/pdf/1211.3711.pdf">https://arxiv.org/pdf/1211.3711.pdf</a></p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.sort_nbest">
<code class="sig-name descname">sort_nbest</code><span class="sig-paren">(</span><em class="sig-param">hyps: Union[List[espnet2.asr.transducer.beam_search_transducer.Hypothesis], List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]]</em><span class="sig-paren">)</span> &#x2192; Union[List[espnet2.asr.transducer.beam_search_transducer.Hypothesis], List[espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis]]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.sort_nbest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.sort_nbest" title="Permalink to this definition">¶</a></dt>
<dd><p>Sort hypotheses by score or score given sequence length.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyps</strong> – Hypothesis.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Sorted hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>hyps</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.time_sync_decoding">
<code class="sig-name descname">time_sync_decoding</code><span class="sig-paren">(</span><em class="sig-param">enc_out: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; List[espnet2.asr.transducer.beam_search_transducer.Hypothesis]<a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#BeamSearchTransducer.time_sync_decoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.BeamSearchTransducer.time_sync_decoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Time synchronous beam search implementation.</p>
<p>Based on <a class="reference external" href="https://ieeexplore.ieee.org/document/9053040">https://ieeexplore.ieee.org/document/9053040</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enc_out</strong> – Encoder output sequence. (T, D)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best hypothesis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nbest_hyps</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.beam_search_transducer.</code><code class="sig-name descname">ExtendedHypothesis</code><span class="sig-paren">(</span><em class="sig-param">score: float, yseq: List[int], dec_state: Union[Tuple[torch.Tensor, Optional[torch.Tensor]], List[Optional[torch.Tensor]], torch.Tensor], lm_state: Union[Dict[str, Any], List[Any]] = None, dec_out: List[torch.Tensor] = None, lm_scores: torch.Tensor = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#ExtendedHypothesis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.transducer.beam_search_transducer.Hypothesis" title="espnet2.asr.transducer.beam_search_transducer.Hypothesis"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.transducer.beam_search_transducer.Hypothesis</span></code></a></p>
<p>Extended hypothesis definition for NSC beam search and mAES.</p>
<dl class="attribute">
<dt id="espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.dec_out">
<code class="sig-name descname">dec_out</code><em class="property"> = None</em><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.dec_out" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.lm_scores">
<code class="sig-name descname">lm_scores</code><em class="property"> = None</em><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.ExtendedHypothesis.lm_scores" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.transducer.beam_search_transducer.Hypothesis">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.transducer.beam_search_transducer.</code><code class="sig-name descname">Hypothesis</code><span class="sig-paren">(</span><em class="sig-param">score: float, yseq: List[int], dec_state: Union[Tuple[torch.Tensor, Optional[torch.Tensor]], List[Optional[torch.Tensor]], torch.Tensor], lm_state: Union[Dict[str, Any], List[Any]] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/transducer/beam_search_transducer.html#Hypothesis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.Hypothesis" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Default hypothesis definition for Transducer search algorithms.</p>
<dl class="attribute">
<dt id="espnet2.asr.transducer.beam_search_transducer.Hypothesis.lm_state">
<code class="sig-name descname">lm_state</code><em class="property"> = None</em><a class="headerlink" href="#espnet2.asr.transducer.beam_search_transducer.Hypothesis.lm_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-transducer-init">
<span id="id13"></span><h2>espnet2.asr.transducer.__init__<a class="headerlink" href="#espnet2-asr-transducer-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.transducer.__init__"></span></section>
<section id="espnet2-asr-preencoder-abs-preencoder">
<span id="id14"></span><h2>espnet2.asr.preencoder.abs_preencoder<a class="headerlink" href="#espnet2-asr-preencoder-abs-preencoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.abs_preencoder"></span><dl class="class">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.abs_preencoder.</code><code class="sig-name descname">AbsPreEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/abs_preencoder.html#AbsPreEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-preencoder-sinc">
<span id="id15"></span><h2>espnet2.asr.preencoder.sinc<a class="headerlink" href="#espnet2-asr-preencoder-sinc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.sinc"></span><p>Sinc convolutions for raw audio input.</p>
<dl class="class">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.sinc.</code><code class="sig-name descname">LightweightSincConvs</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str</em>, <em class="sig-param">float] = 16000</em>, <em class="sig-param">in_channels: int = 1</em>, <em class="sig-param">out_channels: int = 256</em>, <em class="sig-param">activation_type: str = 'leakyrelu'</em>, <em class="sig-param">dropout_type: str = 'dropout'</em>, <em class="sig-param">windowing_type: str = 'hamming'</em>, <em class="sig-param">scale_type: str = 'mel'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder</span></code></a></p>
<p>Lightweight Sinc Convolutions.</p>
<p>Instead of using precomputed features, end-to-end speech recognition
can also be done directly from raw audio using sinc convolutions, as
described in “Lightweight End-to-End Speech Recognition from Raw Audio
Data Using Sinc-Convolutions” by Kürzinger et al.
<a class="reference external" href="https://arxiv.org/abs/2010.07597">https://arxiv.org/abs/2010.07597</a></p>
<p>To use Sinc convolutions in your model instead of the default f-bank
frontend, set this module as your pre-encoder with <cite>preencoder: sinc</cite>
and use the input of the sliding window frontend with
<cite>frontend: sliding_window</cite> in your yaml configuration file.
So that the process flow is:</p>
<p>Frontend (SlidingWindow) -&gt; SpecAug -&gt; Normalization -&gt;
Pre-encoder (LightweightSincConvs) -&gt; Encoder -&gt; Decoder</p>
<p>Note that this method also performs data augmentation in time domain
(vs. in spectral domain in the default frontend).
Use <cite>plot_sinc_filters.py</cite> to visualize the learned Sinc filters.</p>
<p>Initialize the module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fs</strong> – Sample rate.</p></li>
<li><p><strong>in_channels</strong> – Number of input channels.</p></li>
<li><p><strong>out_channels</strong> – Number of output channels (for each input channel).</p></li>
<li><p><strong>activation_type</strong> – Choice of activation function.</p></li>
<li><p><strong>dropout_type</strong> – Choice of dropout function.</p></li>
<li><p><strong>windowing_type</strong> – Choice of windowing function.</p></li>
<li><p><strong>scale_type</strong> – Choice of filter-bank initialization scale.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.espnet_initialization_fn">
<code class="sig-name descname">espnet_initialization_fn</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.espnet_initialization_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.espnet_initialization_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize sinc filters with filterbank values.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply Lightweight Sinc Convolutions.</p>
<p>The input shall be formatted as (B, T, C_in, D_in)
with B as batch size, T as time dimension, C_in as channels,
and D_in as feature dimension.</p>
<p>The output will then be (B, T, C_out*D_out)
with C_out and D_out as output dimensions.</p>
<p>The current module structure only handles D_in=400, so that D_out=1.
Remark for the multichannel case: C_out is the number of out_channels
given at initialization multiplied with C_in.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.gen_lsc_block">
<code class="sig-name descname">gen_lsc_block</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int</em>, <em class="sig-param">out_channels: int</em>, <em class="sig-param">depthwise_kernel_size: int = 9</em>, <em class="sig-param">depthwise_stride: int = 1</em>, <em class="sig-param">depthwise_groups=None</em>, <em class="sig-param">pointwise_groups=0</em>, <em class="sig-param">dropout_probability: float = 0.15</em>, <em class="sig-param">avgpool=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.gen_lsc_block"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.gen_lsc_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a convolutional block for Lightweight Sinc convolutions.</p>
<p>Each block consists of either a depthwise or a depthwise-separable
convolutions together with dropout, (batch-)normalization layer, and
an optional average-pooling layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> – Number of input channels.</p></li>
<li><p><strong>out_channels</strong> – Number of output channels.</p></li>
<li><p><strong>depthwise_kernel_size</strong> – Kernel size of the depthwise convolution.</p></li>
<li><p><strong>depthwise_stride</strong> – Stride of the depthwise convolution.</p></li>
<li><p><strong>depthwise_groups</strong> – Number of groups of the depthwise convolution.</p></li>
<li><p><strong>pointwise_groups</strong> – Number of groups of the pointwise convolution.</p></li>
<li><p><strong>dropout_probability</strong> – Dropout probability in the block.</p></li>
<li><p><strong>avgpool</strong> – If True, an AvgPool layer is inserted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Neural network building block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Sequential</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.LightweightSincConvs.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#LightweightSincConvs.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.LightweightSincConvs.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the output size.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.preencoder.sinc.SpatialDropout">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.sinc.</code><code class="sig-name descname">SpatialDropout</code><span class="sig-paren">(</span><em class="sig-param">dropout_probability: float = 0.15</em>, <em class="sig-param">shape: Union[tuple</em>, <em class="sig-param">list</em>, <em class="sig-param">None] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#SpatialDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.SpatialDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Spatial dropout module.</p>
<p>Apply dropout to full channels on tensors of input (B, C, D)</p>
<p>Initialize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout_probability</strong> – Dropout probability.</p></li>
<li><p><strong>shape</strong> (<em>tuple</em><em>, </em><em>list</em>) – Shape of input tensors.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.preencoder.sinc.SpatialDropout.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/asr/preencoder/sinc.html#SpatialDropout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.sinc.SpatialDropout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward of spatial dropout module.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-preencoder-linear">
<span id="id16"></span><h2>espnet2.asr.preencoder.linear<a class="headerlink" href="#espnet2-asr-preencoder-linear" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.linear"></span><p>Linear Projection.</p>
<dl class="class">
<dt id="espnet2.asr.preencoder.linear.LinearProjection">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.preencoder.linear.</code><code class="sig-name descname">LinearProjection</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/preencoder/linear.html#LinearProjection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.linear.LinearProjection" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder" title="espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.preencoder.abs_preencoder.AbsPreEncoder</span></code></a></p>
<p>Linear Projection Preencoder.</p>
<p>Initialize the module.</p>
<dl class="method">
<dt id="espnet2.asr.preencoder.linear.LinearProjection.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/preencoder/linear.html#LinearProjection.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.linear.LinearProjection.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.preencoder.linear.LinearProjection.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/preencoder/linear.html#LinearProjection.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.preencoder.linear.LinearProjection.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the output size.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-preencoder-init">
<span id="id17"></span><h2>espnet2.asr.preencoder.__init__<a class="headerlink" href="#espnet2-asr-preencoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.preencoder.__init__"></span></section>
<section id="espnet2-asr-decoder-mlm-decoder">
<span id="id18"></span><h2>espnet2.asr.decoder.mlm_decoder<a class="headerlink" href="#espnet2-asr-decoder-mlm-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.mlm_decoder"></span><p>Masked LM Decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.mlm_decoder.MLMDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.mlm_decoder.</code><code class="sig-name descname">MLMDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/mlm_decoder.html#MLMDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.mlm_decoder.MLMDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.decoder.mlm_decoder.MLMDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/mlm_decoder.html#MLMDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.mlm_decoder.MLMDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out)
if input_layer == “embed”
input tensor (batch, maxlen_out, #mels) in the other cases</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:
x: decoded token score before softmax (batch, maxlen_out, token)</p>
<blockquote>
<div><p>if use_output_layer is True,</p>
</div></blockquote>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-abs-decoder">
<span id="id19"></span><h2>espnet2.asr.decoder.abs_decoder<a class="headerlink" href="#espnet2-asr-decoder-abs-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.abs_decoder"></span><dl class="class">
<dt id="espnet2.asr.decoder.abs_decoder.AbsDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.abs_decoder.</code><code class="sig-name descname">AbsDecoder</code><a class="reference internal" href="../_modules/espnet2/asr/decoder/abs_decoder.html#AbsDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.ScorerInterface" title="espnet.nets.scorer_interface.ScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.ScorerInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.decoder.abs_decoder.AbsDecoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/abs_decoder.html#AbsDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-decoder-transformer-decoder">
<span id="id20"></span><h2>espnet2.asr.decoder.transformer_decoder<a class="headerlink" href="#espnet2-asr-decoder-transformer-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.transformer_decoder"></span><p>Decoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">BaseTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a>, <a class="reference internal" href="espnet.nets.html#espnet.nets.scorer_interface.BatchScorerInterface" title="espnet.nets.scorer_interface.BatchScorerInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.scorer_interface.BatchScorerInterface</span></code></a></p>
<p>Base class of Transfomer decoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – output dim</p></li>
<li><p><strong>encoder_output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>self_attention_dropout_rate</strong> – dropout rate for attention</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>use_output_layer</strong> – whether to use output layer</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.batch_score">
<code class="sig-name descname">batch_score</code><span class="sig-paren">(</span><em class="sig-param">ys: torch.Tensor, states: List[Any], xs: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[Any]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.batch_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.batch_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ys</strong> (<em>torch.Tensor</em>) – torch.int64 prefix tokens (n_batch, ylen).</p></li>
<li><p><strong>states</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>) – Scorer states for prefix tokens.</p></li>
<li><p><strong>xs</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys (n_batch, xlen, n_feat).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>batchfied scores for next token with shape of <cite>(n_batch, n_vocab)</cite>
and next state list for ys.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, List[Any]]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad: torch.Tensor</em>, <em class="sig-param">hlens: torch.Tensor</em>, <em class="sig-param">ys_in_pad: torch.Tensor</em>, <em class="sig-param">ys_in_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>hlens</strong> – (batch)</p></li>
<li><p><strong>ys_in_pad</strong> – input token ids, int64 (batch, maxlen_out)
if input_layer == “embed”
input tensor (batch, maxlen_out, #mels) in the other cases</p></li>
<li><p><strong>ys_in_lens</strong> – (batch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>tuple containing:</p>
<dl class="simple">
<dt>x: decoded token score before softmax (batch, maxlen_out, token)</dt><dd><p>if use_output_layer is True,</p>
</dd>
</dl>
<p>olens: (batch, )</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward_one_step">
<code class="sig-name descname">forward_one_step</code><span class="sig-paren">(</span><em class="sig-param">tgt: torch.Tensor</em>, <em class="sig-param">tgt_mask: torch.Tensor</em>, <em class="sig-param">memory: torch.Tensor</em>, <em class="sig-param">cache: List[torch.Tensor] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, List[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.forward_one_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.forward_one_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward one step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> – input token ids, int64 (batch, maxlen_out)</p></li>
<li><p><strong>tgt_mask</strong> – input token mask,  (batch, maxlen_out)
dtype=torch.uint8 in PyTorch 1.2-
dtype=torch.bool in PyTorch 1.2+ (include 1.2)</p></li>
<li><p><strong>memory</strong> – encoded memory, float32  (batch, maxlen_in, feat)</p></li>
<li><p><strong>cache</strong> – cached output list of (batch, max_time_out-1, size)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>NN output value and cache per <cite>self.decoders</cite>.
y.shape` is (batch, maxlen_out, token)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>y, cache</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">ys</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#BaseTransformerDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">DynamicConvolution2DTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#DynamicConvolution2DTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.DynamicConvolution2DTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">DynamicConvolutionTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#DynamicConvolutionTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.DynamicConvolutionTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">LightweightConvolution2DTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#LightweightConvolution2DTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.LightweightConvolution2DTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">LightweightConvolutionTransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">conv_wshare: int = 4</em>, <em class="sig-param">conv_kernel_length: Sequence[int] = (11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11</em>, <em class="sig-param">11)</em>, <em class="sig-param">conv_usebias: int = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#LightweightConvolutionTransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.LightweightConvolutionTransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet2.asr.decoder.transformer_decoder.TransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.transformer_decoder.</code><code class="sig-name descname">TransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">self_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">src_attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'embed'</em>, <em class="sig-param">use_output_layer: bool = True</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/transformer_decoder.html#TransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.transformer_decoder.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder" title="espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.transformer_decoder.BaseTransformerDecoder</span></code></a></p>
</dd></dl>

</section>
<section id="espnet2-asr-decoder-rnn-decoder">
<span id="id21"></span><h2>espnet2.asr.decoder.rnn_decoder<a class="headerlink" href="#espnet2-asr-decoder-rnn-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.rnn_decoder"></span><dl class="class">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.decoder.rnn_decoder.</code><code class="sig-name descname">RNNDecoder</code><span class="sig-paren">(</span><em class="sig-param">vocab_size: int</em>, <em class="sig-param">encoder_output_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">num_layers: int = 1</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">sampling_probability: float = 0.0</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">context_residual: bool = False</em>, <em class="sig-param">replace_sos: bool = False</em>, <em class="sig-param">num_encs: int = 1</em>, <em class="sig-param">att_conf: dict = {'aconv_chans': 10</em>, <em class="sig-param">'aconv_filts': 100</em>, <em class="sig-param">'adim': 320</em>, <em class="sig-param">'aheads': 4</em>, <em class="sig-param">'atype': 'location'</em>, <em class="sig-param">'awin': 5</em>, <em class="sig-param">'han_conv_chans': -1</em>, <em class="sig-param">'han_conv_filts': 100</em>, <em class="sig-param">'han_dim': 320</em>, <em class="sig-param">'han_heads': 4</em>, <em class="sig-param">'han_mode': False</em>, <em class="sig-param">'han_type': None</em>, <em class="sig-param">'han_win': 5</em>, <em class="sig-param">'num_att': 1</em>, <em class="sig-param">'num_encs': 1}</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.decoder.abs_decoder.AbsDecoder" title="espnet2.asr.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">ys_in_pad</em>, <em class="sig-param">ys_in_lens</em>, <em class="sig-param">strm_idx=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.init_state">
<code class="sig-name descname">init_state</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.init_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.init_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Get an initial state for decoding (optional).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – The encoded feature tensor</p>
</dd>
</dl>
<p>Returns: initial state</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.rnn_forward">
<code class="sig-name descname">rnn_forward</code><span class="sig-paren">(</span><em class="sig-param">ey</em>, <em class="sig-param">z_list</em>, <em class="sig-param">c_list</em>, <em class="sig-param">z_prev</em>, <em class="sig-param">c_prev</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.rnn_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.rnn_forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">yseq</em>, <em class="sig-param">state</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score new token (required).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>torch.Tensor</em>) – 1D torch.int64 prefix tokens.</p></li>
<li><p><strong>state</strong> – Scorer state for prefix tokens</p></li>
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – The encoder feature that generates ys.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple of</dt><dd><p>scores for next token that has a shape of <cite>(n_vocab)</cite>
and next state for ys</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple[torch.Tensor, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.decoder.rnn_decoder.RNNDecoder.zero_state">
<code class="sig-name descname">zero_state</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#RNNDecoder.zero_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.RNNDecoder.zero_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.decoder.rnn_decoder.build_attention_list">
<code class="sig-prename descclassname">espnet2.asr.decoder.rnn_decoder.</code><code class="sig-name descname">build_attention_list</code><span class="sig-paren">(</span><em class="sig-param">eprojs: int</em>, <em class="sig-param">dunits: int</em>, <em class="sig-param">atype: str = 'location'</em>, <em class="sig-param">num_att: int = 1</em>, <em class="sig-param">num_encs: int = 1</em>, <em class="sig-param">aheads: int = 4</em>, <em class="sig-param">adim: int = 320</em>, <em class="sig-param">awin: int = 5</em>, <em class="sig-param">aconv_chans: int = 10</em>, <em class="sig-param">aconv_filts: int = 100</em>, <em class="sig-param">han_mode: bool = False</em>, <em class="sig-param">han_type=None</em>, <em class="sig-param">han_heads: int = 4</em>, <em class="sig-param">han_dim: int = 320</em>, <em class="sig-param">han_conv_chans: int = -1</em>, <em class="sig-param">han_conv_filts: int = 100</em>, <em class="sig-param">han_win: int = 5</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/decoder/rnn_decoder.html#build_attention_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.decoder.rnn_decoder.build_attention_list" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-decoder-init">
<span id="id22"></span><h2>espnet2.asr.decoder.__init__<a class="headerlink" href="#espnet2-asr-decoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.decoder.__init__"></span></section>
<section id="espnet2-asr-specaug-abs-specaug">
<span id="id23"></span><h2>espnet2.asr.specaug.abs_specaug<a class="headerlink" href="#espnet2-asr-specaug-abs-specaug" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.abs_specaug"></span><dl class="class">
<dt id="espnet2.asr.specaug.abs_specaug.AbsSpecAug">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.specaug.abs_specaug.</code><code class="sig-name descname">AbsSpecAug</code><a class="reference internal" href="../_modules/espnet2/asr/specaug/abs_specaug.html#AbsSpecAug"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Abstract class for the augmentation of spectrogram</p>
<p>The process-flow:</p>
<p>Frontend  -&gt; SpecAug -&gt; Normalization -&gt; Encoder -&gt; Decoder</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.specaug.abs_specaug.AbsSpecAug.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor</em>, <em class="sig-param">x_lengths: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/specaug/abs_specaug.html#AbsSpecAug.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-specaug-specaug">
<span id="id24"></span><h2>espnet2.asr.specaug.specaug<a class="headerlink" href="#espnet2-asr-specaug-specaug" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.specaug"></span><p>SpecAugment module.</p>
<dl class="class">
<dt id="espnet2.asr.specaug.specaug.SpecAug">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.specaug.specaug.</code><code class="sig-name descname">SpecAug</code><span class="sig-paren">(</span><em class="sig-param">apply_time_warp: bool = True, time_warp_window: int = 5, time_warp_mode: str = 'bicubic', apply_freq_mask: bool = True, freq_mask_width_range: Union[int, Sequence[int]] = (0, 20), num_freq_mask: int = 2, apply_time_mask: bool = True, time_mask_width_range: Union[int, Sequence[int], None] = None, time_mask_width_ratio_range: Union[float, Sequence[float], None] = None, num_time_mask: int = 2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/specaug/specaug.html#SpecAug"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.specaug.SpecAug" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.specaug.abs_specaug.AbsSpecAug" title="espnet2.asr.specaug.abs_specaug.AbsSpecAug"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.specaug.abs_specaug.AbsSpecAug</span></code></a></p>
<p>Implementation of SpecAug.</p>
<dl>
<dt>Reference:</dt><dd><p>Daniel S. Park et al.
“SpecAugment: A Simple Data</p>
<blockquote>
<div><p>Augmentation Method for Automatic Speech Recognition”</p>
</div></blockquote>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using cuda mode, time_warp doesn’t have reproducibility
due to <cite>torch.nn.functional.interpolate</cite>.</p>
</div>
<dl class="method">
<dt id="espnet2.asr.specaug.specaug.SpecAug.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">x_lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/specaug/specaug.html#SpecAug.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.specaug.specaug.SpecAug.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-specaug-init">
<span id="id25"></span><h2>espnet2.asr.specaug.__init__<a class="headerlink" href="#espnet2-asr-specaug-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.specaug.__init__"></span></section>
<section id="espnet2-asr-frontend-abs-frontend">
<span id="id26"></span><h2>espnet2.asr.frontend.abs_frontend<a class="headerlink" href="#espnet2-asr-frontend-abs-frontend" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.abs_frontend"></span><dl class="class">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.abs_frontend.</code><code class="sig-name descname">AbsFrontend</code><a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.abs_frontend.AbsFrontend.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/abs_frontend.html#AbsFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-default">
<span id="id27"></span><h2>espnet2.asr.frontend.default<a class="headerlink" href="#espnet2-asr-frontend-default" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.default"></span><dl class="class">
<dt id="espnet2.asr.frontend.default.DefaultFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.default.</code><code class="sig-name descname">DefaultFrontend</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str] = 16000</em>, <em class="sig-param">n_fft: int = 512</em>, <em class="sig-param">win_length: int = None</em>, <em class="sig-param">hop_length: int = 128</em>, <em class="sig-param">window: Optional[str] = 'hann'</em>, <em class="sig-param">center: bool = True</em>, <em class="sig-param">normalized: bool = False</em>, <em class="sig-param">onesided: bool = True</em>, <em class="sig-param">n_mels: int = 80</em>, <em class="sig-param">fmin: int = None</em>, <em class="sig-param">fmax: int = None</em>, <em class="sig-param">htk: bool = False</em>, <em class="sig-param">frontend_conf: Optional[dict] = {'badim': 320</em>, <em class="sig-param">'bdropout_rate': 0.0</em>, <em class="sig-param">'blayers': 3</em>, <em class="sig-param">'bnmask': 2</em>, <em class="sig-param">'bprojs': 320</em>, <em class="sig-param">'btype': 'blstmp'</em>, <em class="sig-param">'bunits': 300</em>, <em class="sig-param">'delay': 3</em>, <em class="sig-param">'ref_channel': -1</em>, <em class="sig-param">'taps': 5</em>, <em class="sig-param">'use_beamformer': False</em>, <em class="sig-param">'use_dnn_mask_for_wpe': True</em>, <em class="sig-param">'use_wpe': False</em>, <em class="sig-param">'wdropout_rate': 0.0</em>, <em class="sig-param">'wlayers': 3</em>, <em class="sig-param">'wprojs': 320</em>, <em class="sig-param">'wtype': 'blstmp'</em>, <em class="sig-param">'wunits': 300}</em>, <em class="sig-param">apply_stft: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Conventional frontend structure for ASR.</p>
<p>Stft -&gt; WPE -&gt; MVDR-Beamformer -&gt; Power-spec -&gt; Mel-Fbank -&gt; CMVN</p>
<dl class="method">
<dt id="espnet2.asr.frontend.default.DefaultFrontend.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.default.DefaultFrontend.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/default.html#DefaultFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.default.DefaultFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-fused">
<span id="id28"></span><h2>espnet2.asr.frontend.fused<a class="headerlink" href="#espnet2-asr-frontend-fused" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.fused"></span><dl class="class">
<dt id="espnet2.asr.frontend.fused.FusedFrontends">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.fused.</code><code class="sig-name descname">FusedFrontends</code><span class="sig-paren">(</span><em class="sig-param">frontends=None</em>, <em class="sig-param">align_method='linear_projection'</em>, <em class="sig-param">proj_dim=100</em>, <em class="sig-param">fs=16000</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/fused.html#FusedFrontends"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.fused.FusedFrontends" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<dl class="method">
<dt id="espnet2.asr.frontend.fused.FusedFrontends.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/fused.html#FusedFrontends.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.fused.FusedFrontends.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.fused.FusedFrontends.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/fused.html#FusedFrontends.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.fused.FusedFrontends.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-frontend-s3prl">
<span id="id29"></span><h2>espnet2.asr.frontend.s3prl<a class="headerlink" href="#espnet2-asr-frontend-s3prl" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.s3prl"></span><dl class="class">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.s3prl.</code><code class="sig-name descname">S3prlFrontend</code><span class="sig-paren">(</span><em class="sig-param">fs: Union[int</em>, <em class="sig-param">str] = 16000</em>, <em class="sig-param">frontend_conf: Optional[dict] = {'badim': 320</em>, <em class="sig-param">'bdropout_rate': 0.0</em>, <em class="sig-param">'blayers': 3</em>, <em class="sig-param">'bnmask': 2</em>, <em class="sig-param">'bprojs': 320</em>, <em class="sig-param">'btype': 'blstmp'</em>, <em class="sig-param">'bunits': 300</em>, <em class="sig-param">'delay': 3</em>, <em class="sig-param">'ref_channel': -1</em>, <em class="sig-param">'taps': 5</em>, <em class="sig-param">'use_beamformer': False</em>, <em class="sig-param">'use_dnn_mask_for_wpe': True</em>, <em class="sig-param">'use_wpe': False</em>, <em class="sig-param">'wdropout_rate': 0.0</em>, <em class="sig-param">'wlayers': 3</em>, <em class="sig-param">'wprojs': 320</em>, <em class="sig-param">'wtype': 'blstmp'</em>, <em class="sig-param">'wunits': 300}</em>, <em class="sig-param">download_dir: str = None</em>, <em class="sig-param">multilayer_feature: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Speech Pretrained Representation frontend structure for ASR.</p>
<dl class="method">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.s3prl.S3prlFrontend.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#S3prlFrontend.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.S3prlFrontend.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.frontend.s3prl.base_s3prl_setup">
<code class="sig-prename descclassname">espnet2.asr.frontend.s3prl.</code><code class="sig-name descname">base_s3prl_setup</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/s3prl.html#base_s3prl_setup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.s3prl.base_s3prl_setup" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-frontend-init">
<span id="id30"></span><h2>espnet2.asr.frontend.__init__<a class="headerlink" href="#espnet2-asr-frontend-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.__init__"></span></section>
<section id="espnet2-asr-frontend-windowing">
<span id="id31"></span><h2>espnet2.asr.frontend.windowing<a class="headerlink" href="#espnet2-asr-frontend-windowing" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.frontend.windowing"></span><p>Sliding Window for raw audio input data.</p>
<dl class="class">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.frontend.windowing.</code><code class="sig-name descname">SlidingWindow</code><span class="sig-paren">(</span><em class="sig-param">win_length: int = 400</em>, <em class="sig-param">hop_length: int = 160</em>, <em class="sig-param">channels: int = 1</em>, <em class="sig-param">padding: int = None</em>, <em class="sig-param">fs=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.frontend.abs_frontend.AbsFrontend" title="espnet2.asr.frontend.abs_frontend.AbsFrontend"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.frontend.abs_frontend.AbsFrontend</span></code></a></p>
<p>Sliding Window.</p>
<p>Provides a sliding window over a batched continuous raw audio tensor.
Optionally, provides padding (Currently not implemented).
Combine this module with a pre-encoder compatible with raw audio data,
for example Sinc convolutions.</p>
<p>Known issues:
Output length is calculated incorrectly if audio shorter than win_length.
WARNING: trailing values are discarded - padding not implemented yet.
There is currently no additional window function applied to input values.</p>
<p>Initialize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>win_length</strong> – Length of frame.</p></li>
<li><p><strong>hop_length</strong> – Relative starting point of next frame.</p></li>
<li><p><strong>channels</strong> – Number of input channels.</p></li>
<li><p><strong>padding</strong> – Padding (placeholder, currently not implemented).</p></li>
<li><p><strong>fs</strong> – Sampling rate (placeholder for compatibility, not used).</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply a sliding window on the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Input (B, T, C*D) or (B, T*C*D), with D=C=1.</p></li>
<li><p><strong>input_lengths</strong> – Input lengths within batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output with dimensions (B, T, C, D), with D=win_length.
Tensor: Output lengths within batch.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.frontend.windowing.SlidingWindow.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/frontend/windowing.html#SlidingWindow.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.frontend.windowing.SlidingWindow.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Return output length of feature dimension D, i.e. the window length.</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-postencoder-abs-postencoder">
<span id="id32"></span><h2>espnet2.asr.postencoder.abs_postencoder<a class="headerlink" href="#espnet2-asr-postencoder-abs-postencoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.postencoder.abs_postencoder"></span><dl class="class">
<dt id="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.postencoder.abs_postencoder.</code><code class="sig-name descname">AbsPostEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/postencoder/abs_postencoder.html#AbsPostEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/postencoder/abs_postencoder.html#AbsPostEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/postencoder/abs_postencoder.html#AbsPostEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-postencoder-hugging-face-transformers-postencoder">
<span id="id33"></span><h2>espnet2.asr.postencoder.hugging_face_transformers_postencoder<a class="headerlink" href="#espnet2-asr-postencoder-hugging-face-transformers-postencoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.postencoder.hugging_face_transformers_postencoder"></span><p>Hugging Face Transformers PostEncoder.</p>
<dl class="class">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.postencoder.hugging_face_transformers_postencoder.</code><code class="sig-name descname">HuggingFaceTransformersPostEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">model_name_or_path: str</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder" title="espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.postencoder.abs_postencoder.AbsPostEncoder</span></code></a></p>
<p>Hugging Face Transformers PostEncoder.</p>
<p>Initialize the module.</p>
<dl class="method">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">input_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the output size.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/postencoder/hugging_face_transformers_postencoder.html#HuggingFaceTransformersPostEncoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.postencoder.hugging_face_transformers_postencoder.HuggingFaceTransformersPostEncoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-postencoder-init">
<span id="id34"></span><h2>espnet2.asr.postencoder.__init__<a class="headerlink" href="#espnet2-asr-postencoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.postencoder.__init__"></span></section>
<section id="espnet2-asr-encoder-wav2vec2-encoder">
<span id="id35"></span><h2>espnet2.asr.encoder.wav2vec2_encoder<a class="headerlink" href="#espnet2-asr-encoder-wav2vec2-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.wav2vec2_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.wav2vec2_encoder.</code><code class="sig-name descname">FairSeqWav2Vec2Encoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">w2v_url: str</em>, <em class="sig-param">w2v_dir_path: str = './'</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">normalize_before: bool = False</em>, <em class="sig-param">freeze_finetune_updates: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>FairSeq Wav2Vec2 encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>w2v_url</strong> – url to Wav2Vec2.0 pretrained model</p></li>
<li><p><strong>w2v_dir_path</strong> – directory to download the Wav2Vec2.0 pretrained model.</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>finetune_last_n_layers</strong> – last n layers to be finetuned in Wav2Vec2.0
0 means to finetune every layer if freeze_w2v=False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward FairSeqWav2Vec2 Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#FairSeqWav2Vec2Encoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.FairSeqWav2Vec2Encoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.encoder.wav2vec2_encoder.download_w2v">
<code class="sig-prename descclassname">espnet2.asr.encoder.wav2vec2_encoder.</code><code class="sig-name descname">download_w2v</code><span class="sig-paren">(</span><em class="sig-param">model_url</em>, <em class="sig-param">dir_path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/wav2vec2_encoder.html#download_w2v"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.wav2vec2_encoder.download_w2v" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-encoder-transformer-encoder">
<span id="id36"></span><h2>espnet2.asr.encoder.transformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-transformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.transformer_encoder"></span><p>Transformer encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.transformer_encoder.</code><code class="sig-name descname">TransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.PositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">interctc_layer_idx: List[int] = []</em>, <em class="sig-param">interctc_use_conditioning: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Transformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">ctc: espnet2.asr.ctc.CTC = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.transformer_encoder.TransformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/transformer_encoder.html#TransformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.transformer_encoder.TransformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-abs-encoder">
<span id="id37"></span><h2>espnet2.asr.encoder.abs_encoder<a class="headerlink" href="#espnet2-asr-encoder-abs-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.abs_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.abs_encoder.</code><code class="sig-name descname">AbsEncoder</code><a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.abs_encoder.AbsEncoder.output_size">
<em class="property">abstract </em><code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/abs_encoder.html#AbsEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-longformer-encoder">
<span id="id38"></span><h2>espnet2.asr.encoder.longformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-longformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.longformer_encoder"></span><p>Conformer encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.longformer_encoder.LongformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.longformer_encoder.</code><code class="sig-name descname">LongformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int, output_size: int = 256, attention_heads: int = 4, linear_units: int = 2048, num_blocks: int = 6, dropout_rate: float = 0.1, positional_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.0, input_layer: str = 'conv2d', normalize_before: bool = True, concat_after: bool = False, positionwise_layer_type: str = 'linear', positionwise_conv_kernel_size: int = 3, macaron_style: bool = False, rel_pos_type: str = 'legacy', pos_enc_layer_type: str = 'abs_pos', selfattention_layer_type: str = 'lf_selfattn', activation_type: str = 'swish', use_cnn_module: bool = True, zero_triu: bool = False, cnn_module_kernel: int = 31, padding_idx: int = -1, interctc_layer_idx: List[int] = [], interctc_use_conditioning: bool = False, attention_windows: list = [100, 100, 100, 100, 100, 100], attention_dilation: list = [1, 1, 1, 1, 1, 1], attention_mode: str = 'sliding_chunks'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/longformer_encoder.html#LongformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.longformer_encoder.LongformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder" title="espnet2.asr.encoder.conformer_encoder.ConformerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.conformer_encoder.ConformerEncoder</span></code></a></p>
<p>Longformer SA Conformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – Input dimension.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Dimension of attention.</p></li>
<li><p><strong>attention_heads</strong> (<em>int</em>) – The number of heads of multi head attention.</p></li>
<li><p><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em>) – The number of decoder blocks.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</p></li>
<li><p><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</p></li>
<li><p><strong>input_layer</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.nn.Module</em><em>]</em>) – Input layer type.</p></li>
<li><p><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</p></li>
<li><p><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output.
If True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
If False, no additional linear will be applied. i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</p></li>
<li><p><strong>rel_pos_type</strong> (<em>str</em>) – Whether to use the latest relative positional encoding or
the legacy one. The legacy relative positional encoding will be deprecated
in the future. More Details can be found in
<a class="reference external" href="https://github.com/espnet/espnet/pull/2816">https://github.com/espnet/espnet/pull/2816</a>.</p></li>
<li><p><strong>encoder_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</p></li>
<li><p><strong>encoder_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</p></li>
<li><p><strong>activation_type</strong> (<em>str</em>) – Encoder activation function type.</p></li>
<li><p><strong>macaron_style</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</p></li>
<li><p><strong>use_cnn_module</strong> (<em>bool</em>) – Whether to use convolution module.</p></li>
<li><p><strong>zero_triu</strong> (<em>bool</em>) – Whether to zero the upper triangular part of attention matrix.</p></li>
<li><p><strong>cnn_module_kernel</strong> (<em>int</em>) – Kernerl size of convolution module.</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</p></li>
<li><p><strong>attention_windows</strong> (<em>list</em>) – Layer-wise attention window sizes
for longformer self-attn</p></li>
<li><p><strong>attention_dilation</strong> (<em>list</em>) – Layer-wise attention dilation sizes
for longformer self-attn</p></li>
<li><p><strong>attention_mode</strong> (<em>str</em>) – Implementation for longformer self-attn.
Default=”sliding_chunks”
Choose ‘n2’, ‘tvm’ or ‘sliding_chunks’. More details in
<a class="reference external" href="https://github.com/allenai/longformer">https://github.com/allenai/longformer</a></p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.longformer_encoder.LongformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">ctc: espnet2.asr.ctc.CTC = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/longformer_encoder.html#LongformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.longformer_encoder.LongformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</p></li>
<li><p><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, L, output_size).
torch.Tensor: Output length (#batch).
torch.Tensor: Not to be used now.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.longformer_encoder.LongformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/longformer_encoder.html#LongformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.longformer_encoder.LongformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-rnn-encoder">
<span id="id39"></span><h2>espnet2.asr.encoder.rnn_encoder<a class="headerlink" href="#espnet2-asr-encoder-rnn-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.rnn_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.rnn_encoder.</code><code class="sig-name descname">RNNEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">use_projection: bool = True</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">output_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">subsample: Optional[Sequence[int]] = (2</em>, <em class="sig-param">2</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>RNNEncoder class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input</p></li>
<li><p><strong>output_size</strong> – The number of output features</p></li>
<li><p><strong>hidden_size</strong> – The number of hidden features</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code> becomes a bidirectional LSTM</p></li>
<li><p><strong>use_projection</strong> – Use projection layer or not</p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers</p></li>
<li><p><strong>dropout</strong> – dropout probability</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.rnn_encoder.RNNEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/rnn_encoder.html#RNNEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.rnn_encoder.RNNEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-hubert-encoder">
<span id="id40"></span><h2>espnet2.asr.encoder.hubert_encoder<a class="headerlink" href="#espnet2-asr-encoder-hubert-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.hubert_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.hubert_encoder.</code><code class="sig-name descname">FairseqHubertEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">hubert_url: str = './'</em>, <em class="sig-param">hubert_dir_path: str = './'</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">normalize_before: bool = False</em>, <em class="sig-param">freeze_finetune_updates: int = 0</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">activation_dropout: float = 0.1</em>, <em class="sig-param">attention_dropout: float = 0.0</em>, <em class="sig-param">mask_length: int = 10</em>, <em class="sig-param">mask_prob: float = 0.75</em>, <em class="sig-param">mask_selection: str = 'static'</em>, <em class="sig-param">mask_other: int = 0</em>, <em class="sig-param">apply_mask: bool = True</em>, <em class="sig-param">mask_channel_length: int = 64</em>, <em class="sig-param">mask_channel_prob: float = 0.5</em>, <em class="sig-param">mask_channel_other: int = 0</em>, <em class="sig-param">mask_channel_selection: str = 'static'</em>, <em class="sig-param">layerdrop: float = 0.1</em>, <em class="sig-param">feature_grad_mult: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>FairSeq Hubert encoder module, used for loading pretrained weight and finetuning</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>hubert_url</strong> – url to Hubert pretrained model</p></li>
<li><p><strong>hubert_dir_path</strong> – directory to download the Wav2Vec2.0 pretrained model.</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>freeze_finetune_updates</strong> – steps that freeze all layers except output layer
before tuning the whole model (nessasary to prevent overfit).</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>activation_dropout</strong> – dropout rate in activation function</p></li>
<li><p><strong>attention_dropout</strong> – dropout rate in attention</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Hubert specific Args:</dt><dd><p>Please refer to:
<a class="reference external" href="https://github.com/pytorch/fairseq/blob/master/fairseq/models/hubert/hubert.py">https://github.com/pytorch/fairseq/blob/master/fairseq/models/hubert/hubert.py</a></p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward Hubert ASR Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertEncoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertEncoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.hubert_encoder.</code><code class="sig-name descname">FairseqHubertPretrainEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int = 1</em>, <em class="sig-param">output_size: int = 1024</em>, <em class="sig-param">linear_units: int = 1024</em>, <em class="sig-param">attention_heads: int = 12</em>, <em class="sig-param">num_blocks: int = 12</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">activation_dropout_rate: float = 0.0</em>, <em class="sig-param">hubert_dict: str = './dict.txt'</em>, <em class="sig-param">label_rate: int = 100</em>, <em class="sig-param">checkpoint_activations: bool = False</em>, <em class="sig-param">sample_rate: int = 16000</em>, <em class="sig-param">use_amp: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>FairSeq Hubert pretrain encoder module, only used for pretraining stage</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>linear_units</strong> – dimension of feedforward layers</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>num_blocks</strong> – the number of encoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>hubert_dict</strong> – target dictionary for Hubert pretraining</p></li>
<li><p><strong>label_rate</strong> – label frame rate. -1 for sequence label</p></li>
<li><p><strong>sample_rate</strong> – target sample rate.</p></li>
<li><p><strong>use_amp</strong> – whether to use automatic mixed precision</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.cast_mask_emb">
<code class="sig-name descname">cast_mask_emb</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.cast_mask_emb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.cast_mask_emb" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_length: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward Hubert Pretrain Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.reload_pretrained_parameters">
<code class="sig-name descname">reload_pretrained_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#FairseqHubertPretrainEncoder.reload_pretrained_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.FairseqHubertPretrainEncoder.reload_pretrained_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.asr.encoder.hubert_encoder.download_hubert">
<code class="sig-prename descclassname">espnet2.asr.encoder.hubert_encoder.</code><code class="sig-name descname">download_hubert</code><span class="sig-paren">(</span><em class="sig-param">model_url</em>, <em class="sig-param">dir_path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/hubert_encoder.html#download_hubert"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.hubert_encoder.download_hubert" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-asr-encoder-contextual-block-transformer-encoder">
<span id="id41"></span><h2>espnet2.asr.encoder.contextual_block_transformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-contextual-block-transformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.contextual_block_transformer_encoder"></span><p>Encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.contextual_block_transformer_encoder.</code><code class="sig-name descname">ContextualBlockTransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'&gt;</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">block_size: int = 40</em>, <em class="sig-param">hop_size: int = 16</em>, <em class="sig-param">look_ahead: int = 16</em>, <em class="sig-param">init_average: bool = True</em>, <em class="sig-param">ctx_pos_enc: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Contextual Block Transformer encoder module.</p>
<p>Details in Tsunoo et al. “Transformer ASR with contextual block processing”
(<a class="reference external" href="https://arxiv.org/abs/1910.07204">https://arxiv.org/abs/1910.07204</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of encoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
<li><p><strong>block_size</strong> – block size for contextual block processing</p></li>
<li><p><strong>hop_Size</strong> – hop size for block processing</p></li>
<li><p><strong>look_ahead</strong> – look-ahead size for block_processing</p></li>
<li><p><strong>init_average</strong> – whether to use average as initial context (otherwise max values)</p></li>
<li><p><strong>ctx_pos_enc</strong> – whether to use positional encoding to the context vectors</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final=True</em>, <em class="sig-param">infer_mode=False</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
<li><p><strong>infer_mode</strong> – whether to be used for inference. This is used to
distinguish between forward_train (train and validate) and
forward_infer (decode).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_infer">
<code class="sig-name descname">forward_infer</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final: bool = True</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.forward_infer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_infer" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_train">
<code class="sig-name descname">forward_train</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.forward_train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.forward_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_transformer_encoder.html#ContextualBlockTransformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_transformer_encoder.ContextualBlockTransformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-vgg-rnn-encoder">
<span id="id42"></span><h2>espnet2.asr.encoder.vgg_rnn_encoder<a class="headerlink" href="#espnet2-asr-encoder-vgg-rnn-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.vgg_rnn_encoder"></span><dl class="class">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.vgg_rnn_encoder.</code><code class="sig-name descname">VGGRNNEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">use_projection: bool = True</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">hidden_size: int = 320</em>, <em class="sig-param">output_size: int = 320</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">in_channel: int = 1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>VGGRNNEncoder class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code> becomes a bidirectional LSTM</p></li>
<li><p><strong>use_projection</strong> – Use projection layer or not</p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers</p></li>
<li><p><strong>hidden_size</strong> – The number of hidden features</p></li>
<li><p><strong>output_size</strong> – The number of output features</p></li>
<li><p><strong>dropout</strong> – dropout probability</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/vgg_rnn_encoder.html#VGGRNNEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.vgg_rnn_encoder.VGGRNNEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-conformer-encoder">
<span id="id43"></span><h2>espnet2.asr.encoder.conformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-conformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.conformer_encoder"></span><p>Conformer encoder definition.</p>
<dl class="class">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.conformer_encoder.</code><code class="sig-name descname">ConformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: str = 'conv2d'</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 3</em>, <em class="sig-param">macaron_style: bool = False</em>, <em class="sig-param">rel_pos_type: str = 'legacy'</em>, <em class="sig-param">pos_enc_layer_type: str = 'rel_pos'</em>, <em class="sig-param">selfattention_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">activation_type: str = 'swish'</em>, <em class="sig-param">use_cnn_module: bool = True</em>, <em class="sig-param">zero_triu: bool = False</em>, <em class="sig-param">cnn_module_kernel: int = 31</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">interctc_layer_idx: List[int] = []</em>, <em class="sig-param">interctc_use_conditioning: bool = False</em>, <em class="sig-param">stochastic_depth_rate: Union[float</em>, <em class="sig-param">List[float]] = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Conformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – Input dimension.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Dimension of attention.</p></li>
<li><p><strong>attention_heads</strong> (<em>int</em>) – The number of heads of multi head attention.</p></li>
<li><p><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em>) – The number of decoder blocks.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</p></li>
<li><p><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding positional encoding.</p></li>
<li><p><strong>input_layer</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.nn.Module</em><em>]</em>) – Input layer type.</p></li>
<li><p><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</p></li>
<li><p><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output.
If True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
If False, no additional linear will be applied. i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of positionwise conv1d layer.</p></li>
<li><p><strong>rel_pos_type</strong> (<em>str</em>) – Whether to use the latest relative positional encoding or
the legacy one. The legacy relative positional encoding will be deprecated
in the future. More Details can be found in
<a class="reference external" href="https://github.com/espnet/espnet/pull/2816">https://github.com/espnet/espnet/pull/2816</a>.</p></li>
<li><p><strong>encoder_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</p></li>
<li><p><strong>encoder_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</p></li>
<li><p><strong>activation_type</strong> (<em>str</em>) – Encoder activation function type.</p></li>
<li><p><strong>macaron_style</strong> (<em>bool</em>) – Whether to use macaron style for positionwise layer.</p></li>
<li><p><strong>use_cnn_module</strong> (<em>bool</em>) – Whether to use convolution module.</p></li>
<li><p><strong>zero_triu</strong> (<em>bool</em>) – Whether to zero the upper triangular part of attention matrix.</p></li>
<li><p><strong>cnn_module_kernel</strong> (<em>int</em>) – Kernerl size of convolution module.</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">ctc: espnet2.asr.ctc.CTC = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – Input tensor (#batch, L, input_size).</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – Input length (#batch).</p></li>
<li><p><strong>prev_states</strong> (<em>torch.Tensor</em>) – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output tensor (#batch, L, output_size).
torch.Tensor: Output length (#batch).
torch.Tensor: Not to be used now.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.conformer_encoder.ConformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/conformer_encoder.html#ConformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.conformer_encoder.ConformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-asr-encoder-init">
<span id="id44"></span><h2>espnet2.asr.encoder.__init__<a class="headerlink" href="#espnet2-asr-encoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.__init__"></span></section>
<section id="espnet2-asr-encoder-contextual-block-conformer-encoder">
<span id="id45"></span><h2>espnet2.asr.encoder.contextual_block_conformer_encoder<a class="headerlink" href="#espnet2-asr-encoder-contextual-block-conformer-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.asr.encoder.contextual_block_conformer_encoder"></span><p>Created on Sat Aug 21 17:27:16 2021.</p>
<p>&#64;author: Keqi Deng (UCAS)</p>
<dl class="class">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.asr.encoder.contextual_block_conformer_encoder.</code><code class="sig-name descname">ContextualBlockConformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">output_size: int = 256</em>, <em class="sig-param">attention_heads: int = 4</em>, <em class="sig-param">linear_units: int = 2048</em>, <em class="sig-param">num_blocks: int = 6</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.0</em>, <em class="sig-param">input_layer: Optional[str] = 'conv2d'</em>, <em class="sig-param">normalize_before: bool = True</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 3</em>, <em class="sig-param">macaron_style: bool = False</em>, <em class="sig-param">pos_enc_class=&lt;class 'espnet.nets.pytorch_backend.transformer.embedding.StreamPositionalEncoding'&gt;</em>, <em class="sig-param">selfattention_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">activation_type: str = 'swish'</em>, <em class="sig-param">use_cnn_module: bool = True</em>, <em class="sig-param">cnn_module_kernel: int = 31</em>, <em class="sig-param">padding_idx: int = -1</em>, <em class="sig-param">block_size: int = 40</em>, <em class="sig-param">hop_size: int = 16</em>, <em class="sig-param">look_ahead: int = 16</em>, <em class="sig-param">init_average: bool = True</em>, <em class="sig-param">ctx_pos_enc: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.asr.encoder.abs_encoder.AbsEncoder" title="espnet2.asr.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.asr.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Contextual Block Conformer encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – input dim</p></li>
<li><p><strong>output_size</strong> – dimension of attention</p></li>
<li><p><strong>attention_heads</strong> – the number of heads of multi head attention</p></li>
<li><p><strong>linear_units</strong> – the number of units of position-wise feed forward</p></li>
<li><p><strong>num_blocks</strong> – the number of decoder blocks</p></li>
<li><p><strong>dropout_rate</strong> – dropout rate</p></li>
<li><p><strong>attention_dropout_rate</strong> – dropout rate in attention</p></li>
<li><p><strong>positional_dropout_rate</strong> – dropout rate after adding positional encoding</p></li>
<li><p><strong>input_layer</strong> – input layer type</p></li>
<li><p><strong>pos_enc_class</strong> – PositionalEncoding or ScaledPositionalEncoding</p></li>
<li><p><strong>normalize_before</strong> – whether to use layer_norm before the first block</p></li>
<li><p><strong>concat_after</strong> – whether to concat attention layer’s input and output
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied.
i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> – linear of conv1d</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> – kernel size of positionwise conv1d layer</p></li>
<li><p><strong>padding_idx</strong> – padding_idx for input_layer=embed</p></li>
<li><p><strong>block_size</strong> – block size for contextual block processing</p></li>
<li><p><strong>hop_Size</strong> – hop size for block processing</p></li>
<li><p><strong>look_ahead</strong> – look-ahead size for block_processing</p></li>
<li><p><strong>init_average</strong> – whether to use average as initial context (otherwise max values)</p></li>
<li><p><strong>ctx_pos_enc</strong> – whether to use positional encoding to the context vectors</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final=True</em>, <em class="sig-param">infer_mode=False</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
<li><p><strong>infer_mode</strong> – whether to be used for inference. This is used to
distinguish between forward_train (train and validate) and
forward_infer (decode).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_infer">
<code class="sig-name descname">forward_infer</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em>, <em class="sig-param">is_final: bool = True</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.forward_infer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_infer" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_train">
<code class="sig-name descname">forward_train</code><span class="sig-paren">(</span><em class="sig-param">xs_pad: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">prev_states: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.forward_train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.forward_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed positions in tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> – input tensor (B, L, D)</p></li>
<li><p><strong>ilens</strong> – input length (B)</p></li>
<li><p><strong>prev_states</strong> – Not to be used now.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position embedded tensor and mask</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.output_size">
<code class="sig-name descname">output_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/espnet2/asr/encoder/contextual_block_conformer_encoder.html#ContextualBlockConformerEncoder.output_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.asr.encoder.contextual_block_conformer_encoder.ContextualBlockConformerEncoder.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="espnet2.tts.html" class="btn btn-neutral float-left" title="espnet2.tts package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="espnet2.text.html" class="btn btn-neutral float-right" title="espnet2.text package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017, Shinji Watanabe.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>