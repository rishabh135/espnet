<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>espnet2.enh package &mdash; ESPnet 202205 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="espnet2.fileio package" href="espnet2.fileio.html" />
    <link rel="prev" title="espnet2.lm package" href="espnet2.lm.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> ESPnet
          </a>
              <div class="version">
                202205
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelization.html">Using Job scheduling system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Docker</a></li>
</ul>
<p><span class="caption-text">ESPnet2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html">ESPnet2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html#instruction-for-run-sh">Instruction for run.sh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_training_option.html">Change the configuration for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_task.html">Task class and data input system for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_distributed.html">Distributed training</a></li>
</ul>
<p><span class="caption-text">Notebook:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_cli.html">Speech Recognition (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/asr_library.html">Speech Recognition (Library)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_realtime_demo.html">ESPnet2-ASR realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html"><strong>Use transfer learning for ASR in ESPnet2</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#Abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#ESPnet-installation-(about-10-minutes-in-total)">ESPnet installation (about 10 minutes in total)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_asr_transfer_learning_demo.html#mini_an4-recipe-as-a-transfer-learning-example">mini_an4 recipe as a transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_streaming_asr_demo.html">ESPnet2 real streaming Transformer demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tts_realtime_demo.html">ESPnet2-TTS realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html">CMU 11751/18781 2021: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-an-inference-example">Run an inference example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Full-installation">Full installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-a-recipe-example">Run a recipe example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#Contents">Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#(1)-Tutorials-on-the-Basic-Usage">(1) Tutorials on the Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/espnet_se_demonstration_for_waspaa_2021.html#(2)-Tutorials-on-Contributing-to-ESPNet-SE-Project">(2) Tutorials on Contributing to ESPNet-SE Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/pretrained.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/se_demo.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/st_demo.html">ESPnet Speech Translation Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_cli.html">Text-to-Speech (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebook/tts_realtime_demo.html">ESPnet real time E2E-TTS demonstration</a></li>
</ul>
<p><span class="caption-text">Package Reference:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="espnet.lm.html">espnet.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.utils.html">espnet.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.bin.html">espnet.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.nets.html">espnet.nets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.mt.html">espnet.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.optimizer.html">espnet.optimizer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.scheduler.html">espnet.scheduler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.st.html">espnet.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet.vc.html">espnet.vc package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.lm.html">espnet2.lm package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">espnet2.enh package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-espnet-enh-s2t-model">espnet2.enh.espnet_enh_s2t_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-espnet-model">espnet2.enh.espnet_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-init">espnet2.enh.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-abs-enh">espnet2.enh.abs_enh</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-dnn-beamformer">espnet2.enh.layers.dnn_beamformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-complex-utils">espnet2.enh.layers.complex_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-wpe">espnet2.enh.layers.wpe</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-conv-utils">espnet2.enh.layers.conv_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-tcn">espnet2.enh.layers.tcn</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-dnn-wpe">espnet2.enh.layers.dnn_wpe</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-complexnn">espnet2.enh.layers.complexnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-ifasnet">espnet2.enh.layers.ifasnet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-dprnn">espnet2.enh.layers.dprnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-init">espnet2.enh.layers.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-beamformer">espnet2.enh.layers.beamformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-dc-crn">espnet2.enh.layers.dc_crn</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-dpmulcat">espnet2.enh.layers.dpmulcat</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-mask-estimator">espnet2.enh.layers.mask_estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-skim">espnet2.enh.layers.skim</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-layers-fasnet">espnet2.enh.layers.fasnet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-init">espnet2.enh.loss.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-wrappers-pit-solver">espnet2.enh.loss.wrappers.pit_solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-wrappers-fixed-order">espnet2.enh.loss.wrappers.fixed_order</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-wrappers-multilayer-pit-solver">espnet2.enh.loss.wrappers.multilayer_pit_solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-wrappers-abs-wrapper">espnet2.enh.loss.wrappers.abs_wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-wrappers-dpcl-solver">espnet2.enh.loss.wrappers.dpcl_solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-wrappers-init">espnet2.enh.loss.wrappers.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-criterions-tf-domain">espnet2.enh.loss.criterions.tf_domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-criterions-time-domain">espnet2.enh.loss.criterions.time_domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-criterions-abs-loss">espnet2.enh.loss.criterions.abs_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-loss-criterions-init">espnet2.enh.loss.criterions.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-encoder-stft-encoder">espnet2.enh.encoder.stft_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-encoder-conv-encoder">espnet2.enh.encoder.conv_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-encoder-init">espnet2.enh.encoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-encoder-abs-encoder">espnet2.enh.encoder.abs_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-encoder-null-encoder">espnet2.enh.encoder.null_encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-rnn-separator">espnet2.enh.separator.rnn_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-conformer-separator">espnet2.enh.separator.conformer_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-skim-separator">espnet2.enh.separator.skim_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-asteroid-models">espnet2.enh.separator.asteroid_models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-dc-crn-separator">espnet2.enh.separator.dc_crn_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-dccrn-separator">espnet2.enh.separator.dccrn_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-neural-beamformer">espnet2.enh.separator.neural_beamformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-dan-separator">espnet2.enh.separator.dan_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-init">espnet2.enh.separator.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-transformer-separator">espnet2.enh.separator.transformer_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-svoice-separator">espnet2.enh.separator.svoice_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-tcn-separator">espnet2.enh.separator.tcn_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-dprnn-separator">espnet2.enh.separator.dprnn_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-fasnet-separator">espnet2.enh.separator.fasnet_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-dpcl-e2e-separator">espnet2.enh.separator.dpcl_e2e_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-dpcl-separator">espnet2.enh.separator.dpcl_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-separator-abs-separator">espnet2.enh.separator.abs_separator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-decoder-conv-decoder">espnet2.enh.decoder.conv_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-decoder-abs-decoder">espnet2.enh.decoder.abs_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-decoder-init">espnet2.enh.decoder.__init__</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-decoder-stft-decoder">espnet2.enh.decoder.stft_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet2-enh-decoder-null-decoder">espnet2.enh.decoder.null_decoder</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.fileio.html">espnet2.fileio package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.train.html">espnet2.train package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.hubert.html">espnet2.hubert package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.torch_utils.html">espnet2.torch_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.layers.html">espnet2.layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.gan_tts.html">espnet2.gan_tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.main_funcs.html">espnet2.main_funcs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.utils.html">espnet2.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.text.html">espnet2.text package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.bin.html">espnet2.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.optimizers.html">espnet2.optimizers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.mt.html">espnet2.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tasks.html">espnet2.tasks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.asr.html">espnet2.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.samplers.html">espnet2.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.schedulers.html">espnet2.schedulers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.st.html">espnet2.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.iterators.html">espnet2.iterators package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.fst.html">espnet2.fst package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.tts.html">espnet2.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2.diar.html">espnet2.diar package</a></li>
</ul>
<p><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet2_bin.html">core tools (espnet2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_sh.html">bash utility tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ESPnet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>espnet2.enh package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/_gen/espnet2.enh.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="espnet2-enh-package">
<h1>espnet2.enh package<a class="headerlink" href="#espnet2-enh-package" title="Permalink to this headline">¶</a></h1>
<section id="espnet2-enh-espnet-enh-s2t-model">
<span id="id1"></span><h2>espnet2.enh.espnet_enh_s2t_model<a class="headerlink" href="#espnet2-enh-espnet-enh-s2t-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.espnet_enh_s2t_model"></span><dl class="class">
<dt id="espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.espnet_enh_s2t_model.</code><code class="sig-name descname">ESPnetEnhS2TModel</code><span class="sig-paren">(</span><em class="sig-param">enh_model: espnet2.enh.espnet_model.ESPnetEnhancementModel, s2t_model: Union[espnet2.asr.espnet_model.ESPnetASRModel, espnet2.st.espnet_model.ESPnetSTModel], calc_enh_loss: bool = True, bypass_enh_prob: float = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/espnet_enh_s2t_model.html#ESPnetEnhS2TModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="espnet2.train.html#espnet2.train.abs_espnet_model.AbsESPnetModel" title="espnet2.train.abs_espnet_model.AbsESPnetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.train.abs_espnet_model.AbsESPnetModel</span></code></a></p>
<p>Joint model Enhancement and Speech to Text.</p>
<dl class="method">
<dt id="espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.batchify_nll">
<code class="sig-name descname">batchify_nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em>, <em class="sig-param">batch_size: int = 100</em><span class="sig-paren">)</span><a class="headerlink" href="#espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.batchify_nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>To avoid OOM, this fuction seperate the input into batches.
Then call nll for each batch and combine and return results.
:param encoder_out: (Batch, Length, Dim)
:param encoder_out_lens: (Batch,)
:param ys_pad: (Batch, Length)
:param ys_pad_lens: (Batch,)
:param batch_size: int, samples each batch contain when computing nll,</p>
<blockquote>
<div><p>you may change this to avoid OOM or increase
GPU memory usage</p>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.collect_feats">
<code class="sig-name descname">collect_feats</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/espnet_enh_s2t_model.html#ESPnetEnhS2TModel.collect_feats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.collect_feats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.encode">
<code class="sig-name descname">encode</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/espnet_enh_s2t_model.html#ESPnetEnhS2TModel.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder. Note that this method is used by asr_inference.py</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">text: torch.Tensor</em>, <em class="sig-param">text_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/espnet_enh_s2t_model.html#ESPnetEnhS2TModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech</strong> – (Batch, Length, …)</p></li>
<li><p><strong>speech_lengths</strong> – (Batch, )</p></li>
<li><p><strong>text</strong> – (Batch, Length)</p></li>
<li><p><strong>text_lengths</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.inherite_attributes">
<code class="sig-name descname">inherite_attributes</code><span class="sig-paren">(</span><em class="sig-param">inherite_enh_attrs: List[str] = []</em>, <em class="sig-param">inherite_s2t_attrs: List[str] = []</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/espnet_enh_s2t_model.html#ESPnetEnhS2TModel.inherite_attributes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.inherite_attributes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.nll">
<code class="sig-name descname">nll</code><span class="sig-paren">(</span><em class="sig-param">encoder_out: torch.Tensor</em>, <em class="sig-param">encoder_out_lens: torch.Tensor</em>, <em class="sig-param">ys_pad: torch.Tensor</em>, <em class="sig-param">ys_pad_lens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/espnet_enh_s2t_model.html#ESPnetEnhS2TModel.nll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_enh_s2t_model.ESPnetEnhS2TModel.nll" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute negative log likelihood(nll) from transformer-decoder</p>
<p>Normally, this function is called in batchify_nll.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_out</strong> – (Batch, Length, Dim)</p></li>
<li><p><strong>encoder_out_lens</strong> – (Batch,)</p></li>
<li><p><strong>ys_pad</strong> – (Batch, Length)</p></li>
<li><p><strong>ys_pad_lens</strong> – (Batch,)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-espnet-model">
<span id="id2"></span><h2>espnet2.enh.espnet_model<a class="headerlink" href="#espnet2-enh-espnet-model" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.espnet_model"></span><p>Enhancement model module.</p>
<dl class="class">
<dt id="espnet2.enh.espnet_model.ESPnetEnhancementModel">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.espnet_model.</code><code class="sig-name descname">ESPnetEnhancementModel</code><span class="sig-paren">(</span><em class="sig-param">encoder: espnet2.enh.encoder.abs_encoder.AbsEncoder, separator: espnet2.enh.separator.abs_separator.AbsSeparator, decoder: espnet2.enh.decoder.abs_decoder.AbsDecoder, loss_wrappers: List[espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper], stft_consistency: bool = False, loss_type: str = 'mask_mse', mask_type: Optional[str] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/espnet_model.html#ESPnetEnhancementModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_model.ESPnetEnhancementModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="espnet2.train.html#espnet2.train.abs_espnet_model.AbsESPnetModel" title="espnet2.train.abs_espnet_model.AbsESPnetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.train.abs_espnet_model.AbsESPnetModel</span></code></a></p>
<p>Speech enhancement or separation Frontend model</p>
<dl class="method">
<dt id="espnet2.enh.espnet_model.ESPnetEnhancementModel.collect_feats">
<code class="sig-name descname">collect_feats</code><span class="sig-paren">(</span><em class="sig-param">speech_mix: torch.Tensor</em>, <em class="sig-param">speech_mix_lengths: torch.Tensor</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Dict[str, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/espnet_model.html#ESPnetEnhancementModel.collect_feats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_model.ESPnetEnhancementModel.collect_feats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_model.ESPnetEnhancementModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">speech_mix: torch.Tensor</em>, <em class="sig-param">speech_mix_lengths: torch.Tensor = None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/espnet_model.html#ESPnetEnhancementModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_model.ESPnetEnhancementModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Frontend + Encoder + Decoder + Calc loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>speech_mix</strong> – (Batch, samples) or (Batch, samples, channels)</p></li>
<li><p><strong>speech_ref</strong> – (Batch, num_speaker, samples)
or (Batch, num_speaker, samples, channels)</p></li>
<li><p><strong>speech_mix_lengths</strong> – (Batch,), default None for chunk interator,
because the chunk-iterator does not have the
speech_lengths returned. see in
espnet2/iterators/chunk_iter_factory.py</p></li>
<li><p><strong>kwargs</strong> – “utt_id” is among the input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_model.ESPnetEnhancementModel.forward_enhance">
<code class="sig-name descname">forward_enhance</code><span class="sig-paren">(</span><em class="sig-param">speech_mix: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/espnet_model.html#ESPnetEnhancementModel.forward_enhance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_model.ESPnetEnhancementModel.forward_enhance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.espnet_model.ESPnetEnhancementModel.forward_loss">
<code class="sig-name descname">forward_loss</code><span class="sig-paren">(</span><em class="sig-param">speech_pre: torch.Tensor</em>, <em class="sig-param">speech_lengths: torch.Tensor</em>, <em class="sig-param">feature_mix: torch.Tensor</em>, <em class="sig-param">feature_pre: torch.Tensor</em>, <em class="sig-param">others: OrderedDict</em>, <em class="sig-param">speech_ref: torch.Tensor</em>, <em class="sig-param">noise_ref: torch.Tensor = None</em>, <em class="sig-param">dereverb_speech_ref: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/espnet_model.html#ESPnetEnhancementModel.forward_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.espnet_model.ESPnetEnhancementModel.forward_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-init">
<span id="id3"></span><h2>espnet2.enh.__init__<a class="headerlink" href="#espnet2-enh-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.__init__"></span></section>
<section id="espnet2-enh-abs-enh">
<span id="id4"></span><h2>espnet2.enh.abs_enh<a class="headerlink" href="#espnet2-enh-abs-enh" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.abs_enh"></span><dl class="class">
<dt id="espnet2.enh.abs_enh.AbsEnhancement">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.abs_enh.</code><code class="sig-name descname">AbsEnhancement</code><a class="reference internal" href="../_modules/espnet2/enh/abs_enh.html#AbsEnhancement"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.abs_enh.AbsEnhancement" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.enh.abs_enh.AbsEnhancement.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/abs_enh.html#AbsEnhancement.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.abs_enh.AbsEnhancement.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.abs_enh.AbsEnhancement.forward_rawwav">
<em class="property">abstract </em><code class="sig-name descname">forward_rawwav</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/abs_enh.html#AbsEnhancement.forward_rawwav"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.abs_enh.AbsEnhancement.forward_rawwav" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-layers-dnn-beamformer">
<span id="id5"></span><h2>espnet2.enh.layers.dnn_beamformer<a class="headerlink" href="#espnet2-enh-layers-dnn-beamformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.dnn_beamformer"></span><p>DNN beamformer module.</p>
<dl class="class">
<dt id="espnet2.enh.layers.dnn_beamformer.AttentionReference">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dnn_beamformer.</code><code class="sig-name descname">AttentionReference</code><span class="sig-paren">(</span><em class="sig-param">bidim</em>, <em class="sig-param">att_dim</em>, <em class="sig-param">eps=1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_beamformer.html#AttentionReference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_beamformer.AttentionReference" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.dnn_beamformer.AttentionReference.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">psd_in: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.LongTensor, scaling: float = 2.0</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.LongTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_beamformer.html#AttentionReference.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_beamformer.AttentionReference.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Attention-based reference forward function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_in</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, F, C, C)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – (B,)</p></li>
<li><p><strong>scaling</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, C)
ilens (torch.Tensor): (B,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>u (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dnn_beamformer.DNN_Beamformer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dnn_beamformer.</code><code class="sig-name descname">DNN_Beamformer</code><span class="sig-paren">(</span><em class="sig-param">bidim</em>, <em class="sig-param">btype: str = 'blstmp'</em>, <em class="sig-param">blayers: int = 3</em>, <em class="sig-param">bunits: int = 300</em>, <em class="sig-param">bprojs: int = 320</em>, <em class="sig-param">num_spk: int = 1</em>, <em class="sig-param">use_noise_mask: bool = True</em>, <em class="sig-param">nonlinear: str = 'sigmoid'</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">badim: int = 320</em>, <em class="sig-param">ref_channel: int = -1</em>, <em class="sig-param">beamformer_type: str = 'mvdr_souden'</em>, <em class="sig-param">rtf_iterations: int = 2</em>, <em class="sig-param">mwf_mu: float = 1.0</em>, <em class="sig-param">eps: float = 1e-06</em>, <em class="sig-param">diagonal_loading: bool = True</em>, <em class="sig-param">diag_eps: float = 1e-07</em>, <em class="sig-param">mask_flooring: bool = False</em>, <em class="sig-param">flooring_thres: float = 1e-06</em>, <em class="sig-param">use_torch_solver: bool = True</em>, <em class="sig-param">btaps: int = 5</em>, <em class="sig-param">bdelay: int = 3</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_beamformer.html#DNN_Beamformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_beamformer.DNN_Beamformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>DNN mask based Beamformer.</p>
<dl class="simple">
<dt>Citation:</dt><dd><p>Multichannel End-to-end Speech Recognition; T. Ochiai et al., 2017;
<a class="reference external" href="http://proceedings.mlr.press/v70/ochiai17a/ochiai17a.pdf">http://proceedings.mlr.press/v70/ochiai17a/ochiai17a.pdf</a></p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dnn_beamformer.DNN_Beamformer.apply_beamforming">
<code class="sig-name descname">apply_beamforming</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">ilens</em>, <em class="sig-param">psd_n</em>, <em class="sig-param">psd_speech</em>, <em class="sig-param">psd_distortion=None</em>, <em class="sig-param">rtf_mat=None</em>, <em class="sig-param">spk=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_beamformer.html#DNN_Beamformer.apply_beamforming"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_beamformer.DNN_Beamformer.apply_beamforming" title="Permalink to this definition">¶</a></dt>
<dd><p>Beamforming with the provided statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, F, C, T)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – (B,)</p></li>
<li><p><strong>psd_n</strong> (<em>torch.complex64/ComplexTensor</em>) – Noise covariance matrix for MVDR (B, F, C, C)
Observation covariance matrix for MPDR/wMPDR (B, F, C, C)
Stacked observation covariance for WPD (B,F,(btaps+1)*C,(btaps+1)*C)</p></li>
<li><p><strong>psd_speech</strong> (<em>torch.complex64/ComplexTensor</em>) – Speech covariance matrix (B, F, C, C)</p></li>
<li><p><strong>psd_distortion</strong> (<em>torch.complex64/ComplexTensor</em>) – Noise covariance matrix (B, F, C, C)</p></li>
<li><p><strong>rtf_mat</strong> (<em>torch.complex64/ComplexTensor</em>) – RTF matrix (B, F, C, num_spk)</p></li>
<li><p><strong>spk</strong> (<em>int</em>) – speaker index</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, F, T)
ws (torch.complex64/ComplexTensor): (B, F) or (B, F, (btaps+1)*C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>enhanced (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.dnn_beamformer.DNN_Beamformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">data: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.LongTensor, powers: Optional[List[torch.Tensor]] = None, oracle_masks: Optional[List[torch.Tensor]] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[Union[torch.Tensor, torch_complex.tensor.ComplexTensor], torch.LongTensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_beamformer.html#DNN_Beamformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_beamformer.DNN_Beamformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>DNN_Beamformer forward function.</p>
<dl class="simple">
<dt>Notation:</dt><dd><p>B: Batch
C: Channel
T: Time or Sequence length
F: Freq</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, T, C, F)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – (B,)</p></li>
<li><p><strong>powers</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>] or </em><em>None</em>) – used for wMPDR or WPD (B, F, T)</p></li>
<li><p><strong>oracle_masks</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>] or </em><em>None</em>) – oracle masks (B, F, C, T)
if not None, oracle_masks will be used instead of self.mask</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, T, F)
ilens (torch.Tensor): (B,)
masks (torch.Tensor): (B, T, C, F)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>enhanced (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.dnn_beamformer.DNN_Beamformer.predict_mask">
<code class="sig-name descname">predict_mask</code><span class="sig-paren">(</span><em class="sig-param">data: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.LongTensor</em><span class="sig-paren">)</span> &#x2192; Tuple[Tuple[torch.Tensor, ...], torch.LongTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_beamformer.html#DNN_Beamformer.predict_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_beamformer.DNN_Beamformer.predict_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict masks for beamforming.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, T, C, F), double precision</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – (B,)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, T, C, F)
ilens (torch.Tensor): (B,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masks (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-layers-complex-utils">
<span id="id6"></span><h2>espnet2.enh.layers.complex_utils<a class="headerlink" href="#espnet2-enh-layers-complex-utils" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.complex_utils"></span><p>Beamformer module.</p>
<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.cat">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">cat</code><span class="sig-paren">(</span><em class="sig-param">seq: Sequence[Union[torch_complex.tensor.ComplexTensor, torch.Tensor]], *args, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#cat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.cat" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.complex_norm">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">complex_norm</code><span class="sig-paren">(</span><em class="sig-param">c: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], dim=-1, keepdim=False</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#complex_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.complex_norm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.einsum">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">einsum</code><span class="sig-paren">(</span><em class="sig-param">equation</em>, <em class="sig-param">*operands</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#einsum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.einsum" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.inverse">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">inverse</code><span class="sig-paren">(</span><em class="sig-param">c: Union[torch.Tensor, torch_complex.tensor.ComplexTensor]</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#inverse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.inverse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.is_complex">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">is_complex</code><span class="sig-paren">(</span><em class="sig-param">c</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#is_complex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.is_complex" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.is_torch_complex_tensor">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">is_torch_complex_tensor</code><span class="sig-paren">(</span><em class="sig-param">c</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#is_torch_complex_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.is_torch_complex_tensor" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.matmul">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">matmul</code><span class="sig-paren">(</span><em class="sig-param">a: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], b: Union[torch.Tensor, torch_complex.tensor.ComplexTensor]</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#matmul"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.matmul" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.new_complex_like">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">new_complex_like</code><span class="sig-paren">(</span><em class="sig-param">ref: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], real_imag: Tuple[torch.Tensor, torch.Tensor]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#new_complex_like"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.new_complex_like" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.reverse">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">reverse</code><span class="sig-paren">(</span><em class="sig-param">a: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#reverse"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.reverse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.solve">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">solve</code><span class="sig-paren">(</span><em class="sig-param">b: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], a: Union[torch.Tensor, torch_complex.tensor.ComplexTensor]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#solve"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.solve" title="Permalink to this definition">¶</a></dt>
<dd><p>Solve the linear equation ax = b.</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.stack">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">stack</code><span class="sig-paren">(</span><em class="sig-param">seq: Sequence[Union[torch_complex.tensor.ComplexTensor, torch.Tensor]], *args, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#stack"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.stack" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.to_double">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">to_double</code><span class="sig-paren">(</span><em class="sig-param">c</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#to_double"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.to_double" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.to_float">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">to_float</code><span class="sig-paren">(</span><em class="sig-param">c</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#to_float"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.to_float" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complex_utils.trace">
<code class="sig-prename descclassname">espnet2.enh.layers.complex_utils.</code><code class="sig-name descname">trace</code><span class="sig-paren">(</span><em class="sig-param">a: Union[torch.Tensor, torch_complex.tensor.ComplexTensor]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complex_utils.html#trace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complex_utils.trace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-enh-layers-wpe">
<span id="id7"></span><h2>espnet2.enh.layers.wpe<a class="headerlink" href="#espnet2-enh-layers-wpe" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.wpe"></span><dl class="function">
<dt id="espnet2.enh.layers.wpe.get_correlations">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">get_correlations</code><span class="sig-paren">(</span><em class="sig-param">Y: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], inverse_power: torch.Tensor, taps, delay</em><span class="sig-paren">)</span> &#x2192; Tuple[Union[torch.Tensor, torch_complex.tensor.ComplexTensor], Union[torch.Tensor, torch_complex.tensor.ComplexTensor]]<a class="reference internal" href="../_modules/espnet2/enh/layers/wpe.html#get_correlations"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.wpe.get_correlations" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates weighted correlations of a window of length taps</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Y</strong> – Complex-valued STFT signal with shape (F, C, T)</p></li>
<li><p><strong>inverse_power</strong> – Weighting factor with shape (F, T)</p></li>
<li><p><strong>taps</strong> (<em>int</em>) – Lenghts of correlation window</p></li>
<li><p><strong>delay</strong> (<em>int</em>) – Delay for the weighting factor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Correlation matrix of shape (F, taps*C, taps*C)
Correlation vector of shape (F, taps, C, C)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.wpe.get_filter_matrix_conj">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">get_filter_matrix_conj</code><span class="sig-paren">(</span><em class="sig-param">correlation_matrix: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], correlation_vector: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], eps: float = 1e-10</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/wpe.html#get_filter_matrix_conj"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.wpe.get_filter_matrix_conj" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate (conjugate) filter matrix based on correlations for one freq.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>correlation_matrix</strong> – Correlation matrix (F, taps * C, taps * C)</p></li>
<li><p><strong>correlation_vector</strong> – Correlation vector (F, taps, C, C)</p></li>
<li><p><strong>eps</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(F, taps, C, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>filter_matrix_conj (torch.complex/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.wpe.get_power">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">get_power</code><span class="sig-paren">(</span><em class="sig-param">signal</em>, <em class="sig-param">dim=-2</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/layers/wpe.html#get_power"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.wpe.get_power" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates power for <cite>signal</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>signal</strong> – Single frequency signal
with shape (F, C, T).</p></li>
<li><p><strong>axis</strong> – reduce_mean axis</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Power with shape (F, T)</p>
</dd>
</dl>
</dd></dl>

<dl class="data">
<dt id="espnet2.enh.layers.wpe.is_torch_1_9_plus">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">is_torch_1_9_plus</code><em class="property"> = True</em><a class="headerlink" href="#espnet2.enh.layers.wpe.is_torch_1_9_plus" title="Permalink to this definition">¶</a></dt>
<dd><p>//github.com/fgnt/nara_wpe
Many functions aren’t enough tested</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>WPE pytorch version</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Ported from https</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.wpe.perform_filter_operation">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">perform_filter_operation</code><span class="sig-paren">(</span><em class="sig-param">Y: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], filter_matrix_conj: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], taps, delay</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/wpe.html#perform_filter_operation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.wpe.perform_filter_operation" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Y</strong> – Complex-valued STFT signal of shape (F, C, T)</p></li>
<li><p><strong>Matrix</strong> (<em>filter</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.wpe.signal_framing">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">signal_framing</code><span class="sig-paren">(</span><em class="sig-param">signal: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], frame_length: int, frame_step: int, pad_value=0</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/wpe.html#signal_framing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.wpe.signal_framing" title="Permalink to this definition">¶</a></dt>
<dd><p>Expands signal into frames of frame_length.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>signal</strong> – (B * F, D, T)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B * F, D, T, W)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.wpe.wpe">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">wpe</code><span class="sig-paren">(</span><em class="sig-param">Y: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], taps=10, delay=3, iterations=3</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/wpe.html#wpe"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.wpe.wpe" title="Permalink to this definition">¶</a></dt>
<dd><p>WPE</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Y</strong> – Complex valued STFT signal with shape (F, C, T)</p></li>
<li><p><strong>taps</strong> – Number of filter taps</p></li>
<li><p><strong>delay</strong> – Delay as a guard interval, such that X does not become zero.</p></li>
<li><p><strong>iterations</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(F, C, T)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>enhanced</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.wpe.wpe_one_iteration">
<code class="sig-prename descclassname">espnet2.enh.layers.wpe.</code><code class="sig-name descname">wpe_one_iteration</code><span class="sig-paren">(</span><em class="sig-param">Y: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], power: torch.Tensor, taps: int = 10, delay: int = 3, eps: float = 1e-10, inverse_power: bool = True</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/wpe.html#wpe_one_iteration"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.wpe.wpe_one_iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>WPE for one iteration</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Y</strong> – Complex valued STFT signal with shape (…, C, T)</p></li>
<li><p><strong>power</strong> – : (…, T)</p></li>
<li><p><strong>taps</strong> – Number of filter taps</p></li>
<li><p><strong>delay</strong> – Delay as a guard interval, such that X does not become zero.</p></li>
<li><p><strong>eps</strong> – </p></li>
<li><p><strong>inverse_power</strong> (<em>bool</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, C, T)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>enhanced</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="espnet2-enh-layers-conv-utils">
<span id="id8"></span><h2>espnet2.enh.layers.conv_utils<a class="headerlink" href="#espnet2-enh-layers-conv-utils" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.conv_utils"></span><dl class="function">
<dt id="espnet2.enh.layers.conv_utils.conv2d_output_shape">
<code class="sig-prename descclassname">espnet2.enh.layers.conv_utils.</code><code class="sig-name descname">conv2d_output_shape</code><span class="sig-paren">(</span><em class="sig-param">h_w</em>, <em class="sig-param">kernel_size=1</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">pad=0</em>, <em class="sig-param">dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/conv_utils.html#conv2d_output_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.conv_utils.conv2d_output_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.conv_utils.convtransp2d_output_shape">
<code class="sig-prename descclassname">espnet2.enh.layers.conv_utils.</code><code class="sig-name descname">convtransp2d_output_shape</code><span class="sig-paren">(</span><em class="sig-param">h_w</em>, <em class="sig-param">kernel_size=1</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">pad=0</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">out_pad=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/conv_utils.html#convtransp2d_output_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.conv_utils.convtransp2d_output_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.conv_utils.num2tuple">
<code class="sig-prename descclassname">espnet2.enh.layers.conv_utils.</code><code class="sig-name descname">num2tuple</code><span class="sig-paren">(</span><em class="sig-param">num</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/conv_utils.html#num2tuple"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.conv_utils.num2tuple" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-enh-layers-tcn">
<span id="id9"></span><h2>espnet2.enh.layers.tcn<a class="headerlink" href="#espnet2-enh-layers-tcn" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.tcn"></span><dl class="class">
<dt id="espnet2.enh.layers.tcn.ChannelwiseLayerNorm">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">ChannelwiseLayerNorm</code><span class="sig-paren">(</span><em class="sig-param">channel_size</em>, <em class="sig-param">shape='BDT'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#ChannelwiseLayerNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.ChannelwiseLayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Channel-wise Layer Normalization (cLN).</p>
<dl class="method">
<dt id="espnet2.enh.layers.tcn.ChannelwiseLayerNorm.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#ChannelwiseLayerNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.ChannelwiseLayerNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> – [M, N, K], M is batch size, N is channel size, K is length</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[M, N, K]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>cLN_y</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.tcn.ChannelwiseLayerNorm.reset_parameters">
<code class="sig-name descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#ChannelwiseLayerNorm.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.ChannelwiseLayerNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.tcn.Chomp1d">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">Chomp1d</code><span class="sig-paren">(</span><em class="sig-param">chomp_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#Chomp1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.Chomp1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>To ensure the output length is the same as the input.</p>
<dl class="method">
<dt id="espnet2.enh.layers.tcn.Chomp1d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#Chomp1d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.Chomp1d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – [M, H, Kpad]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[M, H, K]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.tcn.DepthwiseSeparableConv">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">DepthwiseSeparableConv</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride</em>, <em class="sig-param">padding</em>, <em class="sig-param">dilation</em>, <em class="sig-param">norm_type='gLN'</em>, <em class="sig-param">causal=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#DepthwiseSeparableConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.DepthwiseSeparableConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.tcn.DepthwiseSeparableConv.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#DepthwiseSeparableConv.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.DepthwiseSeparableConv.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – [M, H, K]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[M, B, K]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>result</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.tcn.GlobalLayerNorm">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">GlobalLayerNorm</code><span class="sig-paren">(</span><em class="sig-param">channel_size</em>, <em class="sig-param">shape='BDT'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#GlobalLayerNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.GlobalLayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Global Layer Normalization (gLN).</p>
<dl class="method">
<dt id="espnet2.enh.layers.tcn.GlobalLayerNorm.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#GlobalLayerNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.GlobalLayerNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>y</strong> – [M, N, K], M is batch size, N is channel size, K is length</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[M, N, K]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>gLN_y</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.tcn.GlobalLayerNorm.reset_parameters">
<code class="sig-name descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#GlobalLayerNorm.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.GlobalLayerNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.tcn.TemporalBlock">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">TemporalBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride</em>, <em class="sig-param">padding</em>, <em class="sig-param">dilation</em>, <em class="sig-param">norm_type='gLN'</em>, <em class="sig-param">causal=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#TemporalBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.TemporalBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.tcn.TemporalBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#TemporalBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.TemporalBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – [M, B, K]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[M, B, K]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.tcn.TemporalConvNet">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">TemporalConvNet</code><span class="sig-paren">(</span><em class="sig-param">N</em>, <em class="sig-param">B</em>, <em class="sig-param">H</em>, <em class="sig-param">P</em>, <em class="sig-param">X</em>, <em class="sig-param">R</em>, <em class="sig-param">C</em>, <em class="sig-param">norm_type='gLN'</em>, <em class="sig-param">causal=False</em>, <em class="sig-param">mask_nonlinear='relu'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#TemporalConvNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.TemporalConvNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Basic Module of tasnet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>N</strong> – Number of filters in autoencoder</p></li>
<li><p><strong>B</strong> – Number of channels in bottleneck 1 * 1-conv block</p></li>
<li><p><strong>H</strong> – Number of channels in convolutional blocks</p></li>
<li><p><strong>P</strong> – Kernel size in convolutional blocks</p></li>
<li><p><strong>X</strong> – Number of convolutional blocks in each repeat</p></li>
<li><p><strong>R</strong> – Number of repeats</p></li>
<li><p><strong>C</strong> – Number of speakers</p></li>
<li><p><strong>norm_type</strong> – BN, gLN, cLN</p></li>
<li><p><strong>causal</strong> – causal or non-causal</p></li>
<li><p><strong>mask_nonlinear</strong> – use which non-linear function to generate mask</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.tcn.TemporalConvNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">mixture_w</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#TemporalConvNet.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.TemporalConvNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Keep this API same with TasNet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mixture_w</strong> – [M, N, K], M is batch size</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[M, C, N, K]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>est_mask</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.tcn.check_nonlinear">
<code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">check_nonlinear</code><span class="sig-paren">(</span><em class="sig-param">nolinear_type</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#check_nonlinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.check_nonlinear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.tcn.choose_norm">
<code class="sig-prename descclassname">espnet2.enh.layers.tcn.</code><code class="sig-name descname">choose_norm</code><span class="sig-paren">(</span><em class="sig-param">norm_type</em>, <em class="sig-param">channel_size</em>, <em class="sig-param">shape='BDT'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/tcn.html#choose_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.tcn.choose_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>The input of normalization will be (M, C, K), where M is batch size.</p>
<p>C is channel size and K is sequence length.</p>
</dd></dl>

</section>
<section id="espnet2-enh-layers-dnn-wpe">
<span id="id10"></span><h2>espnet2.enh.layers.dnn_wpe<a class="headerlink" href="#espnet2-enh-layers-dnn-wpe" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.dnn_wpe"></span><dl class="class">
<dt id="espnet2.enh.layers.dnn_wpe.DNN_WPE">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dnn_wpe.</code><code class="sig-name descname">DNN_WPE</code><span class="sig-paren">(</span><em class="sig-param">wtype: str = 'blstmp'</em>, <em class="sig-param">widim: int = 257</em>, <em class="sig-param">wlayers: int = 3</em>, <em class="sig-param">wunits: int = 300</em>, <em class="sig-param">wprojs: int = 320</em>, <em class="sig-param">dropout_rate: float = 0.0</em>, <em class="sig-param">taps: int = 5</em>, <em class="sig-param">delay: int = 3</em>, <em class="sig-param">use_dnn_mask: bool = True</em>, <em class="sig-param">nmask: int = 1</em>, <em class="sig-param">nonlinear: str = 'sigmoid'</em>, <em class="sig-param">iterations: int = 1</em>, <em class="sig-param">normalization: bool = False</em>, <em class="sig-param">eps: float = 1e-06</em>, <em class="sig-param">diagonal_loading: bool = True</em>, <em class="sig-param">diag_eps: float = 1e-07</em>, <em class="sig-param">mask_flooring: bool = False</em>, <em class="sig-param">flooring_thres: float = 1e-06</em>, <em class="sig-param">use_torch_solver: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_wpe.html#DNN_WPE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_wpe.DNN_WPE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.dnn_wpe.DNN_WPE.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">data: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.LongTensor</em><span class="sig-paren">)</span> &#x2192; Tuple[Union[torch.Tensor, torch_complex.tensor.ComplexTensor], torch.LongTensor, Union[torch.Tensor, torch_complex.tensor.ComplexTensor]]<a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_wpe.html#DNN_WPE.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_wpe.DNN_WPE.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>DNN_WPE forward function.</p>
<dl class="simple">
<dt>Notation:</dt><dd><p>B: Batch
C: Channel
T: Time or Sequence length
F: Freq or Some dimension of the feature vector</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – (B, T, C, F)</p></li>
<li><p><strong>ilens</strong> – (B,)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, T, C, F)
ilens: (B,)
masks (torch.Tensor or List[torch.Tensor]): (B, T, C, F)
power (List[torch.Tensor]): (B, F, T)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>enhanced (torch.Tensor or List[torch.Tensor])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.dnn_wpe.DNN_WPE.predict_mask">
<code class="sig-name descname">predict_mask</code><span class="sig-paren">(</span><em class="sig-param">data: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.LongTensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.LongTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/dnn_wpe.html#DNN_WPE.predict_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dnn_wpe.DNN_WPE.predict_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict mask for WPE dereverberation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, T, C, F), double precision</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – (B,)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, T, C, F)
ilens (torch.Tensor): (B,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masks (torch.Tensor or List[torch.Tensor])</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-layers-complexnn">
<span id="id11"></span><h2>espnet2.enh.layers.complexnn<a class="headerlink" href="#espnet2-enh-layers-complexnn" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.complexnn"></span><dl class="class">
<dt id="espnet2.enh.layers.complexnn.ComplexBatchNorm">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.complexnn.</code><code class="sig-name descname">ComplexBatchNorm</code><span class="sig-paren">(</span><em class="sig-param">num_features</em>, <em class="sig-param">eps=1e-05</em>, <em class="sig-param">momentum=0.1</em>, <em class="sig-param">affine=True</em>, <em class="sig-param">track_running_stats=True</em>, <em class="sig-param">complex_axis=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexBatchNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexBatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.complexnn.ComplexBatchNorm.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexBatchNorm.extra_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexBatchNorm.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.complexnn.ComplexBatchNorm.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexBatchNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexBatchNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.complexnn.ComplexBatchNorm.reset_parameters">
<code class="sig-name descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexBatchNorm.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexBatchNorm.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.complexnn.ComplexBatchNorm.reset_running_stats">
<code class="sig-name descname">reset_running_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexBatchNorm.reset_running_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexBatchNorm.reset_running_stats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.complexnn.ComplexConv2d">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.complexnn.</code><code class="sig-name descname">ComplexConv2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">stride=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">padding=(0</em>, <em class="sig-param">0)</em>, <em class="sig-param">dilation=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">causal=True</em>, <em class="sig-param">complex_axis=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>ComplexConv2d.</p>
<p>in_channels: real+imag
out_channels: real+imag
kernel_size : input [B,C,D,T] kernel size in [D,T]
padding : input [B,C,D,T] padding in [D,T]
causal: if causal, will padding time dimension’s left side,</p>
<blockquote>
<div><p>otherwise both</p>
</div></blockquote>
<dl class="method">
<dt id="espnet2.enh.layers.complexnn.ComplexConv2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexConv2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexConv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.complexnn.ComplexConvTranspose2d">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.complexnn.</code><code class="sig-name descname">ComplexConvTranspose2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">stride=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">padding=(0</em>, <em class="sig-param">0)</em>, <em class="sig-param">output_padding=(0</em>, <em class="sig-param">0)</em>, <em class="sig-param">causal=False</em>, <em class="sig-param">complex_axis=1</em>, <em class="sig-param">groups=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexConvTranspose2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>ComplexConvTranspose2d.</p>
<p>in_channels: real+imag
out_channels: real+imag</p>
<dl class="method">
<dt id="espnet2.enh.layers.complexnn.ComplexConvTranspose2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#ComplexConvTranspose2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.ComplexConvTranspose2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.complexnn.NavieComplexLSTM">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.complexnn.</code><code class="sig-name descname">NavieComplexLSTM</code><span class="sig-paren">(</span><em class="sig-param">input_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">projection_dim=None</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">batch_first=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#NavieComplexLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.NavieComplexLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.complexnn.NavieComplexLSTM.flatten_parameters">
<code class="sig-name descname">flatten_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#NavieComplexLSTM.flatten_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.NavieComplexLSTM.flatten_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.complexnn.NavieComplexLSTM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#NavieComplexLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.NavieComplexLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.complexnn.complex_cat">
<code class="sig-prename descclassname">espnet2.enh.layers.complexnn.</code><code class="sig-name descname">complex_cat</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">axis</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/complexnn.html#complex_cat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.complexnn.complex_cat" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-enh-layers-ifasnet">
<span id="id12"></span><h2>espnet2.enh.layers.ifasnet<a class="headerlink" href="#espnet2-enh-layers-ifasnet" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.ifasnet"></span><dl class="class">
<dt id="espnet2.enh.layers.ifasnet.iFaSNet">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.ifasnet.</code><code class="sig-name descname">iFaSNet</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/ifasnet.html#iFaSNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.ifasnet.iFaSNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.layers.fasnet.FaSNet_base" title="espnet2.enh.layers.fasnet.FaSNet_base"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.layers.fasnet.FaSNet_base</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.layers.ifasnet.iFaSNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">num_mic</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/ifasnet.html#iFaSNet.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.ifasnet.iFaSNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>abstract forward function</p>
<p>input: shape (batch, max_num_ch, T)
num_mic: shape (batch, ), the number of channels for each input.</p>
<blockquote>
<div><p>Zero for fixed geometry configuration.</p>
</div></blockquote>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.ifasnet.test_model">
<code class="sig-prename descclassname">espnet2.enh.layers.ifasnet.</code><code class="sig-name descname">test_model</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/ifasnet.html#test_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.ifasnet.test_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-enh-layers-dprnn">
<span id="id13"></span><h2>espnet2.enh.layers.dprnn<a class="headerlink" href="#espnet2-enh-layers-dprnn" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.dprnn"></span><dl class="class">
<dt id="espnet2.enh.layers.dprnn.DPRNN">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dprnn.</code><code class="sig-name descname">DPRNN</code><span class="sig-paren">(</span><em class="sig-param">rnn_type</em>, <em class="sig-param">input_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">output_size</em>, <em class="sig-param">dropout=0</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">bidirectional=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#DPRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.DPRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Deep dual-path RNN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rnn_type</strong> – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.</p></li>
<li><p><strong>input_size</strong> – int, dimension of the input feature. The input should have shape
(batch, seq_len, input_size).</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>output_size</strong> – int, dimension of the output size.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>num_layers</strong> – int, number of stacked RNN layers. Default is 1.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the RNN layers are bidirectional. Default is True.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dprnn.DPRNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#DPRNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.DPRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dprnn.DPRNN_TAC">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dprnn.</code><code class="sig-name descname">DPRNN_TAC</code><span class="sig-paren">(</span><em class="sig-param">rnn_type</em>, <em class="sig-param">input_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">output_size</em>, <em class="sig-param">dropout=0</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">bidirectional=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#DPRNN_TAC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.DPRNN_TAC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Deep duaL-path RNN with TAC applied to each layer/block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rnn_type</strong> – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.</p></li>
<li><p><strong>input_size</strong> – int, dimension of the input feature. The input should
have shape (batch, seq_len, input_size).</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>output_size</strong> – int, dimension of the output size.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>num_layers</strong> – int, number of stacked RNN layers. Default is 1.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the RNN layers are bidirectional.
Default is False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dprnn.DPRNN_TAC.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">num_mic</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#DPRNN_TAC.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.DPRNN_TAC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dprnn.SingleRNN">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dprnn.</code><code class="sig-name descname">SingleRNN</code><span class="sig-paren">(</span><em class="sig-param">rnn_type</em>, <em class="sig-param">input_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">dropout=0</em>, <em class="sig-param">bidirectional=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#SingleRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.SingleRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Container module for a single RNN layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rnn_type</strong> – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.</p></li>
<li><p><strong>input_size</strong> – int, dimension of the input feature. The input should have shape
(batch, seq_len, input_size).</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the RNN layers are bidirectional. Default is False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dprnn.SingleRNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#SingleRNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.SingleRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.dprnn.merge_feature">
<code class="sig-prename descclassname">espnet2.enh.layers.dprnn.</code><code class="sig-name descname">merge_feature</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">rest</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#merge_feature"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.merge_feature" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.dprnn.split_feature">
<code class="sig-prename descclassname">espnet2.enh.layers.dprnn.</code><code class="sig-name descname">split_feature</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">segment_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dprnn.html#split_feature"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dprnn.split_feature" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-enh-layers-init">
<span id="id14"></span><h2>espnet2.enh.layers.__init__<a class="headerlink" href="#espnet2-enh-layers-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.__init__"></span></section>
<section id="espnet2-enh-layers-beamformer">
<span id="id15"></span><h2>espnet2.enh.layers.beamformer<a class="headerlink" href="#espnet2-enh-layers-beamformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.beamformer"></span><p>Beamformer module.</p>
<dl class="function">
<dt id="espnet2.enh.layers.beamformer.apply_beamforming_vector">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">apply_beamforming_vector</code><span class="sig-paren">(</span><em class="sig-param">beamform_vector: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], mix: Union[torch.Tensor, torch_complex.tensor.ComplexTensor]</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#apply_beamforming_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.apply_beamforming_vector" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.blind_analytic_normalization">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">blind_analytic_normalization</code><span class="sig-paren">(</span><em class="sig-param">ws</em>, <em class="sig-param">psd_noise</em>, <em class="sig-param">eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#blind_analytic_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.blind_analytic_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Blind analytic normalization (BAN) for post-filtering</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> (<em>torch.complex64/ComplexTensor</em>) – beamformer vector (…, F, C)</p></li>
<li><p><strong>psd_noise</strong> (<em>torch.complex64/ComplexTensor</em>) – noise PSD matrix (…, F, C, C)</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>normalized beamformer vector (…, F)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ws_ban (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.generalized_eigenvalue_decomposition">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">generalized_eigenvalue_decomposition</code><span class="sig-paren">(</span><em class="sig-param">a: torch.Tensor</em>, <em class="sig-param">b: torch.Tensor</em>, <em class="sig-param">eps=1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#generalized_eigenvalue_decomposition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.generalized_eigenvalue_decomposition" title="Permalink to this definition">¶</a></dt>
<dd><p>Solves the generalized eigenvalue decomposition through Cholesky decomposition.</p>
<p>ported from <a class="reference external" href="https://github.com/asteroid-team/asteroid/blob/master/asteroid/dsp/beamforming.py#L464">https://github.com/asteroid-team/asteroid/blob/master/asteroid/dsp/beamforming.py#L464</a></p>
<p>a &#64; e_vec = e_val * b &#64; e_vec
|
|   Cholesky decomposition on <cite>b</cite>:
|       b = L &#64; L^H, where <cite>L</cite> is a lower triangular matrix
|
|   Let C = L^-1 &#64; a &#64; L^-H, it is Hermitian.
|
=&gt; C &#64; y = lambda * y
=&gt; e_vec = L^-H &#64; y</p>
<p>Reference: <a class="reference external" href="https://www.netlib.org/lapack/lug/node54.html">https://www.netlib.org/lapack/lug/node54.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> – A complex Hermitian or real symmetric matrix whose eigenvalues and
eigenvectors will be computed. (…, C, C)</p></li>
<li><p><strong>b</strong> – A complex Hermitian or real symmetric definite positive matrix. (…, C, C)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>generalized eigenvalues (ascending order)
e_vec: generalized eigenvectors</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>e_val</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_WPD_filter">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_WPD_filter</code><span class="sig-paren">(</span><em class="sig-param">Phi: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], Rf: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], reference_vector: torch.Tensor, use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_WPD_filter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_WPD_filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the WPD vector.</p>
<blockquote>
<div><p>WPD is the Weighted Power minimization Distortionless response
convolutional beamformer. As follows:</p>
<p>h = (Rf^-1 &#64; Phi_{xx}) / tr[(Rf^-1) &#64; Phi_{xx}] &#64; u</p>
</div></blockquote>
<dl class="simple">
<dt>Reference:</dt><dd><p>T. Nakatani and K. Kinoshita, “A Unified Convolutional Beamformer
for Simultaneous Denoising and Dereverberation,” in IEEE Signal
Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi:
10.1109/LSP.2019.2911179.
<a class="reference external" href="https://ieeexplore.ieee.org/document/8691481">https://ieeexplore.ieee.org/document/8691481</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Phi</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, F, (btaps+1) * C, (btaps+1) * C)
is the PSD of zero-padded speech [x^T(t,f) 0 … 0]^T.</p></li>
<li><p><strong>Rf</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, F, (btaps+1) * C, (btaps+1) * C)
is the power normalized spatio-temporal covariance matrix.</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em>) – (B, (btaps+1) * C)
is the reference_vector.</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, F, (btaps + 1) * C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>filter_matrix (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_WPD_filter_v2">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_WPD_filter_v2</code><span class="sig-paren">(</span><em class="sig-param">Phi: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], Rf: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], reference_vector: torch.Tensor, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_WPD_filter_v2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_WPD_filter_v2" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the WPD vector (v2).</p>
<blockquote>
<div><dl class="simple">
<dt>This implementation is more efficient than <cite>get_WPD_filter</cite> as</dt><dd><p>it skips unnecessary computation with zeros.</p>
</dd>
</dl>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Phi</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, F, C, C)
is speech PSD.</p></li>
<li><p><strong>Rf</strong> (<em>torch.complex64/ComplexTensor</em>) – (B, F, (btaps+1) * C, (btaps+1) * C)
is the power normalized spatio-temporal covariance matrix.</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em>) – (B, C)
is the reference_vector.</p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, F, (btaps+1) * C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>filter_matrix (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_WPD_filter_with_rtf">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_WPD_filter_with_rtf</code><span class="sig-paren">(</span><em class="sig-param">psd_observed_bar: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], psd_speech: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], psd_noise: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], iterations: int = 3, reference_vector: Union[int, torch.Tensor, None] = None, normalize_ref_channel: Optional[int] = None, use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-15</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_WPD_filter_with_rtf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_WPD_filter_with_rtf" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the WPD vector calculated with RTF.</p>
<blockquote>
<div><p>WPD is the Weighted Power minimization Distortionless response
convolutional beamformer. As follows:</p>
<p>h = (Rf^-1 &#64; vbar) / (vbar^H &#64; R^-1 &#64; vbar)</p>
</div></blockquote>
<dl class="simple">
<dt>Reference:</dt><dd><p>T. Nakatani and K. Kinoshita, “A Unified Convolutional Beamformer
for Simultaneous Denoising and Dereverberation,” in IEEE Signal
Processing Letters, vol. 26, no. 6, pp. 903-907, June 2019, doi:
10.1109/LSP.2019.2911179.
<a class="reference external" href="https://ieeexplore.ieee.org/document/8691481">https://ieeexplore.ieee.org/document/8691481</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_observed_bar</strong> (<em>torch.complex64/ComplexTensor</em>) – stacked observation covariance matrix</p></li>
<li><p><strong>psd_speech</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_noise</strong> (<em>torch.complex64/ComplexTensor</em>) – noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>iterations</strong> (<em>int</em>) – number of iterations in power method</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, C) or scalar</p></li>
<li><p><strong>normalize_ref_channel</strong> (<em>int</em>) – reference channel for normalizing the RTF</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)r</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_covariances">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_covariances</code><span class="sig-paren">(</span><em class="sig-param">Y: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], inverse_power: torch.Tensor, bdelay: int, btaps: int, get_vector: bool = False</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_covariances"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_covariances" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Calculates the power normalized spatio-temporal covariance</dt><dd><p>matrix of the framed signal.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Y</strong> – Complex STFT signal with shape (B, F, C, T)</p></li>
<li><p><strong>inverse_power</strong> – Weighting factor with shape (B, F, T)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, F, (btaps+1) * C, (btaps+1) * C)
Correlation vector: (B, F, btaps + 1, C, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Correlation matrix</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_gev_vector">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_gev_vector</code><span class="sig-paren">(</span><em class="sig-param">psd_noise: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], psd_speech: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], mode='power', reference_vector: Union[int, torch.Tensor] = 0, iterations: int = 3, use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_gev_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_gev_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the generalized eigenvalue (GEV) beamformer vector:</p>
<blockquote>
<div><p>psd_speech &#64; h = lambda * psd_noise &#64; h</p>
</div></blockquote>
<dl class="simple">
<dt>Reference:</dt><dd><p>Blind acoustic beamforming based on generalized eigenvalue decomposition;
E. Warsitz and R. Haeb-Umbach, 2007.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_noise</strong> (<em>torch.complex64/ComplexTensor</em>) – noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_speech</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>mode</strong> (<em>str</em>) – one of (“power”, “evd”)
“power”: power method
“evd”: eigenvalue decomposition (only for torch builtin complex tensors)</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, C) or scalar</p></li>
<li><p><strong>iterations</strong> (<em>int</em>) – number of iterations in power method</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_lcmv_vector_with_rtf">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_lcmv_vector_with_rtf</code><span class="sig-paren">(</span><em class="sig-param">psd_n: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], rtf_mat: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], reference_vector: Union[int, torch.Tensor, None] = None, use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_lcmv_vector_with_rtf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_lcmv_vector_with_rtf" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Return the LCMV (Linearly Constrained Minimum Variance) vector</dt><dd><p>calculated with RTF:</p>
<p>h = (Npsd^-1 &#64; rtf_mat) &#64; (rtf_mat^H &#64; Npsd^-1 &#64; rtf_mat)^-1 &#64; p</p>
</dd>
<dt>Reference:</dt><dd><p>H. L. Van Trees, “Optimum array processing: Part IV of detection, estimation,
and modulation theory,” John Wiley &amp; Sons, 2004. (Chapter 6.7)</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_n</strong> (<em>torch.complex64/ComplexTensor</em>) – observation/noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>rtf_mat</strong> (<em>torch.complex64/ComplexTensor</em>) – RTF matrix (…, F, C, num_spk)</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, num_spk) or scalar</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_mvdr_vector">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_mvdr_vector</code><span class="sig-paren">(</span><em class="sig-param">psd_s</em>, <em class="sig-param">psd_n</em>, <em class="sig-param">reference_vector: torch.Tensor</em>, <em class="sig-param">use_torch_solver: bool = True</em>, <em class="sig-param">diagonal_loading: bool = True</em>, <em class="sig-param">diag_eps: float = 1e-07</em>, <em class="sig-param">eps: float = 1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_mvdr_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_mvdr_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the MVDR (Minimum Variance Distortionless Response) vector:</p>
<blockquote>
<div><p>h = (Npsd^-1 &#64; Spsd) / (Tr(Npsd^-1 &#64; Spsd)) &#64; u</p>
</div></blockquote>
<dl class="simple">
<dt>Reference:</dt><dd><p>On optimal frequency-domain multichannel linear filtering
for noise reduction; M. Souden et al., 2010;
<a class="reference external" href="https://ieeexplore.ieee.org/document/5089420">https://ieeexplore.ieee.org/document/5089420</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_s</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_n</strong> (<em>torch.complex64/ComplexTensor</em>) – observation/noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em>) – (…, C)</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_mvdr_vector_with_rtf">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_mvdr_vector_with_rtf</code><span class="sig-paren">(</span><em class="sig-param">psd_n: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], psd_speech: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], psd_noise: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], iterations: int = 3, reference_vector: Union[int, torch.Tensor, None] = None, normalize_ref_channel: Optional[int] = None, use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_mvdr_vector_with_rtf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_mvdr_vector_with_rtf" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Return the MVDR (Minimum Variance Distortionless Response) vector</dt><dd><p>calculated with RTF:</p>
<p>h = (Npsd^-1 &#64; rtf) / (rtf^H &#64; Npsd^-1 &#64; rtf)</p>
</dd>
<dt>Reference:</dt><dd><p>On optimal frequency-domain multichannel linear filtering
for noise reduction; M. Souden et al., 2010;
<a class="reference external" href="https://ieeexplore.ieee.org/document/5089420">https://ieeexplore.ieee.org/document/5089420</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_n</strong> (<em>torch.complex64/ComplexTensor</em>) – observation/noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_speech</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_noise</strong> (<em>torch.complex64/ComplexTensor</em>) – noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>iterations</strong> (<em>int</em>) – number of iterations in power method</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, C) or scalar</p></li>
<li><p><strong>normalize_ref_channel</strong> (<em>int</em>) – reference channel for normalizing the RTF</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_mwf_vector">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_mwf_vector</code><span class="sig-paren">(</span><em class="sig-param">psd_s, psd_n, reference_vector: Union[torch.Tensor, int], use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_mwf_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_mwf_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the MWF (Minimum Multi-channel Wiener Filter) vector:</p>
<blockquote>
<div><p>h = (Npsd^-1 &#64; Spsd) &#64; u</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_s</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_n</strong> (<em>torch.complex64/ComplexTensor</em>) – power-normalized observation covariance matrix (…, F, C, C)</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, C) or scalar</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_power_spectral_density_matrix">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_power_spectral_density_matrix</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">mask</em>, <em class="sig-param">normalization=True</em>, <em class="sig-param">reduction='mean'</em>, <em class="sig-param">eps: float = 1e-15</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_power_spectral_density_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_power_spectral_density_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Return cross-channel power spectral density (PSD) matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>torch.complex64/ComplexTensor</em>) – (…, F, C, T)</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – “mean” or “median”</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – (…, F, C, T)</p></li>
<li><p><strong>normalization</strong> (<em>bool</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns</dt><dd><p>psd (torch.complex64/ComplexTensor): (…, F, C, C)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_rank1_mwf_vector">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_rank1_mwf_vector</code><span class="sig-paren">(</span><em class="sig-param">psd_speech, psd_noise, reference_vector: Union[torch.Tensor, int], denoising_weight: float = 1.0, approx_low_rank_psd_speech: bool = False, iterations: int = 3, use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_rank1_mwf_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_rank1_mwf_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the R1-MWF (Rank-1 Multi-channel Wiener Filter) vector</p>
<blockquote>
<div><p>h = (Npsd^-1 &#64; Spsd) / (mu + Tr(Npsd^-1 &#64; Spsd)) &#64; u</p>
</div></blockquote>
<dl class="simple">
<dt>Reference:</dt><dd><p>[1] Rank-1 constrained multichannel Wiener filter for speech recognition in
noisy environments; Z. Wang et al, 2018
<a class="reference external" href="https://hal.inria.fr/hal-01634449/document">https://hal.inria.fr/hal-01634449/document</a>
[2] Low-rank approximation based multichannel Wiener filter algorithms for
noise reduction with application in cochlear implants; R. Serizel, 2014
<a class="reference external" href="https://ieeexplore.ieee.org/document/6730918">https://ieeexplore.ieee.org/document/6730918</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_speech</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_noise</strong> (<em>torch.complex64/ComplexTensor</em>) – noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, C) or scalar</p></li>
<li><p><strong>denoising_weight</strong> (<em>float</em>) – a trade-off parameter between noise reduction and
speech distortion.
A larger value leads to more noise reduction at the expense of more speech
distortion.
When <cite>denoising_weight = 0</cite>, it corresponds to MVDR beamformer.</p></li>
<li><p><strong>approx_low_rank_psd_speech</strong> (<em>bool</em>) – whether to replace original input psd_speech
with its low-rank approximation as in [1]</p></li>
<li><p><strong>iterations</strong> (<em>int</em>) – number of iterations in power method, only used when
<cite>approx_low_rank_psd_speech = True</cite></p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_rtf">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_rtf</code><span class="sig-paren">(</span><em class="sig-param">psd_speech</em>, <em class="sig-param">psd_noise</em>, <em class="sig-param">mode='power'</em>, <em class="sig-param">reference_vector: Union[int</em>, <em class="sig-param">torch.Tensor] = 0</em>, <em class="sig-param">iterations: int = 3</em>, <em class="sig-param">use_torch_solver: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_rtf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_rtf" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the relative transfer function (RTF)</p>
<dl class="simple">
<dt>Algorithm of power method:</dt><dd><ol class="arabic simple">
<li><p>rtf = reference_vector</p></li>
<li><dl class="simple">
<dt>for i in range(iterations):</dt><dd><p>rtf = (psd_noise^-1 &#64; psd_speech) &#64; rtf
rtf = rtf / ||rtf||_2  # this normalization can be skipped</p>
</dd>
</dl>
</li>
<li><p>rtf = psd_noise &#64; rtf</p></li>
<li><p>rtf = rtf / rtf[…, ref_channel, :]</p></li>
</ol>
</dd>
</dl>
<p>Note: 4) Normalization at the reference channel is not performed here.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_speech</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_noise</strong> (<em>torch.complex64/ComplexTensor</em>) – noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>mode</strong> (<em>str</em>) – one of (“power”, “evd”)
“power”: power method
“evd”: eigenvalue decomposition</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, C) or scalar</p></li>
<li><p><strong>iterations</strong> (<em>int</em>) – number of iterations in power method</p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C, 1)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>rtf (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_rtf_matrix">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_rtf_matrix</code><span class="sig-paren">(</span><em class="sig-param">psd_speeches</em>, <em class="sig-param">psd_noises</em>, <em class="sig-param">diagonal_loading: bool = True</em>, <em class="sig-param">ref_channel: int = 0</em>, <em class="sig-param">rtf_iterations: int = 3</em>, <em class="sig-param">use_torch_solver: bool = True</em>, <em class="sig-param">diag_eps: float = 1e-07</em>, <em class="sig-param">eps: float = 1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_rtf_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_rtf_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the RTF matrix with each column the relative transfer function
of the corresponding source.</p>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.get_sdw_mwf_vector">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">get_sdw_mwf_vector</code><span class="sig-paren">(</span><em class="sig-param">psd_speech, psd_noise, reference_vector: Union[torch.Tensor, int], denoising_weight: float = 1.0, approx_low_rank_psd_speech: bool = False, iterations: int = 3, use_torch_solver: bool = True, diagonal_loading: bool = True, diag_eps: float = 1e-07, eps: float = 1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#get_sdw_mwf_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.get_sdw_mwf_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the SDW-MWF (Speech Distortion Weighted Multi-channel Wiener Filter) vector</p>
<blockquote>
<div><p>h = (Spsd + mu * Npsd)^-1 &#64; Spsd &#64; u</p>
</div></blockquote>
<dl class="simple">
<dt>Reference:</dt><dd><p>[1] Spatially pre-processed speech distortion weighted multi-channel Wiener
filtering for noise reduction; A. Spriet et al, 2004
<a class="reference external" href="https://dl.acm.org/doi/abs/10.1016/j.sigpro.2004.07.028">https://dl.acm.org/doi/abs/10.1016/j.sigpro.2004.07.028</a>
[2] Rank-1 constrained multichannel Wiener filter for speech recognition in
noisy environments; Z. Wang et al, 2018
<a class="reference external" href="https://hal.inria.fr/hal-01634449/document">https://hal.inria.fr/hal-01634449/document</a>
[3] Low-rank approximation based multichannel Wiener filter algorithms for
noise reduction with application in cochlear implants; R. Serizel, 2014
<a class="reference external" href="https://ieeexplore.ieee.org/document/6730918">https://ieeexplore.ieee.org/document/6730918</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>psd_speech</strong> (<em>torch.complex64/ComplexTensor</em>) – speech covariance matrix (…, F, C, C)</p></li>
<li><p><strong>psd_noise</strong> (<em>torch.complex64/ComplexTensor</em>) – noise covariance matrix (…, F, C, C)</p></li>
<li><p><strong>reference_vector</strong> (<em>torch.Tensor</em><em> or </em><em>int</em>) – (…, C) or scalar</p></li>
<li><p><strong>denoising_weight</strong> (<em>float</em>) – a trade-off parameter between noise reduction and
speech distortion.
A larger value leads to more noise reduction at the expense of more speech
distortion.
The plain MWF is obtained with <cite>denoising_weight = 1</cite> (by default).</p></li>
<li><p><strong>approx_low_rank_psd_speech</strong> (<em>bool</em>) – whether to replace original input psd_speech
with its low-rank approximation as in [2]</p></li>
<li><p><strong>iterations</strong> (<em>int</em>) – number of iterations in power method, only used when
<cite>approx_low_rank_psd_speech = True</cite></p></li>
<li><p><strong>use_torch_solver</strong> (<em>bool</em>) – Whether to use <cite>solve</cite> instead of <cite>inverse</cite></p></li>
<li><p><strong>diagonal_loading</strong> (<em>bool</em>) – Whether to add a tiny term to the diagonal of psd_n</p></li>
<li><p><strong>diag_eps</strong> (<em>float</em>) – </p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(…, F, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamform_vector (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.gev_phase_correction">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">gev_phase_correction</code><span class="sig-paren">(</span><em class="sig-param">vector</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#gev_phase_correction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.gev_phase_correction" title="Permalink to this definition">¶</a></dt>
<dd><p>Phase correction to reduce distortions due to phase inconsistencies.</p>
<p>ported from <a class="reference external" href="https://github.com/fgnt/nn-gev/blob/master/fgnt/beamforming.py#L169">https://github.com/fgnt/nn-gev/blob/master/fgnt/beamforming.py#L169</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>vector</strong> – Beamforming vector with shape (…, F, C)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Phase corrected beamforming vectors</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>w</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.perform_WPD_filtering">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">perform_WPD_filtering</code><span class="sig-paren">(</span><em class="sig-param">filter_matrix: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], Y: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], bdelay: int, btaps: int</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#perform_WPD_filtering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.perform_WPD_filtering" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform WPD filtering.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filter_matrix</strong> – Filter matrix (B, F, (btaps + 1) * C)</p></li>
<li><p><strong>Y</strong> – Complex STFT signal with shape (B, F, C, T)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, F, T)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>enhanced (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.prepare_beamformer_stats">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">prepare_beamformer_stats</code><span class="sig-paren">(</span><em class="sig-param">signal</em>, <em class="sig-param">masks_speech</em>, <em class="sig-param">mask_noise</em>, <em class="sig-param">powers=None</em>, <em class="sig-param">beamformer_type='mvdr'</em>, <em class="sig-param">bdelay=3</em>, <em class="sig-param">btaps=5</em>, <em class="sig-param">eps=1e-06</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#prepare_beamformer_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.prepare_beamformer_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare necessary statistics for constructing the specified beamformer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>signal</strong> (<em>torch.complex64/ComplexTensor</em>) – (…, F, C, T)</p></li>
<li><p><strong>masks_speech</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – (…, F, C, T) masks for all speech sources</p></li>
<li><p><strong>mask_noise</strong> (<em>torch.Tensor</em>) – (…, F, C, T) noise mask</p></li>
<li><p><strong>powers</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – powers for all speech sources (…, F, T)
used for wMPDR or WPD beamformers</p></li>
<li><p><strong>beamformer_type</strong> (<em>str</em>) – one of the pre-defined beamformer types</p></li>
<li><p><strong>bdelay</strong> (<em>int</em>) – delay factor, used for WPD beamformser</p></li>
<li><p><strong>btaps</strong> (<em>int</em>) – number of filter taps, used for WPD beamformser</p></li>
<li><p><strong>eps</strong> (<em>torch.Tensor</em>) – tiny constant</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>a dictionary containing all necessary statistics</dt><dd><p>e.g. “psd_n”, “psd_speech”, “psd_distortion”
Note:
* When <cite>masks_speech</cite> is a tensor or a single-element list, all returned</p>
<blockquote>
<div><p>statistics are tensors;</p>
</div></blockquote>
<ul class="simple">
<li><p>When <cite>masks_speech</cite> is a multi-element list, some returned statistics
can be a list, e.g., “psd_n” for MVDR, “psd_speech” and “psd_distortion”.</p></li>
</ul>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>beamformer_stats (dict)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.signal_framing">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">signal_framing</code><span class="sig-paren">(</span><em class="sig-param">signal: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], frame_length: int, frame_step: int, bdelay: int, do_padding: bool = False, pad_value: int = 0, indices: List = None</em><span class="sig-paren">)</span> &#x2192; Union[torch.Tensor, torch_complex.tensor.ComplexTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#signal_framing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.signal_framing" title="Permalink to this definition">¶</a></dt>
<dd><p>Expand <cite>signal</cite> into several frames, with each frame of length <cite>frame_length</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>signal</strong> – (…, T)</p></li>
<li><p><strong>frame_length</strong> – length of each segment</p></li>
<li><p><strong>frame_step</strong> – step for selecting frames</p></li>
<li><p><strong>bdelay</strong> – delay for WPD</p></li>
<li><p><strong>do_padding</strong> – whether or not to pad the input signal at the beginning
of the time dimension</p></li>
<li><p><strong>pad_value</strong> – value to fill in the padding</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>if do_padding: (…, T, frame_length)
else:          (…, T - bdelay - frame_length + 2, frame_length)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.beamformer.tik_reg">
<code class="sig-prename descclassname">espnet2.enh.layers.beamformer.</code><code class="sig-name descname">tik_reg</code><span class="sig-paren">(</span><em class="sig-param">mat</em>, <em class="sig-param">reg: float = 1e-08</em>, <em class="sig-param">eps: float = 1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/beamformer.html#tik_reg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.beamformer.tik_reg" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Tikhonov regularization (only modifying real part).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mat</strong> (<em>torch.complex64/ComplexTensor</em>) – input matrix (…, C, C)</p></li>
<li><p><strong>reg</strong> (<em>float</em>) – regularization factor</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>regularized matrix (…, C, C)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ret (torch.complex64/ComplexTensor)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="espnet2-enh-layers-dc-crn">
<span id="id16"></span><h2>espnet2.enh.layers.dc_crn<a class="headerlink" href="#espnet2-enh-layers-dc-crn" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.dc_crn"></span><dl class="class">
<dt id="espnet2.enh.layers.dc_crn.DC_CRN">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dc_crn.</code><code class="sig-name descname">DC_CRN</code><span class="sig-paren">(</span><em class="sig-param">input_dim, input_channels: List = [2, 16, 32, 64, 128, 256], enc_hid_channels=8, enc_kernel_size=(1, 3), enc_padding=(0, 1), enc_last_kernel_size=(1, 4), enc_last_stride=(1, 2), enc_last_padding=(0, 1), enc_layers=5, skip_last_kernel_size=(1, 3), skip_last_stride=(1, 1), skip_last_padding=(0, 1), glstm_groups=2, glstm_layers=2, glstm_bidirectional=False, glstm_rearrange=False, output_channels=2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#DC_CRN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.DC_CRN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Densely-Connected Convolutional Recurrent Network (DC-CRN).</p>
<p>Reference: Fig. 3 and Section III-B in [1]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input feature dimension</p></li>
<li><p><strong>input_channels</strong> (<em>list</em>) – number of input channels for the stacked
DenselyConnectedBlock layers
Its length should be (<cite>number of DenselyConnectedBlock layers</cite>).
It is recommended to use even number of channels to avoid AssertError
when <cite>glstm_bidirectional=True</cite>.</p></li>
<li><p><strong>enc_hid_channels</strong> (<em>int</em>) – common number of intermediate channels for all
DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_kernel_size</strong> (<em>tuple</em>) – common kernel size for all DenselyConnectedBlock
of the encoder</p></li>
<li><p><strong>enc_padding</strong> (<em>tuple</em>) – common padding for all DenselyConnectedBlock
of the encoder</p></li>
<li><p><strong>enc_last_kernel_size</strong> (<em>tuple</em>) – common kernel size for the last Conv layer
in all DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_last_stride</strong> (<em>tuple</em>) – common stride for the last Conv layer in all
DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_last_padding</strong> (<em>tuple</em>) – common padding for the last Conv layer in all
DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_layers</strong> (<em>int</em>) – common total number of Conv layers for all
DenselyConnectedBlock layers of the encoder</p></li>
<li><p><strong>skip_last_kernel_size</strong> (<em>tuple</em>) – common kernel size for the last Conv layer
in all DenselyConnectedBlock of the skip pathways</p></li>
<li><p><strong>skip_last_stride</strong> (<em>tuple</em>) – common stride for the last Conv layer in all
DenselyConnectedBlock of the skip pathways</p></li>
<li><p><strong>skip_last_padding</strong> (<em>tuple</em>) – common padding for the last Conv layer in all
DenselyConnectedBlock of the skip pathways</p></li>
<li><p><strong>glstm_groups</strong> (<em>int</em>) – number of groups in each Grouped LSTM layer</p></li>
<li><p><strong>glstm_layers</strong> (<em>int</em>) – number of Grouped LSTM layers</p></li>
<li><p><strong>glstm_bidirectional</strong> (<em>bool</em>) – whether to use BLSTM or unidirectional LSTM
in Grouped LSTM layers</p></li>
<li><p><strong>glstm_rearrange</strong> (<em>bool</em>) – whether to apply the rearrange operation after each
grouped LSTM layer</p></li>
<li><p><strong>output_channels</strong> (<em>int</em>) – number of output channels (must be an even number to
recover both real and imaginary parts)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dc_crn.DC_CRN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#DC_CRN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.DC_CRN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>DC-CRN forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Concatenated real and imaginary spectrum features
(B, input_channels[0], T, F)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, 2, output_channels, T, F)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>out (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dc_crn.DenselyConnectedBlock">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dc_crn.</code><code class="sig-name descname">DenselyConnectedBlock</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">hid_channels=8</em>, <em class="sig-param">kernel_size=(1</em>, <em class="sig-param">3)</em>, <em class="sig-param">padding=(0</em>, <em class="sig-param">1)</em>, <em class="sig-param">last_kernel_size=(1</em>, <em class="sig-param">4)</em>, <em class="sig-param">last_stride=(1</em>, <em class="sig-param">2)</em>, <em class="sig-param">last_padding=(0</em>, <em class="sig-param">1)</em>, <em class="sig-param">last_output_padding=(0</em>, <em class="sig-param">0)</em>, <em class="sig-param">layers=5</em>, <em class="sig-param">transposed=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#DenselyConnectedBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.DenselyConnectedBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Densely-Connected Convolutional Block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – number of input channels</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – number of output channels</p></li>
<li><p><strong>hid_channels</strong> (<em>int</em>) – number of output channels in intermediate Conv layers</p></li>
<li><p><strong>kernel_size</strong> (<em>tuple</em>) – kernel size for all but the last Conv layers</p></li>
<li><p><strong>padding</strong> (<em>tuple</em>) – padding for all but the last Conv layers</p></li>
<li><p><strong>last_kernel_size</strong> (<em>tuple</em>) – kernel size for the last GluConv layer</p></li>
<li><p><strong>last_stride</strong> (<em>tuple</em>) – stride for the last GluConv layer</p></li>
<li><p><strong>last_padding</strong> (<em>tuple</em>) – padding for the last GluConv layer</p></li>
<li><p><strong>last_output_padding</strong> (<em>tuple</em>) – output padding for the last GluConvTranspose2d
(only used when <cite>transposed=True</cite>)</p></li>
<li><p><strong>layers</strong> (<em>int</em>) – total number of Conv layers</p></li>
<li><p><strong>transposed</strong> (<em>bool</em>) – True to use GluConvTranspose2d in the last layer
False to use GluConv2d in the last layer</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dc_crn.DenselyConnectedBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#DenselyConnectedBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.DenselyConnectedBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>DenselyConnectedBlock forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em>) – (B, C, T_in, F_in)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, C, T_out, F_out)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>out (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dc_crn.GLSTM">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dc_crn.</code><code class="sig-name descname">GLSTM</code><span class="sig-paren">(</span><em class="sig-param">hidden_size=1024</em>, <em class="sig-param">groups=2</em>, <em class="sig-param">layers=2</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">rearrange=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#GLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.GLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Grouped LSTM.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Efficient Sequence Learning with Group Recurrent Networks; Gao et al., 2018</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>) – total hidden size of all LSTMs in each grouped LSTM layer
i.e., hidden size of each LSTM is <cite>hidden_size // groups</cite></p></li>
<li><p><strong>groups</strong> (<em>int</em>) – number of LSTMs in each grouped LSTM layer</p></li>
<li><p><strong>layers</strong> (<em>int</em>) – number of grouped LSTM layers</p></li>
<li><p><strong>bidirectional</strong> (<em>bool</em>) – whether to use BLSTM or unidirectional LSTM</p></li>
<li><p><strong>rearrange</strong> (<em>bool</em>) – whether to apply the rearrange operation after each
grouped LSTM layer</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dc_crn.GLSTM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#GLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.GLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Grouped LSTM forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – (B, C, T, D)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, C, T, D)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>out (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dc_crn.GluConv2d">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dc_crn.</code><code class="sig-name descname">GluConv2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride</em>, <em class="sig-param">padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#GluConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.GluConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Conv2d with Gated Linear Units (GLU).</p>
<p>Input and output shapes are the same as regular Conv2d layers.</p>
<p>Reference: Section III-B in [1]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – number of input channels</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – number of output channels</p></li>
<li><p><strong>kernel_size</strong> (<em>int/tuple</em>) – kernel size in Conv2d</p></li>
<li><p><strong>stride</strong> (<em>int/tuple</em>) – stride size in Conv2d</p></li>
<li><p><strong>padding</strong> (<em>int/tuple</em>) – padding size in Conv2d</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dc_crn.GluConv2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#GluConv2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.GluConv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>ConvGLU forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – (B, C_in, H_in, W_in)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, C_out, H_out, W_out)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>out (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dc_crn.GluConvTranspose2d">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dc_crn.</code><code class="sig-name descname">GluConvTranspose2d</code><span class="sig-paren">(</span><em class="sig-param">in_channels</em>, <em class="sig-param">out_channels</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">stride</em>, <em class="sig-param">padding=0</em>, <em class="sig-param">output_padding=(0</em>, <em class="sig-param">0)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#GluConvTranspose2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.GluConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>ConvTranspose2d with Gated Linear Units (GLU).</p>
<p>Input and output shapes are the same as regular ConvTranspose2d layers.</p>
<p>Reference: Section III-B in [1]</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>int</em>) – number of input channels</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – number of output channels</p></li>
<li><p><strong>kernel_size</strong> (<em>int/tuple</em>) – kernel size in ConvTranspose2d</p></li>
<li><p><strong>stride</strong> (<em>int/tuple</em>) – stride size in ConvTranspose2d</p></li>
<li><p><strong>padding</strong> (<em>int/tuple</em>) – padding size in ConvTranspose2d</p></li>
<li><p><strong>output_padding</strong> (<em>int/tuple</em>) – Additional size added to one side of each
dimension in the output shape</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dc_crn.GluConvTranspose2d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dc_crn.html#GluConvTranspose2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dc_crn.GluConvTranspose2d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>DeconvGLU forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – (B, C_in, H_in, W_in)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(B, C_out, H_out, W_out)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>out (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-layers-dpmulcat">
<span id="id17"></span><h2>espnet2.enh.layers.dpmulcat<a class="headerlink" href="#espnet2-enh-layers-dpmulcat" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.dpmulcat"></span><dl class="class">
<dt id="espnet2.enh.layers.dpmulcat.DPMulCat">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dpmulcat.</code><code class="sig-name descname">DPMulCat</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">output_size: int</em>, <em class="sig-param">num_spk: int</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">input_normalize: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dpmulcat.html#DPMulCat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dpmulcat.DPMulCat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Dual-path RNN module with MulCat blocks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – int, dimension of the input feature.
The input should have shape (batch, seq_len, input_size).</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>output_size</strong> – int, dimension of the output size.</p></li>
<li><p><strong>num_spk</strong> – int, the number of speakers in the output.</p></li>
<li><p><strong>dropout</strong> – float, the dropout rate in the LSTM layer. (Default: 0.0)</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the RNN layers are bidirectional. (Default: True)</p></li>
<li><p><strong>num_layers</strong> – int, number of stacked MulCat blocks. (Default: 4)</p></li>
<li><p><strong>input_normalize</strong> – bool, whether to apply GroupNorm on the input Tensor.
(Default: False)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dpmulcat.DPMulCat.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dpmulcat.html#DPMulCat.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dpmulcat.DPMulCat.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute output after DPMulCat module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em>) – The input feature.
Tensor of shape (batch, N, dim1, dim2)
Apply RNN on dim1 first and then dim2</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>(list(torch.Tensor) or list(list(torch.Tensor))</dt><dd><p>In training mode, the module returns output of each DPMulCat block.
In eval mode, the module only returns output in the last block.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.dpmulcat.MulCatBlock">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.dpmulcat.</code><code class="sig-name descname">MulCatBlock</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">bidirectional: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dpmulcat.html#MulCatBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dpmulcat.MulCatBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The MulCat block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – int, dimension of the input feature.
The input should have shape (batch, seq_len, input_size).</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>dropout</strong> – float, the dropout rate in the LSTM layer. (Default: 0.0)</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the RNN layers are bidirectional. (Default: True)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.dpmulcat.MulCatBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/dpmulcat.html#MulCatBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.dpmulcat.MulCatBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute output after MulCatBlock.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<em>torch.Tensor</em>) – The input feature.
Tensor of shape (batch, time, feature_dim)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>The output feature after MulCatBlock.</dt><dd><p>Tensor of shape (batch, time, feature_dim)</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-layers-mask-estimator">
<span id="id18"></span><h2>espnet2.enh.layers.mask_estimator<a class="headerlink" href="#espnet2-enh-layers-mask-estimator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.mask_estimator"></span><dl class="class">
<dt id="espnet2.enh.layers.mask_estimator.MaskEstimator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.mask_estimator.</code><code class="sig-name descname">MaskEstimator</code><span class="sig-paren">(</span><em class="sig-param">type</em>, <em class="sig-param">idim</em>, <em class="sig-param">layers</em>, <em class="sig-param">units</em>, <em class="sig-param">projs</em>, <em class="sig-param">dropout</em>, <em class="sig-param">nmask=1</em>, <em class="sig-param">nonlinear='sigmoid'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/mask_estimator.html#MaskEstimator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.mask_estimator.MaskEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.mask_estimator.MaskEstimator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.LongTensor</em><span class="sig-paren">)</span> &#x2192; Tuple[Tuple[torch.Tensor, ...], torch.LongTensor]<a class="reference internal" href="../_modules/espnet2/enh/layers/mask_estimator.html#MaskEstimator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.mask_estimator.MaskEstimator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Mask estimator forward function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> – (B, F, C, T)</p></li>
<li><p><strong>ilens</strong> – (B,)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The hidden vector (B, F, C, T)
masks: A tuple of the masks. (B, F, C, T)
ilens: (B,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>hs (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-layers-skim">
<span id="id19"></span><h2>espnet2.enh.layers.skim<a class="headerlink" href="#espnet2-enh-layers-skim" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.skim"></span><dl class="class">
<dt id="espnet2.enh.layers.skim.MemLSTM">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.skim.</code><code class="sig-name descname">MemLSTM</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">mem_type='hc'</em>, <em class="sig-param">norm_type='cLN'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/skim.html#MemLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.skim.MemLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>the Mem-LSTM of SkiM</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the LSTM layers are bidirectional.
Default is False.</p></li>
<li><p><strong>mem_type</strong> – ‘hc’, ‘h’, ‘c’ or ‘id’.
It controls whether the hidden (or cell) state of
SegLSTM will be processed by MemLSTM.
In ‘id’ mode, both the hidden and cell states will
be identically returned.</p></li>
<li><p><strong>norm_type</strong> – gLN, cLN. cLN is for causal implementation.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.skim.MemLSTM.extra_repr">
<code class="sig-name descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="../_modules/espnet2/enh/layers/skim.html#MemLSTM.extra_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.skim.MemLSTM.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.skim.MemLSTM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hc</em>, <em class="sig-param">S</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/skim.html#MemLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.skim.MemLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.skim.SegLSTM">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.skim.</code><code class="sig-name descname">SegLSTM</code><span class="sig-paren">(</span><em class="sig-param">input_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">norm_type='cLN'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/skim.html#SegLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.skim.SegLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>the Seg-LSTM of SkiM</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – int, dimension of the input feature.
The input should have shape (batch, seq_len, input_size).</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the LSTM layers are bidirectional.
Default is False.</p></li>
<li><p><strong>norm_type</strong> – gLN, cLN. cLN is for causal implementation.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.skim.SegLSTM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">hc</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/skim.html#SegLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.skim.SegLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.skim.SkiM">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.skim.</code><code class="sig-name descname">SkiM</code><span class="sig-paren">(</span><em class="sig-param">input_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">output_size</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">num_blocks=2</em>, <em class="sig-param">segment_size=20</em>, <em class="sig-param">bidirectional=True</em>, <em class="sig-param">mem_type='hc'</em>, <em class="sig-param">norm_type='gLN'</em>, <em class="sig-param">seg_overlap=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/skim.html#SkiM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.skim.SkiM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Skipping Memory Net</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – int, dimension of the input feature.
Input shape shoud be (batch, length, input_size)</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>output_size</strong> – int, dimension of the output size.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>num_blocks</strong> – number of basic SkiM blocks</p></li>
<li><p><strong>segment_size</strong> – segmentation size for splitting long features</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the RNN layers are bidirectional.</p></li>
<li><p><strong>mem_type</strong> – ‘hc’, ‘h’, ‘c’, ‘id’ or None.
It controls whether the hidden (or cell) state of SegLSTM
will be processed by MemLSTM.
In ‘id’ mode, both the hidden and cell states will
be identically returned.
When mem_type is None, the MemLSTM will be removed.</p></li>
<li><p><strong>norm_type</strong> – gLN, cLN. cLN is for causal implementation.</p></li>
<li><p><strong>seg_overlap</strong> – Bool, whether the segmentation will reserve 50%
overlap for adjacent segments.Default is False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.layers.skim.SkiM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/skim.html#SkiM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.skim.SkiM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-layers-fasnet">
<span id="id20"></span><h2>espnet2.enh.layers.fasnet<a class="headerlink" href="#espnet2-enh-layers-fasnet" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.layers.fasnet"></span><dl class="class">
<dt id="espnet2.enh.layers.fasnet.BF_module">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.fasnet.</code><code class="sig-name descname">BF_module</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em>, <em class="sig-param">feature_dim</em>, <em class="sig-param">hidden_dim</em>, <em class="sig-param">output_dim</em>, <em class="sig-param">num_spk=2</em>, <em class="sig-param">layer=4</em>, <em class="sig-param">segment_size=100</em>, <em class="sig-param">bidirectional=True</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">fasnet_type='ifasnet'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#BF_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.BF_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.fasnet.BF_module.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">num_mic</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#BF_module.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.BF_module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.fasnet.FaSNet_TAC">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.fasnet.</code><code class="sig-name descname">FaSNet_TAC</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_TAC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_TAC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.layers.fasnet.FaSNet_base" title="espnet2.enh.layers.fasnet.FaSNet_base"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.layers.fasnet.FaSNet_base</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.layers.fasnet.FaSNet_TAC.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">num_mic</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_TAC.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_TAC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>abstract forward function</p>
<p>input: shape (batch, max_num_ch, T)
num_mic: shape (batch, ), the number of channels for each input.</p>
<blockquote>
<div><p>Zero for fixed geometry configuration.</p>
</div></blockquote>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.layers.fasnet.FaSNet_base">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.layers.fasnet.</code><code class="sig-name descname">FaSNet_base</code><span class="sig-paren">(</span><em class="sig-param">enc_dim</em>, <em class="sig-param">feature_dim</em>, <em class="sig-param">hidden_dim</em>, <em class="sig-param">layer</em>, <em class="sig-param">segment_size=24</em>, <em class="sig-param">nspk=2</em>, <em class="sig-param">win_len=16</em>, <em class="sig-param">context_len=16</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">sr=16000</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_base"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_base" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.layers.fasnet.FaSNet_base.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">num_mic</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_base.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_base.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>abstract forward function</p>
<p>input: shape (batch, max_num_ch, T)
num_mic: shape (batch, ), the number of channels for each input.</p>
<blockquote>
<div><p>Zero for fixed geometry configuration.</p>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.fasnet.FaSNet_base.pad_input">
<code class="sig-name descname">pad_input</code><span class="sig-paren">(</span><em class="sig-param">input</em>, <em class="sig-param">window</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_base.pad_input"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_base.pad_input" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero-padding input according to window/stride size.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.fasnet.FaSNet_base.seg_signal_context">
<code class="sig-name descname">seg_signal_context</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">window</em>, <em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_base.seg_signal_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_base.seg_signal_context" title="Permalink to this definition">¶</a></dt>
<dd><p>Segmenting the signal into chunks with specific context.</p>
<dl class="simple">
<dt>input:</dt><dd><p>x: size (B, ch, T)
window: int
context: int</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.fasnet.FaSNet_base.seq_cos_sim">
<code class="sig-name descname">seq_cos_sim</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">target</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_base.seq_cos_sim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_base.seq_cos_sim" title="Permalink to this definition">¶</a></dt>
<dd><p>Cosine similarity between some reference mics and some target mics</p>
<p>ref: shape (nmic1, L, seg1)
target: shape (nmic2, L, seg2)</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.layers.fasnet.FaSNet_base.signal_context">
<code class="sig-name descname">signal_context</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#FaSNet_base.signal_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.FaSNet_base.signal_context" title="Permalink to this definition">¶</a></dt>
<dd><p>signal context function</p>
<p>Segmenting the signal into chunks with specific context.
input:</p>
<blockquote>
<div><p>x: size (B, dim, nframe)
context: int</p>
</div></blockquote>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.enh.layers.fasnet.test_model">
<code class="sig-prename descclassname">espnet2.enh.layers.fasnet.</code><code class="sig-name descname">test_model</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/layers/fasnet.html#test_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.layers.fasnet.test_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="espnet2-enh-loss-init">
<span id="id21"></span><h2>espnet2.enh.loss.__init__<a class="headerlink" href="#espnet2-enh-loss-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.__init__"></span></section>
<section id="espnet2-enh-loss-wrappers-pit-solver">
<span id="id22"></span><h2>espnet2.enh.loss.wrappers.pit_solver<a class="headerlink" href="#espnet2-enh-loss-wrappers-pit-solver" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.wrappers.pit_solver"></span><dl class="class">
<dt id="espnet2.enh.loss.wrappers.pit_solver.PITSolver">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.wrappers.pit_solver.</code><code class="sig-name descname">PITSolver</code><span class="sig-paren">(</span><em class="sig-param">criterion: espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss</em>, <em class="sig-param">weight=1.0</em>, <em class="sig-param">independent_perm=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/pit_solver.html#PITSolver"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.pit_solver.PITSolver" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper" title="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper</span></code></a></p>
<p>Permutation Invariant Training Solver.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<a class="reference internal" href="#espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss" title="espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss"><em>AbsEnhLoss</em></a>) – an instance of AbsEnhLoss</p></li>
<li><p><strong>weight</strong> (<em>float</em>) – weight (between 0 and 1) of current loss
for multi-task learning.</p></li>
<li><p><strong>independent_perm</strong> (<em>bool</em>) – <p>If True, PIT will be performed in forward to find the best permutation;
If False, the permutation from the last LossWrapper output will be
inherited.
NOTE (wangyou): You should be careful about the ordering of loss</p>
<blockquote>
<div><p>wrappers defined in the yaml config, if this argument is False.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.loss.wrappers.pit_solver.PITSolver.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em>, <em class="sig-param">others={}</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/pit_solver.html#PITSolver.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.pit_solver.PITSolver.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>PITSolver forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – [(batch, …), …] x n_spk</p></li>
<li><p><strong>inf</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – [(batch, …), …]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(torch.Tensor): minimum loss with the best permutation
stats: dict, for collecting training status
others: dict, in this PIT solver, permutation order will be returned</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-wrappers-fixed-order">
<span id="id23"></span><h2>espnet2.enh.loss.wrappers.fixed_order<a class="headerlink" href="#espnet2-enh-loss-wrappers-fixed-order" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.wrappers.fixed_order"></span><dl class="class">
<dt id="espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.wrappers.fixed_order.</code><code class="sig-name descname">FixedOrderSolver</code><span class="sig-paren">(</span><em class="sig-param">criterion: espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss</em>, <em class="sig-param">weight=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/fixed_order.html#FixedOrderSolver"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper" title="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em>, <em class="sig-param">others={}</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/fixed_order.html#FixedOrderSolver.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.fixed_order.FixedOrderSolver.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>An naive fixed-order solver</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – [(batch, …), …] x n_spk</p></li>
<li><p><strong>inf</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – [(batch, …), …]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(torch.Tensor): minimum loss with the best permutation
stats: dict, for collecting training status
others: reserved</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-wrappers-multilayer-pit-solver">
<span id="id24"></span><h2>espnet2.enh.loss.wrappers.multilayer_pit_solver<a class="headerlink" href="#espnet2-enh-loss-wrappers-multilayer-pit-solver" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.wrappers.multilayer_pit_solver"></span><dl class="class">
<dt id="espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.wrappers.multilayer_pit_solver.</code><code class="sig-name descname">MultiLayerPITSolver</code><span class="sig-paren">(</span><em class="sig-param">criterion: espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss</em>, <em class="sig-param">weight=1.0</em>, <em class="sig-param">independent_perm=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/multilayer_pit_solver.html#MultiLayerPITSolver"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper" title="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper</span></code></a></p>
<p>Multi-Layer Permutation Invariant Training Solver.</p>
<p>Compute the PIT loss given inferences of multiple layers and a single reference.
It also support single inference and single reference in evaluation stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<a class="reference internal" href="#espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss" title="espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss"><em>AbsEnhLoss</em></a>) – an instance of AbsEnhLoss</p></li>
<li><p><strong>weight</strong> (<em>float</em>) – weight (between 0 and 1) of current loss
for multi-task learning.</p></li>
<li><p><strong>independent_perm</strong> (<em>bool</em>) – If True, PIT will be performed in forward to find the best permutation;
If False, the permutation from the last LossWrapper output will be
inherited.
Note: You should be careful about the ordering of loss
wrappers defined in the yaml config, if this argument is False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">infs</em>, <em class="sig-param">others={}</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/multilayer_pit_solver.html#MultiLayerPITSolver.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.multilayer_pit_solver.MultiLayerPITSolver.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Permutation invariant training solver.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – [(batch, …), …] x n_spk</p></li>
<li><p><strong>infs</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>List</em><em>[</em><em>List</em><em>[</em><em>torch.Tensor</em><em>]</em><em>]</em><em>]</em>) – [(batch, …), …]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(torch.Tensor): minimum loss with the best permutation
stats: dict, for collecting training status
others: dict, in this PIT solver, permutation order will be returned</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-wrappers-abs-wrapper">
<span id="id25"></span><h2>espnet2.enh.loss.wrappers.abs_wrapper<a class="headerlink" href="#espnet2-enh-loss-wrappers-abs-wrapper" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.wrappers.abs_wrapper"></span><dl class="class">
<dt id="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.wrappers.abs_wrapper.</code><code class="sig-name descname">AbsLossWrapper</code><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/abs_wrapper.html#AbsLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base class for all Enhancement loss wrapper modules.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref: List</em>, <em class="sig-param">inf: List</em>, <em class="sig-param">others: Dict</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict, Dict]<a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/abs_wrapper.html#AbsLossWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper.weight">
<code class="sig-name descname">weight</code><em class="property"> = 1.0</em><a class="headerlink" href="#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper.weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-wrappers-dpcl-solver">
<span id="id26"></span><h2>espnet2.enh.loss.wrappers.dpcl_solver<a class="headerlink" href="#espnet2-enh-loss-wrappers-dpcl-solver" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.wrappers.dpcl_solver"></span><dl class="class">
<dt id="espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.wrappers.dpcl_solver.</code><code class="sig-name descname">DPCLSolver</code><span class="sig-paren">(</span><em class="sig-param">criterion: espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss</em>, <em class="sig-param">weight=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/dpcl_solver.html#DPCLSolver"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper" title="espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.wrappers.abs_wrapper.AbsLossWrapper</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em>, <em class="sig-param">others={}</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/wrappers/dpcl_solver.html#DPCLSolver.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.wrappers.dpcl_solver.DPCLSolver.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>A naive DPCL solver</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – [(batch, …), …] x n_spk</p></li>
<li><p><strong>inf</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – [(batch, …), …]</p></li>
<li><p><strong>others</strong> (<em>List</em>) – other data included in this solver
e.g. “tf_embedding” learned embedding of all T-F bins (B, T * F, D)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(torch.Tensor): minimum loss with the best permutation
stats: (dict), for collecting training status
others: reserved</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-wrappers-init">
<span id="id27"></span><h2>espnet2.enh.loss.wrappers.__init__<a class="headerlink" href="#espnet2-enh-loss-wrappers-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.wrappers.__init__"></span></section>
<section id="espnet2-enh-loss-criterions-tf-domain">
<span id="id28"></span><h2>espnet2.enh.loss.criterions.tf_domain<a class="headerlink" href="#espnet2-enh-loss-criterions-tf-domain" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.criterions.tf_domain"></span><dl class="class">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.tf_domain.</code><code class="sig-name descname">FrequencyDomainAbsCoherence</code><span class="sig-paren">(</span><em class="sig-param">compute_on_mask=False</em>, <em class="sig-param">mask_type=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainAbsCoherence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss" title="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.compute_on_mask">
<em class="property">property </em><code class="sig-name descname">compute_on_mask</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.compute_on_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainAbsCoherence.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>time-frequency absolute coherence loss.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Independent Vector Analysis with Deep Neural Network Source Priors;
Li et al 2020; <a class="reference external" href="https://arxiv.org/abs/2008.11273">https://arxiv.org/abs/2008.11273</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – (Batch, T, F) or (Batch, T, C, F)</p></li>
<li><p><strong>inf</strong> – (Batch, T, F) or (Batch, T, C, F)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.mask_type">
<em class="property">property </em><code class="sig-name descname">mask_type</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainAbsCoherence.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.tf_domain.</code><code class="sig-name descname">FrequencyDomainCrossEntropy</code><span class="sig-paren">(</span><em class="sig-param">compute_on_mask=False</em>, <em class="sig-param">mask_type=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainCrossEntropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss" title="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.compute_on_mask">
<em class="property">property </em><code class="sig-name descname">compute_on_mask</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.compute_on_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainCrossEntropy.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>time-frequency cross-entropy loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – (Batch, T) or (Batch, T, C)</p></li>
<li><p><strong>inf</strong> – (Batch, T, nclass) or (Batch, T, C, nclass)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.mask_type">
<em class="property">property </em><code class="sig-name descname">mask_type</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainCrossEntropy.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.tf_domain.</code><code class="sig-name descname">FrequencyDomainDPCL</code><span class="sig-paren">(</span><em class="sig-param">compute_on_mask=False</em>, <em class="sig-param">mask_type='IBM'</em>, <em class="sig-param">loss_type='dpcl'</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainDPCL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss" title="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.compute_on_mask">
<em class="property">property </em><code class="sig-name descname">compute_on_mask</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.compute_on_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainDPCL.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>time-frequency Deep Clustering loss.</p>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Deep clustering: Discriminative embeddings for segmentation and</dt><dd><p>separation; John R. Hershey. et al., 2016;
<a class="reference external" href="https://ieeexplore.ieee.org/document/7471631">https://ieeexplore.ieee.org/document/7471631</a></p>
</dd>
<dt>[2] Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding</dt><dd><p>Vectors Based on Regular Simplex; Tanaka, K. et al., 2021;
<a class="reference external" href="https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html">https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – List[(Batch, T, F) * spks]</p></li>
<li><p><strong>inf</strong> – (Batch, T*F, D)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.mask_type">
<em class="property">property </em><code class="sig-name descname">mask_type</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainDPCL.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.tf_domain.</code><code class="sig-name descname">FrequencyDomainL1</code><span class="sig-paren">(</span><em class="sig-param">compute_on_mask=False</em>, <em class="sig-param">mask_type='IBM'</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainL1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss" title="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.compute_on_mask">
<em class="property">property </em><code class="sig-name descname">compute_on_mask</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.compute_on_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainL1.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>time-frequency L1 loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – (Batch, T, F) or (Batch, T, C, F)</p></li>
<li><p><strong>inf</strong> – (Batch, T, F) or (Batch, T, C, F)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.mask_type">
<em class="property">property </em><code class="sig-name descname">mask_type</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainL1.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.tf_domain.</code><code class="sig-name descname">FrequencyDomainLoss</code><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss" title="espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base class for all frequence-domain Enhancement loss modules.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss.compute_on_mask">
<em class="property">abstract property </em><code class="sig-name descname">compute_on_mask</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss.compute_on_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss.create_mask_label">
<code class="sig-name descname">create_mask_label</code><span class="sig-paren">(</span><em class="sig-param">mix_spec</em>, <em class="sig-param">ref_spec</em>, <em class="sig-param">noise_spec=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainLoss.create_mask_label"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss.create_mask_label" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss.mask_type">
<em class="property">abstract property </em><code class="sig-name descname">mask_type</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.tf_domain.</code><code class="sig-name descname">FrequencyDomainMSE</code><span class="sig-paren">(</span><em class="sig-param">compute_on_mask=False</em>, <em class="sig-param">mask_type='IBM'</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainMSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss" title="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.tf_domain.FrequencyDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.compute_on_mask">
<em class="property">property </em><code class="sig-name descname">compute_on_mask</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.compute_on_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/tf_domain.html#FrequencyDomainMSE.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>time-frequency MSE loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – (Batch, T, F) or (Batch, T, C, F)</p></li>
<li><p><strong>inf</strong> – (Batch, T, F) or (Batch, T, C, F)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.mask_type">
<em class="property">property </em><code class="sig-name descname">mask_type</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.mask_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.tf_domain.FrequencyDomainMSE.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-criterions-time-domain">
<span id="id29"></span><h2>espnet2.enh.loss.criterions.time_domain<a class="headerlink" href="#espnet2-enh-loss-criterions-time-domain" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.criterions.time_domain"></span><dl class="class">
<dt id="espnet2.enh.loss.criterions.time_domain.CISDRLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.time_domain.</code><code class="sig-name descname">CISDRLoss</code><span class="sig-paren">(</span><em class="sig-param">filter_length=512</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#CISDRLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.CISDRLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainLoss" title="espnet2.enh.loss.criterions.time_domain.TimeDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.time_domain.TimeDomainLoss</span></code></a></p>
<p>CI-SDR loss</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Convolutive Transfer Function Invariant SDR Training
Criteria for Multi-Channel Reverberant Speech Separation;
C. Boeddeker et al., 2021;
<a class="reference external" href="https://arxiv.org/abs/2011.15003">https://arxiv.org/abs/2011.15003</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – (Batch, samples)</p></li>
<li><p><strong>inf</strong> – (Batch, samples)</p></li>
<li><p><strong>filter_length</strong> (<em>int</em>) – a time-invariant filter that allows
slight distortion via filtering</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.CISDRLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref: torch.Tensor</em>, <em class="sig-param">inf: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#CISDRLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.CISDRLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.CISDRLoss.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.CISDRLoss.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.time_domain.SDRLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.time_domain.</code><code class="sig-name descname">SDRLoss</code><span class="sig-paren">(</span><em class="sig-param">filter_length=512</em>, <em class="sig-param">use_cg_iter=None</em>, <em class="sig-param">clamp_db=None</em>, <em class="sig-param">zero_mean=True</em>, <em class="sig-param">load_diag=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#SDRLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SDRLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainLoss" title="espnet2.enh.loss.criterions.time_domain.TimeDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.time_domain.TimeDomainLoss</span></code></a></p>
<p>SDR loss.</p>
<dl class="simple">
<dt>filter_length: int</dt><dd><p>The length of the distortion filter allowed (default: <code class="docutils literal notranslate"><span class="pre">512</span></code>)</p>
</dd>
<dt>use_cg_iter:</dt><dd><p>If provided, an iterative method is used to solve for the distortion
filter coefficients instead of direct Gaussian elimination.
This can speed up the computation of the metrics in case the filters
are long. Using a value of 10 here has been shown to provide
good accuracy in most cases and is sufficient when using this
loss to train neural separation networks.</p>
</dd>
<dt>clamp_db: float</dt><dd><p>clamp the output value in  [-clamp_db, clamp_db]</p>
</dd>
<dt>zero_mean: bool</dt><dd><p>When set to True, the mean of all signals is subtracted prior.</p>
</dd>
<dt>load_diag:</dt><dd><p>If provided, this small value is added to the diagonal coefficients of
the system metrics when solving for the filter coefficients.
This can help stabilize the metric in the case where some of the reference
signals may sometimes be zero</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.SDRLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref: torch.Tensor</em>, <em class="sig-param">est: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#SDRLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SDRLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>SDR forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – Tensor, (…, n_samples)
reference signal</p></li>
<li><p><strong>est</strong> – Tensor (…, n_samples)
estimated signal</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>(…,)</dt><dd><p>the SDR loss (negative sdr)</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.SDRLoss.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SDRLoss.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.time_domain.SISNRLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.time_domain.</code><code class="sig-name descname">SISNRLoss</code><span class="sig-paren">(</span><em class="sig-param">clamp_db=None</em>, <em class="sig-param">zero_mean=True</em>, <em class="sig-param">eps=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#SISNRLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SISNRLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainLoss" title="espnet2.enh.loss.criterions.time_domain.TimeDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.time_domain.TimeDomainLoss</span></code></a></p>
<p>SI-SNR (or named SI-SDR) loss</p>
<p>A more stable SI-SNR loss with clamp from <cite>fast_bss_eval</cite>.</p>
<dl class="attribute">
<dt id="espnet2.enh.loss.criterions.time_domain.SISNRLoss.clamp_db">
<code class="sig-name descname">clamp_db</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SISNRLoss.clamp_db" title="Permalink to this definition">¶</a></dt>
<dd><p>float
clamp the output value in  [-clamp_db, clamp_db]</p>
</dd></dl>

<dl class="attribute">
<dt id="espnet2.enh.loss.criterions.time_domain.SISNRLoss.zero_mean">
<code class="sig-name descname">zero_mean</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SISNRLoss.zero_mean" title="Permalink to this definition">¶</a></dt>
<dd><p>bool
When set to True, the mean of all signals is subtracted prior.</p>
</dd></dl>

<dl class="attribute">
<dt id="espnet2.enh.loss.criterions.time_domain.SISNRLoss.eps">
<code class="sig-name descname">eps</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SISNRLoss.eps" title="Permalink to this definition">¶</a></dt>
<dd><p>float
Deprecated. Keeped for compatibility.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.SISNRLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref: torch.Tensor</em>, <em class="sig-param">est: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#SISNRLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SISNRLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>SI-SNR forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – Tensor, (…, n_samples)
reference signal</p></li>
<li><p><strong>est</strong> – Tensor (…, n_samples)
estimated signal</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>(…,)</dt><dd><p>the SI-SDR loss (negative si-sdr)</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.SISNRLoss.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SISNRLoss.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.time_domain.SNRLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.time_domain.</code><code class="sig-name descname">SNRLoss</code><span class="sig-paren">(</span><em class="sig-param">eps=1.1920928955078125e-07</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#SNRLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SNRLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainLoss" title="espnet2.enh.loss.criterions.time_domain.TimeDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.time_domain.TimeDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.SNRLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref: torch.Tensor</em>, <em class="sig-param">inf: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#SNRLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SNRLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.SNRLoss.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.SNRLoss.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.time_domain.TimeDomainL1">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.time_domain.</code><code class="sig-name descname">TimeDomainL1</code><span class="sig-paren">(</span><em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#TimeDomainL1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainL1" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainLoss" title="espnet2.enh.loss.criterions.time_domain.TimeDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.time_domain.TimeDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.TimeDomainL1.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#TimeDomainL1.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainL1.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Time-domain L1 loss forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – (Batch, T) or (Batch, T, C)</p></li>
<li><p><strong>inf</strong> – (Batch, T) or (Batch, T, C)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.TimeDomainL1.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainL1.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.time_domain.TimeDomainLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.time_domain.</code><code class="sig-name descname">TimeDomainLoss</code><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#TimeDomainLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss" title="espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base class for all time-domain Enhancement loss modules.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="class">
<dt id="espnet2.enh.loss.criterions.time_domain.TimeDomainMSE">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.time_domain.</code><code class="sig-name descname">TimeDomainMSE</code><span class="sig-paren">(</span><em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#TimeDomainMSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainMSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainLoss" title="espnet2.enh.loss.criterions.time_domain.TimeDomainLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.loss.criterions.time_domain.TimeDomainLoss</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.TimeDomainMSE.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/time_domain.html#TimeDomainMSE.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainMSE.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Time-domain MSE loss forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref</strong> – (Batch, T) or (Batch, T, C)</p></li>
<li><p><strong>inf</strong> – (Batch, T) or (Batch, T, C)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(Batch,)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.time_domain.TimeDomainMSE.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.time_domain.TimeDomainMSE.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-criterions-abs-loss">
<span id="id30"></span><h2>espnet2.enh.loss.criterions.abs_loss<a class="headerlink" href="#espnet2-enh-loss-criterions-abs-loss" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.criterions.abs_loss"></span><dl class="class">
<dt id="espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.loss.criterions.abs_loss.</code><code class="sig-name descname">AbsEnhLoss</code><a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/abs_loss.html#AbsEnhLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base class for all Enhancement loss modules.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">ref</em>, <em class="sig-param">inf</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/espnet2/enh/loss/criterions/abs_loss.html#AbsEnhLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#espnet2.enh.loss.criterions.abs_loss.AbsEnhLoss.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-loss-criterions-init">
<span id="id31"></span><h2>espnet2.enh.loss.criterions.__init__<a class="headerlink" href="#espnet2-enh-loss-criterions-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.loss.criterions.__init__"></span></section>
<section id="espnet2-enh-encoder-stft-encoder">
<span id="id32"></span><h2>espnet2.enh.encoder.stft_encoder<a class="headerlink" href="#espnet2-enh-encoder-stft-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.encoder.stft_encoder"></span><dl class="class">
<dt id="espnet2.enh.encoder.stft_encoder.STFTEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.encoder.stft_encoder.</code><code class="sig-name descname">STFTEncoder</code><span class="sig-paren">(</span><em class="sig-param">n_fft: int = 512</em>, <em class="sig-param">win_length: int = None</em>, <em class="sig-param">hop_length: int = 128</em>, <em class="sig-param">window='hann'</em>, <em class="sig-param">center: bool = True</em>, <em class="sig-param">normalized: bool = False</em>, <em class="sig-param">onesided: bool = True</em>, <em class="sig-param">use_builtin_complex: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/encoder/stft_encoder.html#STFTEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.stft_encoder.STFTEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.encoder.abs_encoder.AbsEncoder" title="espnet2.enh.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>STFT encoder for speech enhancement and separation</p>
<dl class="method">
<dt id="espnet2.enh.encoder.stft_encoder.STFTEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/encoder/stft_encoder.html#STFTEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.stft_encoder.STFTEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – mixed speech [Batch, sample]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.encoder.stft_encoder.STFTEncoder.output_dim">
<em class="property">property </em><code class="sig-name descname">output_dim</code><a class="headerlink" href="#espnet2.enh.encoder.stft_encoder.STFTEncoder.output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-encoder-conv-encoder">
<span id="id33"></span><h2>espnet2.enh.encoder.conv_encoder<a class="headerlink" href="#espnet2-enh-encoder-conv-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.encoder.conv_encoder"></span><dl class="class">
<dt id="espnet2.enh.encoder.conv_encoder.ConvEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.encoder.conv_encoder.</code><code class="sig-name descname">ConvEncoder</code><span class="sig-paren">(</span><em class="sig-param">channel: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">stride: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/encoder/conv_encoder.html#ConvEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.conv_encoder.ConvEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.encoder.abs_encoder.AbsEncoder" title="espnet2.enh.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Convolutional encoder for speech enhancement and separation</p>
<dl class="method">
<dt id="espnet2.enh.encoder.conv_encoder.ConvEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/encoder/conv_encoder.html#ConvEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.conv_encoder.ConvEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – mixed speech [Batch, sample]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>mixed feature after encoder [Batch, flens, channel]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>feature (torch.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.encoder.conv_encoder.ConvEncoder.output_dim">
<em class="property">property </em><code class="sig-name descname">output_dim</code><a class="headerlink" href="#espnet2.enh.encoder.conv_encoder.ConvEncoder.output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-encoder-init">
<span id="id34"></span><h2>espnet2.enh.encoder.__init__<a class="headerlink" href="#espnet2-enh-encoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.encoder.__init__"></span></section>
<section id="espnet2-enh-encoder-abs-encoder">
<span id="id35"></span><h2>espnet2.enh.encoder.abs_encoder<a class="headerlink" href="#espnet2-enh-encoder-abs-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.encoder.abs_encoder"></span><dl class="class">
<dt id="espnet2.enh.encoder.abs_encoder.AbsEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.encoder.abs_encoder.</code><code class="sig-name descname">AbsEncoder</code><a class="reference internal" href="../_modules/espnet2/enh/encoder/abs_encoder.html#AbsEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.abs_encoder.AbsEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.enh.encoder.abs_encoder.AbsEncoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/encoder/abs_encoder.html#AbsEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.abs_encoder.AbsEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.encoder.abs_encoder.AbsEncoder.output_dim">
<em class="property">abstract property </em><code class="sig-name descname">output_dim</code><a class="headerlink" href="#espnet2.enh.encoder.abs_encoder.AbsEncoder.output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-encoder-null-encoder">
<span id="id36"></span><h2>espnet2.enh.encoder.null_encoder<a class="headerlink" href="#espnet2-enh-encoder-null-encoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.encoder.null_encoder"></span><dl class="class">
<dt id="espnet2.enh.encoder.null_encoder.NullEncoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.encoder.null_encoder.</code><code class="sig-name descname">NullEncoder</code><a class="reference internal" href="../_modules/espnet2/enh/encoder/null_encoder.html#NullEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.null_encoder.NullEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.encoder.abs_encoder.AbsEncoder" title="espnet2.enh.encoder.abs_encoder.AbsEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.encoder.abs_encoder.AbsEncoder</span></code></a></p>
<p>Null encoder.</p>
<dl class="method">
<dt id="espnet2.enh.encoder.null_encoder.NullEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/encoder/null_encoder.html#NullEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.encoder.null_encoder.NullEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – mixed speech [Batch, sample]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.encoder.null_encoder.NullEncoder.output_dim">
<em class="property">property </em><code class="sig-name descname">output_dim</code><a class="headerlink" href="#espnet2.enh.encoder.null_encoder.NullEncoder.output_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-rnn-separator">
<span id="id37"></span><h2>espnet2.enh.separator.rnn_separator<a class="headerlink" href="#espnet2-enh-separator-rnn-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.rnn_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.rnn_separator.RNNSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.rnn_separator.</code><code class="sig-name descname">RNNSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">rnn_type: str = 'blstm'</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">nonlinear: str = 'sigmoid'</em>, <em class="sig-param">layer: int = 3</em>, <em class="sig-param">unit: int = 512</em>, <em class="sig-param">dropout: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/rnn_separator.html#RNNSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.rnn_separator.RNNSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>RNN Separator</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>rnn_type</strong> – string, select from ‘blstm’, ‘lstm’ etc.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the inter-chunk RNN layers are bidirectional.</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
<li><p><strong>layer</strong> – int, number of stacked RNN layers. Default is 3.</p></li>
<li><p><strong>unit</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.rnn_separator.RNNSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/rnn_separator.html#RNNSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.rnn_separator.RNNSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, N]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.rnn_separator.RNNSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.rnn_separator.RNNSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-conformer-separator">
<span id="id38"></span><h2>espnet2.enh.separator.conformer_separator<a class="headerlink" href="#espnet2-enh-separator-conformer-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.conformer_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.conformer_separator.ConformerSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.conformer_separator.</code><code class="sig-name descname">ConformerSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">adim: int = 384</em>, <em class="sig-param">aheads: int = 4</em>, <em class="sig-param">layers: int = 6</em>, <em class="sig-param">linear_units: int = 1536</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">normalize_before: bool = False</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">input_layer: str = 'linear'</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.1</em>, <em class="sig-param">nonlinear: str = 'relu'</em>, <em class="sig-param">conformer_pos_enc_layer_type: str = 'rel_pos'</em>, <em class="sig-param">conformer_self_attn_layer_type: str = 'rel_selfattn'</em>, <em class="sig-param">conformer_activation_type: str = 'swish'</em>, <em class="sig-param">use_macaron_style_in_conformer: bool = True</em>, <em class="sig-param">use_cnn_in_conformer: bool = True</em>, <em class="sig-param">conformer_enc_kernel_size: int = 7</em>, <em class="sig-param">padding_idx: int = -1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/conformer_separator.html#ConformerSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.conformer_separator.ConformerSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Conformer separator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>adim</strong> (<em>int</em>) – Dimension of attention.</p></li>
<li><p><strong>aheads</strong> (<em>int</em>) – The number of heads of multi head attention.</p></li>
<li><p><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</p></li>
<li><p><strong>layers</strong> (<em>int</em>) – The number of transformer blocks.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
<li><p><strong>input_layer</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.nn.Module</em><em>]</em>) – Input layer type.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</p></li>
<li><p><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding
positional encoding.</p></li>
<li><p><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</p></li>
<li><p><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output.
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied. i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>conformer_pos_enc_layer_type</strong> (<em>str</em>) – Encoder positional encoding layer type.</p></li>
<li><p><strong>conformer_self_attn_layer_type</strong> (<em>str</em>) – Encoder attention layer type.</p></li>
<li><p><strong>conformer_activation_type</strong> (<em>str</em>) – Encoder activation function type.</p></li>
<li><p><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of
positionwise conv1d layer.</p></li>
<li><p><strong>use_macaron_style_in_conformer</strong> (<em>bool</em>) – Whether to use macaron style for
positionwise layer.</p></li>
<li><p><strong>use_cnn_in_conformer</strong> (<em>bool</em>) – Whether to use convolution module.</p></li>
<li><p><strong>conformer_enc_kernel_size</strong> (<em>int</em>) – Kernerl size of convolution module.</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em>) – Padding idx for input_layer=embed.</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.conformer_separator.ConformerSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/conformer_separator.html#ConformerSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.conformer_separator.ConformerSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, N]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.conformer_separator.ConformerSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.conformer_separator.ConformerSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-skim-separator">
<span id="id39"></span><h2>espnet2.enh.separator.skim_separator<a class="headerlink" href="#espnet2-enh-separator-skim-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.skim_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.skim_separator.SkiMSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.skim_separator.</code><code class="sig-name descname">SkiMSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">causal: bool = True</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">nonlinear: str = 'relu'</em>, <em class="sig-param">layer: int = 3</em>, <em class="sig-param">unit: int = 512</em>, <em class="sig-param">segment_size: int = 20</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">mem_type: str = 'hc'</em>, <em class="sig-param">seg_overlap: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/skim_separator.html#SkiMSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.skim_separator.SkiMSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Skipping Memory (SkiM) Separator</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>causal</strong> – bool, whether the system is causal.</p></li>
<li><p><strong>num_spk</strong> – number of target speakers.</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
<li><p><strong>layer</strong> – int, number of SkiM blocks. Default is 3.</p></li>
<li><p><strong>unit</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>segment_size</strong> – segmentation size for splitting long features</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>mem_type</strong> – ‘hc’, ‘h’, ‘c’, ‘id’ or None.
It controls whether the hidden (or cell) state of
SegLSTM will be processed by MemLSTM.
In ‘id’ mode, both the hidden and cell states
will be identically returned.
When mem_type is None, the MemLSTM will be removed.</p></li>
<li><p><strong>seg_overlap</strong> – Bool, whether the segmentation will reserve 50%
overlap for adjacent segments. Default is False.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.skim_separator.SkiMSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/skim_separator.html#SkiMSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.skim_separator.SkiMSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, N]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.skim_separator.SkiMSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.skim_separator.SkiMSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-asteroid-models">
<span id="id40"></span><h2>espnet2.enh.separator.asteroid_models<a class="headerlink" href="#espnet2-enh-separator-asteroid-models" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.asteroid_models"></span><dl class="class">
<dt id="espnet2.enh.separator.asteroid_models.AsteroidModel_Converter">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.asteroid_models.</code><code class="sig-name descname">AsteroidModel_Converter</code><span class="sig-paren">(</span><em class="sig-param">encoder_output_dim: int</em>, <em class="sig-param">model_name: str</em>, <em class="sig-param">num_spk: int</em>, <em class="sig-param">pretrained_path: str = ''</em>, <em class="sig-param">loss_type: str = 'si_snr'</em>, <em class="sig-param">**model_related_kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/asteroid_models.html#AsteroidModel_Converter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.asteroid_models.AsteroidModel_Converter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>The class to convert the models from asteroid to AbsSeprator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder_output_dim</strong> – input feature dimension, default=1 after the NullEncoder</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>loss_type</strong> – loss type of enhancement</p></li>
<li><p><strong>model_name</strong> – Asteroid model names, e.g. ConvTasNet, DPTNet. Refers to
<a class="reference external" href="https://github.com/asteroid-team/asteroid/">https://github.com/asteroid-team/asteroid/</a>
blob/master/asteroid/models/__init__.py</p></li>
<li><p><strong>pretrained_path</strong> – the name of pretrained model from Asteroid in HF hub.
Refers to: <a class="reference external" href="https://github.com/asteroid-team/asteroid/">https://github.com/asteroid-team/asteroid/</a>
blob/master/docs/source/readmes/pretrained_models.md and
<a class="reference external" href="https://huggingface.co/models?filter=asteroid">https://huggingface.co/models?filter=asteroid</a></p></li>
<li><p><strong>model_related_kwargs</strong> – more args towards each specific asteroid model.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.asteroid_models.AsteroidModel_Converter.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor = None</em>, <em class="sig-param">additional: Optional[Dict] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/asteroid_models.html#AsteroidModel_Converter.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.asteroid_models.AsteroidModel_Converter.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Whole forward of asteroid models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – Raw Waveforms [B, T]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [B]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, T),
‘mask_spk2’: torch.Tensor(Batch, T),
…
‘mask_spkn’: torch.Tensor(Batch, T),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>estimated Waveforms(List[Union(torch.Tensor])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.asteroid_models.AsteroidModel_Converter.forward_rawwav">
<code class="sig-name descname">forward_rawwav</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/separator/asteroid_models.html#AsteroidModel_Converter.forward_rawwav"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.asteroid_models.AsteroidModel_Converter.forward_rawwav" title="Permalink to this definition">¶</a></dt>
<dd><p>Output with waveforms.</p>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.asteroid_models.AsteroidModel_Converter.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.asteroid_models.AsteroidModel_Converter.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-dc-crn-separator">
<span id="id41"></span><h2>espnet2.enh.separator.dc_crn_separator<a class="headerlink" href="#espnet2-enh-separator-dc-crn-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.dc_crn_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.dc_crn_separator.</code><code class="sig-name descname">DC_CRNSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int, num_spk: int = 2, input_channels: List = [2, 16, 32, 64, 128, 256], enc_hid_channels: int = 8, enc_kernel_size: Tuple = (1, 3), enc_padding: Tuple = (0, 1), enc_last_kernel_size: Tuple = (1, 4), enc_last_stride: Tuple = (1, 2), enc_last_padding: Tuple = (0, 1), enc_layers: int = 5, skip_last_kernel_size: Tuple = (1, 3), skip_last_stride: Tuple = (1, 1), skip_last_padding: Tuple = (0, 1), glstm_groups: int = 2, glstm_layers: int = 2, glstm_bidirectional: bool = False, glstm_rearrange: bool = False, mode: str = 'masking', ref_channel: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dc_crn_separator.html#DC_CRNSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Densely-Connected Convolutional Recurrent Network (DC-CRN) Separator</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Deep Learning Based Real-Time Speech Enhancement for Dual-Microphone
Mobile Phones; Tan et al., 2020
<a class="reference external" href="https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf">https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>input_channels</strong> (<em>list</em>) – number of input channels for the stacked
DenselyConnectedBlock layers
Its length should be (<cite>number of DenselyConnectedBlock layers</cite>).</p></li>
<li><p><strong>enc_hid_channels</strong> (<em>int</em>) – common number of intermediate channels for all
DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_kernel_size</strong> (<em>tuple</em>) – common kernel size for all DenselyConnectedBlock
of the encoder</p></li>
<li><p><strong>enc_padding</strong> (<em>tuple</em>) – common padding for all DenselyConnectedBlock
of the encoder</p></li>
<li><p><strong>enc_last_kernel_size</strong> (<em>tuple</em>) – common kernel size for the last Conv layer
in all DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_last_stride</strong> (<em>tuple</em>) – common stride for the last Conv layer in all
DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_last_padding</strong> (<em>tuple</em>) – common padding for the last Conv layer in all
DenselyConnectedBlock of the encoder</p></li>
<li><p><strong>enc_layers</strong> (<em>int</em>) – common total number of Conv layers for all
DenselyConnectedBlock layers of the encoder</p></li>
<li><p><strong>skip_last_kernel_size</strong> (<em>tuple</em>) – common kernel size for the last Conv layer
in all DenselyConnectedBlock of the skip pathways</p></li>
<li><p><strong>skip_last_stride</strong> (<em>tuple</em>) – common stride for the last Conv layer in all
DenselyConnectedBlock of the skip pathways</p></li>
<li><p><strong>skip_last_padding</strong> (<em>tuple</em>) – common padding for the last Conv layer in all
DenselyConnectedBlock of the skip pathways</p></li>
<li><p><strong>glstm_groups</strong> (<em>int</em>) – number of groups in each Grouped LSTM layer</p></li>
<li><p><strong>glstm_layers</strong> (<em>int</em>) – number of Grouped LSTM layers</p></li>
<li><p><strong>glstm_bidirectional</strong> (<em>bool</em>) – whether to use BLSTM or unidirectional LSTM
in Grouped LSTM layers</p></li>
<li><p><strong>glstm_rearrange</strong> (<em>bool</em>) – whether to apply the rearrange operation after each
grouped LSTM layer</p></li>
<li><p><strong>output_channels</strong> (<em>int</em>) – number of output channels (even number)</p></li>
<li><p><strong>mode</strong> (<em>str</em>) – one of (“mapping”, “masking”)
“mapping”: complex spectral mapping
“masking”: complex masking</p></li>
<li><p><strong>ref_channel</strong> (<em>int</em>) – index of the reference microphone</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/dc_crn_separator.html#DC_CRNSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>DC-CRN Separator Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [Batch, T, F]
or [Batch, T, C, F]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch,]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(Batch, T, F), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.dc_crn_separator.DC_CRNSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-dccrn-separator">
<span id="id42"></span><h2>espnet2.enh.separator.dccrn_separator<a class="headerlink" href="#espnet2-enh-separator-dccrn-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.dccrn_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.dccrn_separator.DCCRNSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.dccrn_separator.</code><code class="sig-name descname">DCCRNSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int, num_spk: int = 1, rnn_layer: int = 2, rnn_units: int = 256, masking_mode: str = 'E', use_clstm: bool = True, bidirectional: bool = False, use_cbn: bool = False, kernel_size: int = 5, kernel_num: List[int] = [32, 64, 128, 256, 256, 256], use_builtin_complex: bool = True, use_noise_mask: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dccrn_separator.html#DCCRNSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dccrn_separator.DCCRNSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>DCCRN separator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – input dimension。</p></li>
<li><p><strong>num_spk</strong> (<em>int</em><em>, </em><em>optional</em>) – number of speakers. Defaults to 1.</p></li>
<li><p><strong>rnn_layer</strong> (<em>int</em><em>, </em><em>optional</em>) – number of lstm layers in the crn. Defaults to 2.</p></li>
<li><p><strong>rnn_units</strong> (<em>int</em><em>, </em><em>optional</em>) – rnn units. Defaults to 128.</p></li>
<li><p><strong>masking_mode</strong> (<em>str</em><em>, </em><em>optional</em>) – usage of the estimated mask. Defaults to “E”.</p></li>
<li><p><strong>use_clstm</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether use complex LSTM. Defaults to False.</p></li>
<li><p><strong>bidirectional</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether use BLSTM. Defaults to False.</p></li>
<li><p><strong>use_cbn</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether use complex BN. Defaults to False.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em>, </em><em>optional</em>) – convolution kernel size. Defaults to 5.</p></li>
<li><p><strong>kernel_num</strong> (<em>list</em><em>, </em><em>optional</em>) – output dimension of each layer of the encoder.</p></li>
<li><p><strong>use_builtin_complex</strong> (<em>bool</em><em>, </em><em>optional</em>) – torch.complex if True,
else ComplexTensor.</p></li>
<li><p><strong>use_noise_mask</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to estimate the mask of noise.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.dccrn_separator.DCCRNSeparator.apply_masks">
<code class="sig-name descname">apply_masks</code><span class="sig-paren">(</span><em class="sig-param">masks: List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], real: torch.Tensor, imag: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dccrn_separator.html#DCCRNSeparator.apply_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dccrn_separator.DCCRNSeparator.apply_masks" title="Permalink to this definition">¶</a></dt>
<dd><p>apply masks</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>masks</strong> – est_masks, [(B, T, F), …]</p></li>
<li><p><strong>real</strong> (<em>torch.Tensor</em>) – real part of the noisy spectrum, (B, F, T)</p></li>
<li><p><strong>imag</strong> (<em>torch.Tensor</em>) – imag part of the noisy spectrum, (B, F, T)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[(B, T, F), …]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dccrn_separator.DCCRNSeparator.create_masks">
<code class="sig-name descname">create_masks</code><span class="sig-paren">(</span><em class="sig-param">mask_tensor: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dccrn_separator.html#DCCRNSeparator.create_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dccrn_separator.DCCRNSeparator.create_masks" title="Permalink to this definition">¶</a></dt>
<dd><p>create estimated mask for each speaker</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mask_tensor</strong> (<em>torch.Tensor</em>) – output of decoder, shape(B, 2*num_spk, F-1, T)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dccrn_separator.DCCRNSeparator.flatten_parameters">
<code class="sig-name descname">flatten_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dccrn_separator.html#DCCRNSeparator.flatten_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dccrn_separator.DCCRNSeparator.flatten_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dccrn_separator.DCCRNSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/dccrn_separator.html#DCCRNSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dccrn_separator.DCCRNSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, F]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, F), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dccrn_separator.DCCRNSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.dccrn_separator.DCCRNSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-neural-beamformer">
<span id="id43"></span><h2>espnet2.enh.separator.neural_beamformer<a class="headerlink" href="#espnet2-enh-separator-neural-beamformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.neural_beamformer"></span><dl class="class">
<dt id="espnet2.enh.separator.neural_beamformer.NeuralBeamformer">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.neural_beamformer.</code><code class="sig-name descname">NeuralBeamformer</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">num_spk: int = 1</em>, <em class="sig-param">loss_type: str = 'mask_mse'</em>, <em class="sig-param">use_wpe: bool = False</em>, <em class="sig-param">wnet_type: str = 'blstmp'</em>, <em class="sig-param">wlayers: int = 3</em>, <em class="sig-param">wunits: int = 300</em>, <em class="sig-param">wprojs: int = 320</em>, <em class="sig-param">wdropout_rate: float = 0.0</em>, <em class="sig-param">taps: int = 5</em>, <em class="sig-param">delay: int = 3</em>, <em class="sig-param">use_dnn_mask_for_wpe: bool = True</em>, <em class="sig-param">wnonlinear: str = 'crelu'</em>, <em class="sig-param">multi_source_wpe: bool = True</em>, <em class="sig-param">wnormalization: bool = False</em>, <em class="sig-param">use_beamformer: bool = True</em>, <em class="sig-param">bnet_type: str = 'blstmp'</em>, <em class="sig-param">blayers: int = 3</em>, <em class="sig-param">bunits: int = 300</em>, <em class="sig-param">bprojs: int = 320</em>, <em class="sig-param">badim: int = 320</em>, <em class="sig-param">ref_channel: int = -1</em>, <em class="sig-param">use_noise_mask: bool = True</em>, <em class="sig-param">bnonlinear: str = 'sigmoid'</em>, <em class="sig-param">beamformer_type: str = 'mvdr_souden'</em>, <em class="sig-param">rtf_iterations: int = 2</em>, <em class="sig-param">bdropout_rate: float = 0.0</em>, <em class="sig-param">shared_power: bool = True</em>, <em class="sig-param">diagonal_loading: bool = True</em>, <em class="sig-param">diag_eps_wpe: float = 1e-07</em>, <em class="sig-param">diag_eps_bf: float = 1e-07</em>, <em class="sig-param">mask_flooring: bool = False</em>, <em class="sig-param">flooring_thres_wpe: float = 1e-06</em>, <em class="sig-param">flooring_thres_bf: float = 1e-06</em>, <em class="sig-param">use_torch_solver: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/neural_beamformer.html#NeuralBeamformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.neural_beamformer.NeuralBeamformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<dl class="method">
<dt id="espnet2.enh.separator.neural_beamformer.NeuralBeamformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/neural_beamformer.html#NeuralBeamformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.neural_beamformer.NeuralBeamformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.complex64/ComplexTensor</em>) – mixed speech [Batch, Frames, Channel, Freq]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>List[torch.complex64/ComplexTensor]
output lengths
other predcited data: OrderedDict[</p>
<blockquote>
<div><p>’dereverb1’: ComplexTensor(Batch, Frames, Channel, Freq),
‘mask_dereverb1’: torch.Tensor(Batch, Frames, Channel, Freq),
‘mask_noise1’: torch.Tensor(Batch, Frames, Channel, Freq),
‘mask_spk1’: torch.Tensor(Batch, Frames, Channel, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Channel, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Channel, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>enhanced speech (single-channel)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.neural_beamformer.NeuralBeamformer.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.neural_beamformer.NeuralBeamformer.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-dan-separator">
<span id="id44"></span><h2>espnet2.enh.separator.dan_separator<a class="headerlink" href="#espnet2-enh-separator-dan-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.dan_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.dan_separator.DANSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.dan_separator.</code><code class="sig-name descname">DANSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">rnn_type: str = 'blstm'</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">nonlinear: str = 'tanh'</em>, <em class="sig-param">layer: int = 2</em>, <em class="sig-param">unit: int = 512</em>, <em class="sig-param">emb_D: int = 40</em>, <em class="sig-param">dropout: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dan_separator.html#DANSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dan_separator.DANSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Deep Attractor Network Separator</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>DEEP ATTRACTOR NETWORK FOR SINGLE-MICROPHONE SPEAKER SEPARATION;
Zhuo Chen. et al., 2017;
<a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/29430212/">https://pubmed.ncbi.nlm.nih.gov/29430212/</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>rnn_type</strong> – string, select from ‘blstm’, ‘lstm’ etc.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the inter-chunk RNN layers are bidirectional.</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
<li><p><strong>layer</strong> – int, number of stacked RNN layers. Default is 3.</p></li>
<li><p><strong>unit</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>emb_D</strong> – int, dimension of the attribute vector for one tf-bin.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.dan_separator.DANSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/dan_separator.html#DANSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dan_separator.DANSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, F]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
e.g. “feature_ref”: list of reference spectra List[(B, T, F)]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dan_separator.DANSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.dan_separator.DANSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-init">
<span id="id45"></span><h2>espnet2.enh.separator.__init__<a class="headerlink" href="#espnet2-enh-separator-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.__init__"></span></section>
<section id="espnet2-enh-separator-transformer-separator">
<span id="id46"></span><h2>espnet2.enh.separator.transformer_separator<a class="headerlink" href="#espnet2-enh-separator-transformer-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.transformer_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.transformer_separator.TransformerSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.transformer_separator.</code><code class="sig-name descname">TransformerSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">adim: int = 384</em>, <em class="sig-param">aheads: int = 4</em>, <em class="sig-param">layers: int = 6</em>, <em class="sig-param">linear_units: int = 1536</em>, <em class="sig-param">positionwise_layer_type: str = 'linear'</em>, <em class="sig-param">positionwise_conv_kernel_size: int = 1</em>, <em class="sig-param">normalize_before: bool = False</em>, <em class="sig-param">concat_after: bool = False</em>, <em class="sig-param">dropout_rate: float = 0.1</em>, <em class="sig-param">positional_dropout_rate: float = 0.1</em>, <em class="sig-param">attention_dropout_rate: float = 0.1</em>, <em class="sig-param">use_scaled_pos_enc: bool = True</em>, <em class="sig-param">nonlinear: str = 'relu'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/transformer_separator.html#TransformerSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.transformer_separator.TransformerSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Transformer separator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>adim</strong> (<em>int</em>) – Dimension of attention.</p></li>
<li><p><strong>aheads</strong> (<em>int</em>) – The number of heads of multi head attention.</p></li>
<li><p><strong>linear_units</strong> (<em>int</em>) – The number of units of position-wise feed forward.</p></li>
<li><p><strong>layers</strong> (<em>int</em>) – The number of transformer blocks.</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – Dropout rate.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<em>float</em>) – Dropout rate in attention.</p></li>
<li><p><strong>positional_dropout_rate</strong> (<em>float</em>) – Dropout rate after adding
positional encoding.</p></li>
<li><p><strong>normalize_before</strong> (<em>bool</em>) – Whether to use layer_norm before the first block.</p></li>
<li><p><strong>concat_after</strong> (<em>bool</em>) – Whether to concat attention layer’s input and output.
if True, additional linear will be applied.
i.e. x -&gt; x + linear(concat(x, att(x)))
if False, no additional linear will be applied. i.e. x -&gt; x + att(x)</p></li>
<li><p><strong>positionwise_layer_type</strong> (<em>str</em>) – “linear”, “conv1d”, or “conv1d-linear”.</p></li>
<li><p><strong>positionwise_conv_kernel_size</strong> (<em>int</em>) – Kernel size of
positionwise conv1d layer.</p></li>
<li><p><strong>use_scaled_pos_enc</strong> (<em>bool</em>) – use scaled positional encoding or not</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.transformer_separator.TransformerSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/transformer_separator.html#TransformerSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.transformer_separator.TransformerSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, N]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.transformer_separator.TransformerSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.transformer_separator.TransformerSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-svoice-separator">
<span id="id47"></span><h2>espnet2.enh.separator.svoice_separator<a class="headerlink" href="#espnet2-enh-separator-svoice-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.svoice_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.svoice_separator.Decoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.svoice_separator.</code><code class="sig-name descname">Decoder</code><span class="sig-paren">(</span><em class="sig-param">kernel_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/svoice_separator.html#Decoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.separator.svoice_separator.Decoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">est_source</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/svoice_separator.html#Decoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.Decoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.separator.svoice_separator.Encoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.svoice_separator.</code><code class="sig-name descname">Encoder</code><span class="sig-paren">(</span><em class="sig-param">enc_kernel_size: int</em>, <em class="sig-param">enc_feat_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/svoice_separator.html#Encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet2.enh.separator.svoice_separator.Encoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">mixture</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/svoice_separator.html#Encoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet2.enh.separator.svoice_separator.SVoiceSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.svoice_separator.</code><code class="sig-name descname">SVoiceSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">enc_dim: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">num_layers: int = 4</em>, <em class="sig-param">segment_size: int = 20</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">input_normalize: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/svoice_separator.html#SVoiceSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.SVoiceSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>SVoice model for speech separation.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Voice Separation with an Unknown Number of Multiple Speakers;
E. Nachmani et al., 2020;
<a class="reference external" href="https://arxiv.org/abs/2003.01531">https://arxiv.org/abs/2003.01531</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>enc_dim</strong> – int, dimension of the encoder module’s output. (Default: 128)</p></li>
<li><p><strong>kernel_size</strong> – int, the kernel size of Conv1D layer in both encoder and
decoder modules. (Default: 8)</p></li>
<li><p><strong>hidden_size</strong> – int, dimension of the hidden state in RNN layers. (Default: 128)</p></li>
<li><p><strong>num_spk</strong> – int, the number of speakers in the output. (Default: 2)</p></li>
<li><p><strong>num_layers</strong> – int, number of stacked MulCat blocks. (Default: 4)</p></li>
<li><p><strong>segment_size</strong> – dual-path segment size. (Default: 20)</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the RNN layers are bidirectional. (Default: True)</p></li>
<li><p><strong>input_normalize</strong> – bool, whether to apply GroupNorm on the input Tensor.
(Default: False)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.svoice_separator.SVoiceSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[torch.Tensor], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/svoice_separator.html#SVoiceSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.SVoiceSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, N]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.svoice_separator.SVoiceSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.SVoiceSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet2.enh.separator.svoice_separator.overlap_and_add">
<code class="sig-prename descclassname">espnet2.enh.separator.svoice_separator.</code><code class="sig-name descname">overlap_and_add</code><span class="sig-paren">(</span><em class="sig-param">signal</em>, <em class="sig-param">frame_step</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/svoice_separator.html#overlap_and_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.svoice_separator.overlap_and_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Reconstructs a signal from a framed representation.</p>
<blockquote>
<div><p>Adds potentially overlapping frames of a signal with shape
<cite>[…, frames, frame_length]</cite>, offsetting subsequent frames by <cite>frame_step</cite>.
The resulting tensor has shape <cite>[…, output_size]</cite> where</p>
<blockquote>
<div><p>output_size = (frames - 1) * frame_step + frame_length</p>
</div></blockquote>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>signal: A […, frames, frame_length] Tensor. All dimensions may be unknown,</dt><dd><p>and rank must be at least 2.</p>
</dd>
<dt>frame_step: An integer denoting overlap offsets.</dt><dd><p>Must be less than or equal to frame_length.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>A Tensor with shape […, output_size] containing the</dt><dd><p>overlap-added frames of signal’s inner-most two dimensions.</p>
</dd>
</dl>
<p>output_size = (frames - 1) * frame_step + frame_length</p>
</dd>
</dl>
<p>Based on</p>
</div></blockquote>
<p><a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py">https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py</a></p>
</dd></dl>

</section>
<section id="espnet2-enh-separator-tcn-separator">
<span id="id48"></span><h2>espnet2.enh.separator.tcn_separator<a class="headerlink" href="#espnet2-enh-separator-tcn-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.tcn_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.tcn_separator.TCNSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.tcn_separator.</code><code class="sig-name descname">TCNSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">layer: int = 8</em>, <em class="sig-param">stack: int = 3</em>, <em class="sig-param">bottleneck_dim: int = 128</em>, <em class="sig-param">hidden_dim: int = 512</em>, <em class="sig-param">kernel: int = 3</em>, <em class="sig-param">causal: bool = False</em>, <em class="sig-param">norm_type: str = 'gLN'</em>, <em class="sig-param">nonlinear: str = 'relu'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/tcn_separator.html#TCNSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.tcn_separator.TCNSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Temporal Convolution Separator</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>layer</strong> – int, number of layers in each stack.</p></li>
<li><p><strong>stack</strong> – int, number of stacks</p></li>
<li><p><strong>bottleneck_dim</strong> – bottleneck dimension</p></li>
<li><p><strong>hidden_dim</strong> – number of convolution channel</p></li>
<li><p><strong>kernel</strong> – int, kernel size.</p></li>
<li><p><strong>causal</strong> – bool, defalut False.</p></li>
<li><p><strong>norm_type</strong> – str, choose from ‘BN’, ‘gLN’, ‘cLN’</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.tcn_separator.TCNSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/tcn_separator.html#TCNSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.tcn_separator.TCNSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, N]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.tcn_separator.TCNSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.tcn_separator.TCNSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-dprnn-separator">
<span id="id49"></span><h2>espnet2.enh.separator.dprnn_separator<a class="headerlink" href="#espnet2-enh-separator-dprnn-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.dprnn_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.dprnn_separator.DPRNNSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.dprnn_separator.</code><code class="sig-name descname">DPRNNSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">rnn_type: str = 'lstm'</em>, <em class="sig-param">bidirectional: bool = True</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">nonlinear: str = 'relu'</em>, <em class="sig-param">layer: int = 3</em>, <em class="sig-param">unit: int = 512</em>, <em class="sig-param">segment_size: int = 20</em>, <em class="sig-param">dropout: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dprnn_separator.html#DPRNNSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dprnn_separator.DPRNNSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Dual-Path RNN (DPRNN) Separator</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>rnn_type</strong> – string, select from ‘RNN’, ‘LSTM’ and ‘GRU’.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the inter-chunk RNN layers are bidirectional.</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
<li><p><strong>layer</strong> – int, number of stacked RNN layers. Default is 3.</p></li>
<li><p><strong>unit</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>segment_size</strong> – dual-path segment size</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.dprnn_separator.DPRNNSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/dprnn_separator.html#DPRNNSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dprnn_separator.DPRNNSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, N]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dprnn_separator.DPRNNSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.dprnn_separator.DPRNNSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-fasnet-separator">
<span id="id50"></span><h2>espnet2.enh.separator.fasnet_separator<a class="headerlink" href="#espnet2-enh-separator-fasnet-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.fasnet_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.fasnet_separator.FaSNetSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.fasnet_separator.</code><code class="sig-name descname">FaSNetSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">enc_dim: int</em>, <em class="sig-param">feature_dim: int</em>, <em class="sig-param">hidden_dim: int</em>, <em class="sig-param">layer: int</em>, <em class="sig-param">segment_size: int</em>, <em class="sig-param">num_spk: int</em>, <em class="sig-param">win_len: int</em>, <em class="sig-param">context_len: int</em>, <em class="sig-param">fasnet_type: str</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">sr: int = 16000</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/fasnet_separator.html#FaSNetSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.fasnet_separator.FaSNetSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Filter-and-sum Network (FaSNet) Separator</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – required by AbsSeparator. Not used in this model.</p></li>
<li><p><strong>enc_dim</strong> – encoder dimension</p></li>
<li><p><strong>feature_dim</strong> – feature dimension</p></li>
<li><p><strong>hidden_dim</strong> – hidden dimension in DPRNN</p></li>
<li><p><strong>layer</strong> – number of DPRNN blocks in iFaSNet</p></li>
<li><p><strong>segment_size</strong> – dual-path segment size</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>win_len</strong> – window length in millisecond</p></li>
<li><p><strong>context_len</strong> – context length in millisecond</p></li>
<li><p><strong>fasnet_type</strong> – ‘fasnet’ or ‘ifasnet’.
Select from origin fasnet or Implicit fasnet</p></li>
<li><p><strong>dropout</strong> – dropout rate. Default is 0.</p></li>
<li><p><strong>sr</strong> – samplerate of input audio</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.fasnet_separator.FaSNetSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[torch.Tensor], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/fasnet_separator.html#FaSNetSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.fasnet_separator.FaSNetSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – (Batch, samples, channels)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. masks: OrderedDict[</p>
<blockquote>
<div><p>’mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>separated (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.fasnet_separator.FaSNetSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.fasnet_separator.FaSNetSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-dpcl-e2e-separator">
<span id="id51"></span><h2>espnet2.enh.separator.dpcl_e2e_separator<a class="headerlink" href="#espnet2-enh-separator-dpcl-e2e-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.dpcl_e2e_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.dpcl_e2e_separator.</code><code class="sig-name descname">DPCLE2ESeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">rnn_type: str = 'blstm'</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">nonlinear: str = 'tanh'</em>, <em class="sig-param">layer: int = 2</em>, <em class="sig-param">unit: int = 512</em>, <em class="sig-param">emb_D: int = 40</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">alpha: float = 5.0</em>, <em class="sig-param">max_iteration: int = 500</em>, <em class="sig-param">threshold: float = 1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dpcl_e2e_separator.html#DPCLE2ESeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Deep Clustering End-to-End Separator</p>
<p class="rubric">References</p>
<p>Single-Channel Multi-Speaker Separation using Deep Clustering;
Yusuf Isik. et al., 2016;
<a class="reference external" href="https://www.isca-speech.org/archive/interspeech_2016/isik16_interspeech.html">https://www.isca-speech.org/archive/interspeech_2016/isik16_interspeech.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>rnn_type</strong> – string, select from ‘blstm’, ‘lstm’ etc.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the inter-chunk RNN layers are bidirectional.</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
<li><p><strong>layer</strong> – int, number of stacked RNN layers. Default is 3.</p></li>
<li><p><strong>unit</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>emb_D</strong> – int, dimension of the feature vector for a tf-bin.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
<li><p><strong>alpha</strong> – float, the clustering hardness parameter.</p></li>
<li><p><strong>max_iteration</strong> – int, the max iterations of soft kmeans.</p></li>
<li><p><strong>threshold</strong> – float, the threshold to end the soft k-means process.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/dpcl_e2e_separator.html#DPCLE2ESeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, F]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. V: OrderedDict[</p>
<blockquote>
<div><p>others predicted data, e.g. masks: OrderedDict[
‘mask_spk1’: torch.Tensor(Batch, Frames, Freq),
‘mask_spk2’: torch.Tensor(Batch, Frames, Freq),
…
‘mask_spkn’: torch.Tensor(Batch, Frames, Freq),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.dpcl_e2e_separator.DPCLE2ESeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-dpcl-separator">
<span id="id52"></span><h2>espnet2.enh.separator.dpcl_separator<a class="headerlink" href="#espnet2-enh-separator-dpcl-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.dpcl_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.dpcl_separator.DPCLSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.dpcl_separator.</code><code class="sig-name descname">DPCLSeparator</code><span class="sig-paren">(</span><em class="sig-param">input_dim: int</em>, <em class="sig-param">rnn_type: str = 'blstm'</em>, <em class="sig-param">num_spk: int = 2</em>, <em class="sig-param">nonlinear: str = 'tanh'</em>, <em class="sig-param">layer: int = 2</em>, <em class="sig-param">unit: int = 512</em>, <em class="sig-param">emb_D: int = 40</em>, <em class="sig-param">dropout: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/separator/dpcl_separator.html#DPCLSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dpcl_separator.DPCLSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="espnet2.enh.separator.abs_separator.AbsSeparator"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.separator.abs_separator.AbsSeparator</span></code></a></p>
<p>Deep Clustering Separator.</p>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Deep clustering: Discriminative embeddings for segmentation and</dt><dd><p>separation; John R. Hershey. et al., 2016;
<a class="reference external" href="https://ieeexplore.ieee.org/document/7471631">https://ieeexplore.ieee.org/document/7471631</a></p>
</dd>
<dt>[2] Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding</dt><dd><p>Vectors Based on Regular Simplex; Tanaka, K. et al., 2021;
<a class="reference external" href="https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html">https://www.isca-speech.org/archive/interspeech_2021/tanaka21_interspeech.html</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – input feature dimension</p></li>
<li><p><strong>rnn_type</strong> – string, select from ‘blstm’, ‘lstm’ etc.</p></li>
<li><p><strong>bidirectional</strong> – bool, whether the inter-chunk RNN layers are bidirectional.</p></li>
<li><p><strong>num_spk</strong> – number of speakers</p></li>
<li><p><strong>nonlinear</strong> – the nonlinear function for mask estimation,
select from ‘relu’, ‘tanh’, ‘sigmoid’</p></li>
<li><p><strong>layer</strong> – int, number of stacked RNN layers. Default is 3.</p></li>
<li><p><strong>unit</strong> – int, dimension of the hidden state.</p></li>
<li><p><strong>emb_D</strong> – int, dimension of the feature vector for a tf-bin.</p></li>
<li><p><strong>dropout</strong> – float, dropout ratio. Default is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet2.enh.separator.dpcl_separator.DPCLSeparator.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: Union[torch.Tensor, torch_complex.tensor.ComplexTensor], ilens: torch.Tensor, additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[List[Union[torch.Tensor, torch_complex.tensor.ComplexTensor]], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/dpcl_separator.html#DPCLSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.dpcl_separator.DPCLSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em><em> or </em><em>ComplexTensor</em>) – Encoded feature [B, T, F]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
<li><p><strong>additional</strong> (<em>Dict</em><em> or </em><em>None</em>) – other data included in model
NOTE: not used in this model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>[(B, T, N), …]
ilens (torch.Tensor): (B,)
others predicted data, e.g. tf_embedding: OrderedDict[</p>
<blockquote>
<div><p>’tf_embedding’: learned embedding of all T-F bins (B, T * F, D),</p>
</div></blockquote>
<p>]</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>masked (List[Union(torch.Tensor, ComplexTensor)])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.dpcl_separator.DPCLSeparator.num_spk">
<em class="property">property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.dpcl_separator.DPCLSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-separator-abs-separator">
<span id="id53"></span><h2>espnet2.enh.separator.abs_separator<a class="headerlink" href="#espnet2-enh-separator-abs-separator" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.separator.abs_separator"></span><dl class="class">
<dt id="espnet2.enh.separator.abs_separator.AbsSeparator">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.separator.abs_separator.</code><code class="sig-name descname">AbsSeparator</code><a class="reference internal" href="../_modules/espnet2/enh/separator/abs_separator.html#AbsSeparator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.abs_separator.AbsSeparator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.enh.separator.abs_separator.AbsSeparator.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em>, <em class="sig-param">additional: Optional[Dict] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[Tuple[torch.Tensor], torch.Tensor, collections.OrderedDict]<a class="reference internal" href="../_modules/espnet2/enh/separator/abs_separator.html#AbsSeparator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.separator.abs_separator.AbsSeparator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet2.enh.separator.abs_separator.AbsSeparator.num_spk">
<em class="property">abstract property </em><code class="sig-name descname">num_spk</code><a class="headerlink" href="#espnet2.enh.separator.abs_separator.AbsSeparator.num_spk" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-decoder-conv-decoder">
<span id="id54"></span><h2>espnet2.enh.decoder.conv_decoder<a class="headerlink" href="#espnet2-enh-decoder-conv-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.decoder.conv_decoder"></span><dl class="class">
<dt id="espnet2.enh.decoder.conv_decoder.ConvDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.decoder.conv_decoder.</code><code class="sig-name descname">ConvDecoder</code><span class="sig-paren">(</span><em class="sig-param">channel: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">stride: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/decoder/conv_decoder.html#ConvDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.conv_decoder.ConvDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.decoder.abs_decoder.AbsDecoder" title="espnet2.enh.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<p>Transposed Convolutional decoder for speech enhancement and separation</p>
<dl class="method">
<dt id="espnet2.enh.decoder.conv_decoder.ConvDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/decoder/conv_decoder.html#ConvDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.conv_decoder.ConvDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<p>Args:
input (torch.Tensor): spectrum [Batch, T, F]
ilens (torch.Tensor): input lengths [Batch]</p>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-decoder-abs-decoder">
<span id="id55"></span><h2>espnet2.enh.decoder.abs_decoder<a class="headerlink" href="#espnet2-enh-decoder-abs-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.decoder.abs_decoder"></span><dl class="class">
<dt id="espnet2.enh.decoder.abs_decoder.AbsDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.decoder.abs_decoder.</code><code class="sig-name descname">AbsDecoder</code><a class="reference internal" href="../_modules/espnet2/enh/decoder/abs_decoder.html#AbsDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.abs_decoder.AbsDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="espnet2.enh.decoder.abs_decoder.AbsDecoder.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/espnet2/enh/decoder/abs_decoder.html#AbsDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.abs_decoder.AbsDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-decoder-init">
<span id="id56"></span><h2>espnet2.enh.decoder.__init__<a class="headerlink" href="#espnet2-enh-decoder-init" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.decoder.__init__"></span></section>
<section id="espnet2-enh-decoder-stft-decoder">
<span id="id57"></span><h2>espnet2.enh.decoder.stft_decoder<a class="headerlink" href="#espnet2-enh-decoder-stft-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.decoder.stft_decoder"></span><dl class="class">
<dt id="espnet2.enh.decoder.stft_decoder.STFTDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.decoder.stft_decoder.</code><code class="sig-name descname">STFTDecoder</code><span class="sig-paren">(</span><em class="sig-param">n_fft: int = 512</em>, <em class="sig-param">win_length: int = None</em>, <em class="sig-param">hop_length: int = 128</em>, <em class="sig-param">window='hann'</em>, <em class="sig-param">center: bool = True</em>, <em class="sig-param">normalized: bool = False</em>, <em class="sig-param">onesided: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/decoder/stft_decoder.html#STFTDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.stft_decoder.STFTDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.decoder.abs_decoder.AbsDecoder" title="espnet2.enh.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<p>STFT decoder for speech enhancement and separation</p>
<dl class="method">
<dt id="espnet2.enh.decoder.stft_decoder.STFTDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch_complex.tensor.ComplexTensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/decoder/stft_decoder.html#STFTDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.stft_decoder.STFTDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>ComplexTensor</em>) – spectrum [Batch, T, F]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="espnet2-enh-decoder-null-decoder">
<span id="id58"></span><h2>espnet2.enh.decoder.null_decoder<a class="headerlink" href="#espnet2-enh-decoder-null-decoder" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet2.enh.decoder.null_decoder"></span><dl class="class">
<dt id="espnet2.enh.decoder.null_decoder.NullDecoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet2.enh.decoder.null_decoder.</code><code class="sig-name descname">NullDecoder</code><a class="reference internal" href="../_modules/espnet2/enh/decoder/null_decoder.html#NullDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.null_decoder.NullDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet2.enh.decoder.abs_decoder.AbsDecoder" title="espnet2.enh.decoder.abs_decoder.AbsDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet2.enh.decoder.abs_decoder.AbsDecoder</span></code></a></p>
<p>Null decoder, return the same args.</p>
<dl class="method">
<dt id="espnet2.enh.decoder.null_decoder.NullDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input: torch.Tensor</em>, <em class="sig-param">ilens: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet2/enh/decoder/null_decoder.html#NullDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet2.enh.decoder.null_decoder.NullDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward. The input should be the waveform already.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>torch.Tensor</em>) – wav [Batch, sample]</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – input lengths [Batch]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="espnet2.lm.html" class="btn btn-neutral float-left" title="espnet2.lm package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="espnet2.fileio.html" class="btn btn-neutral float-right" title="espnet2.fileio package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017, Shinji Watanabe.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>