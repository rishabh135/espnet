batch_type: numel
batch_bins: 14000000
#batch_type: folded
#batch_size: 32
accum_grad: 1
max_epoch: 60
patience: none
log_interval: 3000
# The initialization method for model parameters
init: xavier_uniform
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10

#freeze_param: "encoder.encoders.mask_emb, encoder.encoders.feature_extractor,encoder.encoders.post_extract_proj, encoder.encoders.quantizer,encoder.encoders.project_q, encoder.encoders.encoder.pos_conv,encoder.encoders.encoder.layers.0, encoder.encoders.encoder.layers.1, encoder.encoders.encoder.layers.2,encoder.encoders.encoder.layers.3, encoder.encoders.encoder.layers.4, encoder.encoders.encoder.layers.5,encoder.encoders.encoder.layers.6, encoder.encoders.encoder.layers.7, encoder.encoders.encoder.layers.8,encoder.encoders.encoder.layers.9, encoder.encoders.encoder.layers.10, encoder.encoders.encoder.layers.11,encoder.encoders.encoder.layers.12, encoder.encoders.encoder.layers.13, encoder.encoders.encoder.layers.14,encoder.encoders.encoder.layers.15, encoder.encoders.encoder.layers.16, encoder.encoders.encoder.layers.17,encoder.encoders.encoder.layers.18, encoder.encoders.encoder.layers.19, encoder.encoders.encoder.layers.20,encoder.encoders.encoder.layers.21, encoder.encoders.encoder.layers.22,encoder.encoders.encoder.layer_norm, encoder.encoders.layer_norm"

encoder: wav2vec2
encoder_conf:
    output_size: 768
    normalize_before: false
    freeze_finetune_updates: -1
    w2v_url: https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small_960h.pt
    w2v_dir_path: ./downloads/wav2vec_pretrained_models
decoder: transformer
decoder_conf:
    attention_heads: 8
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.1
    src_attention_dropout_rate: 0.1

model_conf:
    ctc_weight: 1.0
    lsm_weight: 0.0
    report_cer: True
    report_wer: True
    length_normalized_loss: false

ctc_conf:
        ignore_nan_grad: true

optim: adam
optim_conf:
    lr: 0.001

scheduler: warmuplr
scheduler_conf:
    warmup_steps: 10000

    #scheduler: reducelronplateau
    #scheduler_conf:
    #mode: min
    #factor: 0.5
    #patience: 2 
unused_parameters: true

frontend: null

normalize: null

specaug: null
#specaug: specaug
#specaug_conf:
#    apply_time_warp: true
#    time_warp_window: 5
#    time_warp_mode: bicubic
#    apply_freq_mask: true
#    freq_mask_width_range:
#    - 0
#    - 30
#    num_freq_mask: 2
#    apply_time_mask: true
#    time_mask_width_range:
#    - 0
#    - 40
#    num_time_mask: 2
